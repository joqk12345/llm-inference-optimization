# é™„å½•A: å·¥å…·ä¸èµ„æº

> "å·¥æ¬²å–„å…¶äº‹,å¿…å…ˆåˆ©å…¶å™¨ã€‚" - å­”å­

æœ¬é™„å½•æä¾›äº†LLMæ¨ç†ä¼˜åŒ–è¿‡ç¨‹ä¸­å¸¸ç”¨çš„å·¥å…·ã€èµ„æºå’Œå­¦ä¹ ææ–™ï¼Œå¸®åŠ©ä½ å¿«é€Ÿæ‰¾åˆ°æ‰€éœ€çš„æ”¯æŒã€‚

---

## A.1 æ¨ç†æ¡†æ¶å¯¹æ¯”

### A.1.1 vLLM

**ç®€ä»‹**:
- ç”±UC Berkeleyå‘èµ·çš„å¼€æºé¡¹ç›®
- ä¸“æ³¨äºé«˜ååé‡å’Œä½å»¶è¿Ÿçš„LLMæœåŠ¡
- PagedAttentionå’ŒContinuous Batchingçš„å…ˆé©±

**æ ¸å¿ƒç‰¹æ€§**:
```yaml
PagedAttention:
  - åˆ›æ–°çš„KV Cacheç®¡ç†
  - ç±»ä¼¼OSè™šæ‹Ÿå†…å­˜çš„åˆ†é¡µæœºåˆ¶
  - é«˜å†…å­˜åˆ©ç”¨ç‡

Continuous Batching:
  - åŠ¨æ€æ‰¹å¤„ç†
  - è¯·æ±‚çº§åˆ«çš„è°ƒåº¦
  - æœ€ä¼˜GPUåˆ©ç”¨ç‡

OpenAIå…¼å®¹API:
  - é›¶ä»£ç è¿ç§»
  - ç”Ÿæ€å…¼å®¹æ€§å¥½
```

**æ€§èƒ½åŸºå‡†**(Llama-3-8B, A100):
```
ååé‡: ~2000 tokens/s (batch size 32)
å»¶è¿Ÿ: P95 < 50ms
GPUåˆ©ç”¨ç‡: 85%+
```

**é€‚ç”¨åœºæ™¯**:
- âœ… ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- âœ… é«˜å¹¶å‘åœºæ™¯
- âœ… éœ€è¦OpenAIå…¼å®¹æ€§
- âœ… å¤šç§Ÿæˆ·SaaSå¹³å°

**GitHub**: https://github.com/vllm-project/vllm

---

### A.1.2 TGI (Text Generation Inference)

**ç®€ä»‹**:
- Hugging Faceæ¨å‡ºçš„æ¨ç†æ¡†æ¶
- ä¸“æ³¨äºæ˜“ç”¨æ€§å’Œç”Ÿäº§å°±ç»ª

**æ ¸å¿ƒç‰¹æ€§**:
```yaml
æ˜“ç”¨æ€§:
  - ä¸€è¡Œå‘½ä»¤å¯åŠ¨
  - è‡ªåŠ¨æ¨¡å‹ä¼˜åŒ–
  - å†…ç½®é‡åŒ–æ”¯æŒ

å®‰å…¨æ€§:
  - JWTè®¤è¯
  - Bloom filteré˜²æŠ¤
  - é€Ÿç‡é™åˆ¶

ç›‘æ§:
  - PrometheusæŒ‡æ ‡
  - æ—¥å¿—é›†æˆ
```

**æ€§èƒ½åŸºå‡†**(Llama-3-8B, A100):
```
ååé‡: ~1500 tokens/s
å»¶è¿Ÿ: P95 < 60ms
GPUåˆ©ç”¨ç‡: 75%+
```

**é€‚ç”¨åœºæ™¯**:
- âœ… å¿«é€ŸåŸå‹å¼€å‘
- âœ… Hugging Faceç”Ÿæ€ç”¨æˆ·
- âœ… éœ€è¦ä¼ä¸šçº§å®‰å…¨ç‰¹æ€§

**GitHub**: https://github.com/huggingface/text-generation-inference

---

### A.1.3 TensorRT-LLM

**ç®€ä»‹**:
- NVIDIAæ¨å‡ºçš„å®˜æ–¹æ¨ç†æ¡†æ¶
- åŸºäºTensorRTæ·±åº¦ä¼˜åŒ–

**æ ¸å¿ƒç‰¹æ€§**:
```yaml
ç¡¬ä»¶ä¼˜åŒ–:
  - é’ˆå¯¹NVIDIA GPUä¼˜åŒ–
  - FP8ã€INT4é‡åŒ–
  - Fusionç®—å­

æ€§èƒ½:
  - æœ€é«˜ååé‡
  - æœ€ä½å»¶è¿Ÿ
  - Tensor Coreå……åˆ†åˆ©ç”¨

ä¼ä¸šçº§:
  - ç”Ÿäº§çº§æ”¯æŒ
  - NVIDIAå®˜æ–¹ç»´æŠ¤
```

**æ€§èƒ½åŸºå‡†**(Llama-3-8B, H100):
```
ååé‡: ~3000 tokens/s
å»¶è¿Ÿ: P95 < 30ms
GPUåˆ©ç”¨ç‡: 95%+
```

**é€‚ç”¨åœºæ™¯**:
- âœ… æè‡´æ€§èƒ½è¦æ±‚
- âœ… NVIDIA GPUç¯å¢ƒ
- âœ… ä¼ä¸šçº§éƒ¨ç½²

**æ–‡æ¡£**: https://nvidia.github.io/TensorRT-LLM/

---

### A.1.4 TensorRT-LLM vs vLLM

| ç»´åº¦ | TensorRT-LLM | vLLM |
|------|--------------|------|
| **æ€§èƒ½** | æ›´å¿« | å¿« |
| **æ˜“ç”¨æ€§** | å¤æ‚ | ç®€å• |
| **ç”Ÿæ€** | NVIDIAä¸“æœ‰ | å¼€æºå‹å¥½ |
| **ä¼˜åŒ–** | ç¡¬ä»¶çº§ | ç®—æ³•çº§ |
| **å­¦ä¹ æ›²çº¿** | é™¡å³­ | å¹³ç¼“ |
| **æˆæœ¬** | å…è´¹ | å…è´¹ |
| **ç¤¾åŒº** | NVIDIAæ”¯æŒ | ç¤¾åŒºé©±åŠ¨ |

**é€‰æ‹©å»ºè®®**:
```
é€‰æ‹©TensorRT-LLM, å¦‚æœ:
  - è¿½æ±‚æè‡´æ€§èƒ½
  - ä½¿ç”¨NVIDIA GPU
  - æœ‰NVIDIAæŠ€æœ¯æ”¯æŒ

é€‰æ‹©vLLM, å¦‚æœ:
  - éœ€è¦å¿«é€Ÿè¿­ä»£
  - é‡è§†ç¤¾åŒºç”Ÿæ€
  - å›¢é˜Ÿç†Ÿæ‚‰Python
```

---

### A.1.5 é€‰æ‹©å»ºè®®

**å†³ç­–æ ‘**:

```
ç”Ÿäº§ç¯å¢ƒ?
  â”œâ”€ æ˜¯ â†’ é«˜å¹¶å‘(>100 QPS)?
  â”‚   â”œâ”€ æ˜¯ â†’ NVIDIA GPU?
  â”‚   â”‚   â”œâ”€ æ˜¯ â†’ TensorRT-LLM (æè‡´æ€§èƒ½)
  â”‚   â”‚   â””â”€ å¦ â†’ vLLM (å¼€æºå‹å¥½)
  â”‚   â””â”€ å¦ â†’ TGI (å¿«é€Ÿéƒ¨ç½²)
  â””â”€ å¦ â†’ vLLM (æ˜“ç”¨æ€§æœ€ä½³)
```

**æ¡†æ¶é€‰æ‹©çŸ©é˜µ**:

| åœºæ™¯ | æ¨èæ¡†æ¶ | ç†ç”± |
|------|---------|------|
| **ç”Ÿäº§ç¯å¢ƒé«˜å¹¶å‘** | vLLM | æˆç†Ÿç¨³å®š |
| **æè‡´æ€§èƒ½è¦æ±‚** | TensorRT-LLM | ç¡¬ä»¶ä¼˜åŒ– |
| **å¿«é€ŸåŸå‹** | TGI | ä¸€è¡Œå‘½ä»¤ |
| **å­¦æœ¯ç ”ç©¶** | vLLM | æ˜“äºä¿®æ”¹ |
| **ä¼ä¸šçº§éƒ¨ç½²** | TensorRT-LLM | å®˜æ–¹æ”¯æŒ |
| **å¤šæ¨¡æ€** | vLLM | ç”Ÿæ€å®Œå–„ |
| **MoEæ¨¡å‹** | vLLM | Large EPæ”¯æŒ |

---

## A.2 æ¨¡å‹èµ„æº

### A.2.1 å¼€æºæ¨¡å‹ä»“åº“

**Hugging Face Hub**
- URL: https://huggingface.co/models
- æ¨¡å‹æ•°é‡: 100ä¸‡+
- ç‰¹ç‚¹: æœ€å¤§ã€æœ€æ´»è·ƒçš„å¼€æºæ¨¡å‹ç¤¾åŒº
- æœç´¢æŠ€å·§:
  ```python
  # æŒ‰ä»»åŠ¡è¿‡æ»¤
  - task:text-generation
  - task:text2text-generation

  # æŒ‰è¯­è¨€è¿‡æ»¤
  - language:zh
  - language:en

  # æŒ‰è®¸å¯è¿‡æ»¤
  - license:mit
  - license:apache-2.0
  ```

**ModelScope**
- URL: https://modelscope.cn/models
- é˜¿é‡Œäº‘æ¨å‡º
- ç‰¹ç‚¹: å›½å†…è®¿é—®å¿«,ä¸­æ–‡æ¨¡å‹ä¸°å¯Œ

**LG AI Research EXAONE**
- URL: https://huggingface.co/LGAI-EXAONE
- éŸ©å›½LGé›†å›¢å¼€æº
- ç‰¹ç‚¹: é«˜è´¨é‡å¤šè¯­è¨€æ¨¡å‹

**âœ¨ vLLMæ¨¡å‹åº“**
- URL: https://huggingface.co/org/vllm
- vLLMå›¢é˜Ÿä¼˜åŒ–çš„æ¨¡å‹
- ç‰¹ç‚¹: å¼€ç®±å³ç”¨,æ€§èƒ½ä¼˜åŒ–

---

### A.2.2 é‡åŒ–æ¨¡å‹ä¸‹è½½

**TheBlokeé‡åŒ–ç³»åˆ—**
- Hugging Face: https://huggingface.co/TheBloke
- æä¾›å¤§é‡INT4/INT8é‡åŒ–æ¨¡å‹
- æ”¯æŒæ ¼å¼: GPTQã€AWQã€GGUF

**ä¸‹è½½ç¤ºä¾‹**:
```bash
# ä½¿ç”¨huggingface-cli
pip install -U "huggingface_hub[cli]"

# ä¸‹è½½Llama-3-8B-Instruct-Q4_K_M.gguf
huggingface-cli download \
  TheBloke/Llama-3-8B-Instruct-GGUF \
  llama-3-8b-instruct-q4_k_m.gguf \
  --local-dir ./models

# ä½¿ç”¨git lfs
git lfs install
git clone https://huggingface.co/TheBloke/Llama-3-8B-Instruct-GGUF
```

**GGUFæ¨¡å‹(llama.cpp)**
- URL: https://huggingface.co/models?search=gguf
- ç‰¹ç‚¹: CPUæ¨ç†å‹å¥½
- é€‚åˆ: Macã€æœ¬åœ°éƒ¨ç½²

**vLLMæ”¯æŒçš„é‡åŒ–æ ¼å¼**:
```python
# AWQ
vllm serve TheBloke/Llama-3-8B-Instruct-AWQ

# GPTQ
vllm serve TheBloke/Llama-3-8B-Instruct-GPTQ

# BitsAndBytes
vllm serve meta-llama/Llama-3-8B \
  --load-format bitsandbytes \
  --quantization bitsandbytes
```

---

### A.2.3 æ•°æ®é›†èµ„æº

**Hugging Face Datasets**
- URL: https://huggingface.co/datasets
- æ•°æ®é›†æ•°é‡: 10ä¸‡+
- å¸¸ç”¨æ•°æ®é›†:
  ```python
  # å¯¹è¯æ•°æ®é›†
  - OpenAssistant/oasst1
  - LMSys/Chatbot-Arena-Conversations
  - Anthropic/hh-rlhf

  # æŒ‡ä»¤å¾®è°ƒ
  - tatsu-lab/alpaca
  - Open-Orca/OpenOrca

  # è¯„ä¼°åŸºå‡†
  - EleutherAI/lm-evaluation-harness
  - MMLU
  - GSM8K
  ```

**Common Crawl**
- URL: https://commoncrawl.org/
- ç‰¹ç‚¹: å¤§è§„æ¨¡ç½‘é¡µæ•°æ®
- ç”¨é€”: é¢„è®­ç»ƒã€RAGçŸ¥è¯†åº“

**C4 (Colossal Clean Crawled Corpus)**
- URL: https://www.tensorflow.org/datasets/catalog/c4
- ç‰¹ç‚¹: æ¸…æ´—åçš„Common Crawl
- ç”¨é€”: é¢„è®­ç»ƒ

---

### A.2.4 åŸºå‡†æµ‹è¯•ç»“æœ

**LMSys Chatbot Arena**
- URL: https://lmarena.ai/
- ç‰¹ç‚¹: äººç±»å¯¹æˆ˜è¯„ä¼°
- æ›´æ–°: æ¯å‘¨æ›´æ–°æ’å
- æŸ¥çœ‹æ–¹å¼:
  ```bash
  # è®¿é—® leaderboard
  https://lmarena.ai/?leaderboard

  # æŒ‰æ¨¡å‹å¤§å°è¿‡æ»¤
  - <10B params
  - 10B-50B params
  - >50B params
  ```

**MMLU (Massive Multitask Language Understanding)**
- URL: https://github.com/hendrycks/test
- ç‰¹ç‚¹: å­¦æœ¯çŸ¥è¯†è¯„ä¼°
- ç±»åˆ«: 57ä¸ªå­¦ç§‘
- è¿è¡Œ:
  ```bash
  pip install lm-eval

  lm-eval --model hf \
    --model_args pretrained=meta-llama/Llama-3-8B \
    --tasks mmlu \
    --batch_size 8
  ```

**OpenLLM Leaderboard**
- URL: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
- ç‰¹ç‚¹: æ ‡å‡†åŒ–è¯„ä¼°
- æŒ‡æ ‡: MMLUã€MATHã€HumanEvalç­‰

---

## A.3 å¼€å‘å·¥å…·é›†

### A.3.1 æ€§èƒ½åˆ†æå·¥å…·

**NVIDIA Nsight Systems**
- URL: https://developer.nvidia.com/nsight-systems
- åŠŸèƒ½: ç³»ç»Ÿçº§æ€§èƒ½åˆ†æ
- ä½¿ç”¨:
  ```bash
  # é‡‡é›†trace
  nsys profile -o report \
    python your_vllm_app.py

  # æŸ¥çœ‹GUI
  nsys-ui report.qdrep
  ```
- è®¸å¯: å…è´¹

**NVIDIA Nsight Compute**
- URL: https://developer.nvidia.com/nsight-compute
- åŠŸèƒ½: Kernelçº§æ·±åº¦åˆ†æ
- ä½¿ç”¨:
  ```bash
  ncu --set full \
    -o output_report \
    python your_vllm_app.py
  ```
- è®¸å¯: å…è´¹

**PyTorch Profiler**
- URL: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html
- åŠŸèƒ½: Python/CUDAç“¶é¢ˆè¯Šæ–­
- ä½¿ç”¨:
  ```python
  import torch

  with torch.profiler.profile(
      activities=[
          torch.profiler.ProfilerActivity.CPU,
          torch.profiler.ProfilerActivity.CUDA,
      ]
  ) as prof:
      # ä½ çš„ä»£ç 
      pass

  print(prof.key_averages().table(sort_by="cuda_time_total"))
  ```
- è®¸å¯: å¼€æº

**vLLMå†…ç½®benchmark**
- è·¯å¾„: `vllm/benchmark_serving.py`
- ä½¿ç”¨:
  ```bash
  python benchmark_serving.py \
    --model meta-llama/Llama-3-8B \
    --dataset-name sharegpt \
    --num-prompts 1000
  ```
- è®¸å¯: Apache 2.0

---

### A.3.2 å¯è§†åŒ–å·¥å…·

**TensorBoard**
- URL: https://www.tensorflow.org/tensorboard
- åŠŸèƒ½: æŸå¤±æ›²çº¿ã€æŒ‡æ ‡å¯è§†åŒ–
- ä½¿ç”¨:
  ```bash
  pip install tensorboard

  tensorboard --logdir ./logs

  # è®¿é—® http://localhost:6006
  ```

**Weights & Biases**
- URL: https://wandb.ai/
- åŠŸèƒ½: å®éªŒè·Ÿè¸ªã€å¯è§†åŒ–
- ä½¿ç”¨:
  ```python
  import wandb

  wandb.init(project="llm-inference")
  wandb.log({"ttft": 1.2, "tpot": 0.08})
  ```

**Grafana**
- URL: https://grafana.com/
- åŠŸèƒ½: ç›‘æ§ä»ªè¡¨ç›˜
- ä½¿ç”¨: é…åˆPrometheus

**Chrome Trace Viewer**
- åŠŸèƒ½: æŸ¥çœ‹Chrome trace
- ä½¿ç”¨:
  ```
  1. æ‰“å¼€ chrome://tracing
  2. Load trace file
  3. æŸ¥çœ‹æ—¶é—´çº¿
  ```

---

### A.3.3 è°ƒè¯•å·¥å…·

**Python pdb**
- å†…ç½®Pythonè°ƒè¯•å™¨
- ä½¿ç”¨:
  ```python
  import pdb; pdb.set_trace()

  # å‘½ä»¤:
  # n - next
  # s - step
  # c - continue
  # p variable - print variable
  ```

**ipdb**
- å¢å¼ºçš„Pythonè°ƒè¯•å™¨
- å®‰è£…: `pip install ipdb`
- ä½¿ç”¨:
  ```python
  import ipdb; ipdb.set_trace()
  ```

**CUDA-GDB**
- NVIDIA CUDAè°ƒè¯•å™¨
- ä½¿ç”¨:
  ```bash
  cuda-gdb python your_app.py

  (cuda-gdb) run
  (cuda-gdb) bt  # backtrace
  ```

**vLLMè°ƒè¯•æ¨¡å¼**
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
VLLM_LOGGING_LEVEL=DEBUG vllm serve meta-llama/Llama-3-8B

# å¯ç”¨trace
VLLM_USE_TRACING=1 vllm serve meta-llama/Llama-3-8B
```

---

### A.3.4 éƒ¨ç½²å·¥å…·

**Docker**
- URL: https://www.docker.com/
- åŠŸèƒ½: å®¹å™¨åŒ–éƒ¨ç½²
- ä½¿ç”¨:
  ```bash
  docker build -t vllm-app .
  docker run -p 8000:8000 --gpus all vllm-app
  ```

**Kubernetes (k8s)**
- URL: https://kubernetes.io/
- åŠŸèƒ½: å®¹å™¨ç¼–æ’
- æ ¸å¿ƒèµ„æº:
  ```yaml
  Deployment:  # å‰¯æœ¬ç®¡ç†
  Service:     # æœåŠ¡å‘ç°
  ConfigMap:   # é…ç½®ç®¡ç†
  HPA:         # è‡ªåŠ¨ä¼¸ç¼©
  ```

**Helm**
- URL: https://helm.sh/
- åŠŸèƒ½: K8såŒ…ç®¡ç†
- ä½¿ç”¨:
  ```bash
  helm install my-vllm ./vllm-chart
  ```

**Ray**
- URL: https://www.ray.io/
- åŠŸèƒ½: åˆ†å¸ƒå¼è®¡ç®—
- vLLMåˆ†å¸ƒå¼æ‰§è¡Œåç«¯

---

## A.4 å­¦ä¹ èµ„æº

### A.4.1 æ¨èè®ºæ–‡

**PagedAttention (vLLM)**
- æ ‡é¢˜: "Efficient Memory Management for LLM Serving with PagedAttention"
- é“¾æ¥: https://arxiv.org/abs/2309.06180
- æ ¸å¿ƒè´¡çŒ®: PagedAttentionã€Continuous Batching

**Flash Attention**
- æ ‡é¢˜: "Flash Attention: Fast and Memory-Efficient Exact Attention"
- é“¾æ¥: https://arxiv.org/abs/2205.14135
- æ ¸å¿ƒè´¡çŒ®: O(N)å†…å­˜å¤æ‚åº¦çš„Attention

**Speculative Decoding**
- æ ‡é¢˜: "Assisted Decoding: Speculative Decoding for Large Language Models"
- é“¾æ¥: https://arxiv.org/abs/2211.17192
- æ ¸å¿ƒè´¡çŒ®: è‰ç¨¿æ¨¡å‹éªŒè¯æœºåˆ¶

**GPTQ Quantization**
- æ ‡é¢˜: "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
- é“¾æ¥: https://arxiv.org/abs/2210.17323
- æ ¸å¿ƒè´¡çŒ®: INT4é‡åŒ–ç®—æ³•

**AWQ Quantization**
- æ ‡é¢˜: "AWQ: Activation-aware Weight Quantization for LLM Acceleration"
- é“¾æ¥: https://arxiv.org/abs/2306.00978
- æ ¸å¿ƒè´¡çŒ®: Activation-awareé‡åŒ–

**Radix Attention (SGLang)**
- æ ‡é¢˜: "SGLang: Efficient Execution of Structured Language Model Programs"
- é“¾æ¥: https://arxiv.org/abs/2312.15567
- æ ¸å¿ƒè´¡çŒ®: Radix Treeã€KV Cacheå¤ç”¨

---

### A.4.2 åšå®¢å’Œæ–‡ç« 

**vLLMå®˜æ–¹åšå®¢**
- URL: https://blog.vllm.ai/
- æ¨è:
  - Large-scale Expert Parallelism
  - EPD (Expert-Parallel Data Parallelism)
  - vLLM Plugin System

**SGLangåšå®¢**
- URL: https://lmsys.org/blog/
- æ¨è:
  - SGLang v0.4 Release
  - Mini-SGLang Announcement

**Manusåšå®¢**
- URL: https://manus.im/blog
- æ¨è:
  - Context Engineering for AI Agents
  - Lessons from Building Manus

**NVIDIAæŠ€æœ¯åšå®¢**
- URL: https://developer.nvidia.com/blog/
- æ¨è:
  - Flash Attentionç³»åˆ—
  - TensorRT-LLMä¼˜åŒ–

**Jay Alammaråšå®¢**
- URL: https://jalammar.github.io/
- æ¨è:
  - The Illustrated Transformer
  - Visualizing Attention

---

### A.4.3 è§†é¢‘è¯¾ç¨‹

**Andrej Karpathy - "Neural Networks: Zero to Hero"**
- å¹³å°: YouTube
- é“¾æ¥: https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ
- å†…å®¹: ä»é›¶å®ç°ç¥ç»ç½‘ç»œ
- éš¾åº¦: ä¸­çº§

**æ–¯å¦ç¦CS224N - NLP with Deep Learning**
- å¹³å°: YouTube
- é“¾æ¥: https://www.youtube.com/playlist?list=PLoROMvodv4rMFqRtEuo6lUmCPqkqHVh1B
- å†…å®¹: NLPæ·±åº¦å­¦ä¹ 
- éš¾åº¦: æœ¬ç§‘é«˜å¹´çº§/ç ”ç©¶ç”Ÿ

**fast.ai - Practical Deep Learning for Coders**
- å¹³å°: fast.ai
- é“¾æ¥: https://course.fast.ai/
- å†…å®¹: å®æˆ˜æ·±åº¦å­¦ä¹ 
- éš¾åº¦: åˆçº§-ä¸­çº§

**NVIDIA GTCä¼šè®®**
- å¹³å°: NVIDIA On Demand
- é“¾æ¥: https://www.nvidia.com/gtc/
- å†…å®¹: GPUæŠ€æœ¯å‰æ²¿
- éš¾åº¦: ä¸­çº§-é«˜çº§

**PyTorchå¼€å‘è€…å¤§ä¼š**
- å¹³å°: YouTube
- é“¾æ¥: https://www.youtube.com/@PyTorch
- å†…å®¹: PyTorchæœ€æ–°ç‰¹æ€§

---

### A.4.4 ç¤¾åŒºèµ„æº

**DiscordæœåŠ¡å™¨**
- vLLM Discord: https://discord.gg/vllm
- LMSys Discord: https://discord.gg/msys
- PyTorch Discord: https://discord.gg/pytorch

**Redditç¤¾åŒº**
- r/LocalLLaMA: https://reddit.com/r/LocalLLaMA
- r/MachineLearning: https://reddit.com/r/MachineLearning
- r/reddit.com/r/OpenAI

**Stack Overflow**
- æ ‡ç­¾: `vllm`, `llm`, `cuda`, `pytorch`
- é“¾æ¥: https://stackoverflow.com/questions/tagged/vllm

**GitHub Discussions**
- vLLM: https://github.com/vllm-project/vllm/discussions
- SGLang: https://github.com/sgl-project/sglang/discussions

**é‚®ä»¶åˆ—è¡¨**
- vLLM Announcements: https://groups.google.com/g/vllm-announce

---

## A.5 æœ¯è¯­è¡¨

### A.5.1 LLMæœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|------|------|------|
| **å¤§è¯­è¨€æ¨¡å‹** | Large Language Model | å‚æ•°é‡è¾¾åäº¿çº§åˆ«çš„ç¥ç»ç½‘ç»œæ¨¡å‹ |
| **Transformer** | Transformer | Googleæå‡ºçš„æ³¨æ„åŠ›æœºåˆ¶æ¶æ„ |
| **è‡ªå›å½’ç”Ÿæˆ** | Autoregressive Generation | é€tokenç”Ÿæˆ,æ¯ä¸ªtokenä¾èµ–ä¹‹å‰æ‰€æœ‰token |
| **Prefill** | Prefill Phase | å¤„ç†è¾“å…¥promptçš„é˜¶æ®µ,å¹¶è¡Œè®¡ç®— |
| **Decode** | Decode Phase | ç”Ÿæˆè¾“å‡ºtokençš„é˜¶æ®µ,ä¸²è¡Œè®¡ç®— |
| **ä¸Šä¸‹æ–‡çª—å£** | Context Window | æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦ |
| **Token** | Token | æ–‡æœ¬çš„æœ€å°å•ä½,å•è¯æˆ–å­è¯ |
| **Temperature** | Temperature | æ§åˆ¶ç”Ÿæˆéšæœºæ€§çš„å‚æ•° |
| **Top-P / Top-K** | Nucleus Sampling | é‡‡æ ·ç­–ç•¥,é™åˆ¶å€™é€‰tokenèŒƒå›´ |
| **System Prompt** | System Prompt | ç³»ç»Ÿæç¤ºè¯,å®šä¹‰æ¨¡å‹è¡Œä¸º |

---

### A.5.2 GPUæœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|------|------|------|
| **æ˜¾å­˜** | VRAM (Video RAM) | GPUä¸“ç”¨å†…å­˜ |
| **å†…å­˜å¸¦å®½** | Memory Bandwidth | GPUè¯»å†™å†…å­˜çš„é€Ÿåº¦ |
| **Tensor Core** | Tensor Core | NVIDIA GPUä¸Šçš„çŸ©é˜µè¿ç®—åŠ é€Ÿå•å…ƒ |
| **CUDA** | CUDA | NVIDIAçš„å¹¶è¡Œè®¡ç®—å¹³å° |
| **SM** | Streaming Multiprocessor | GPUçš„è®¡ç®—æ ¸å¿ƒå•å…ƒ |
| **Warp** | Warp | CUDAçš„æ‰§è¡Œå•å…ƒ,32ä¸ªçº¿ç¨‹ä¸€ç»„ |
| **Occupancy** | Occupancy | SMä¸Šæ´»è·ƒwarpçš„æ•°é‡ |
| **FLOPS** | Floating Point Operations Per Second | æ¯ç§’æµ®ç‚¹è¿ç®—æ¬¡æ•° |
| **TFLOPS** | TeraFLOPS | æ¯ç§’ä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®— |
| **PCIe** | PCI Express | GPUä¸CPUçš„é€šä¿¡æ€»çº¿ |

---

### A.5.3 æ¨ç†ä¼˜åŒ–æœ¯è¯­

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|------|------|------|
| **KV Cache** | Key-Value Cache | ç¼“å­˜æ³¨æ„åŠ›è®¡ç®—çš„Kã€VçŸ©é˜µ |
| **PagedAttention** | PagedAttention | vLLMçš„åˆ†é¡µå¼KV Cacheç®¡ç† |
| **Continuous Batching** | Continuous Batching | åŠ¨æ€æ‰¹å¤„ç†,è¯·æ±‚çº§åˆ«çš„è°ƒåº¦ |
| **é‡åŒ–** | Quantization | é™ä½æ¨¡å‹ç²¾åº¦(FP16â†’INT8/INT4) |
| **PTQ** | Post-Training Quantization | è®­ç»ƒåé‡åŒ– |
| **QAT** | Quantization-Aware Training | é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ |
| **æŠ•æœºé‡‡æ ·** | Speculative Sampling | ç”¨å°æ¨¡å‹åŠ é€Ÿå¤§æ¨¡å‹ |
| **INT4** | 4-bit Integer | 4ä½æ•´æ•°è¡¨ç¤º |
| **FP8** | 8-bit Floating Point | 8ä½æµ®ç‚¹è¡¨ç¤º |
| **TTFT** | Time To First Token | é¦–ä¸ªtokençš„å»¶è¿Ÿ |
| **TPOT** | Time Per Output Token | æ¯ä¸ªè¾“å‡ºtokençš„å»¶è¿Ÿ |
| **MoE** | Mixture of Experts | æ··åˆä¸“å®¶æ¨¡å‹ |
| **EP** | Expert Parallelism | ä¸“å®¶å¹¶è¡Œ |

---

**ğŸ’¡ ä½¿ç”¨å»ºè®®**

1. **å¿«é€ŸæŸ¥æ‰¾**: ä½¿ç”¨ `Ctrl+F` / `Cmd+F` æœç´¢æœ¯è¯­
2. **æ·±å…¥å­¦ä¹ **: ç‚¹å‡»é“¾æ¥æŸ¥çœ‹åŸå§‹èµ„æº
3. **ç¤¾åŒºæ”¯æŒ**: é‡åˆ°é—®é¢˜æ—¶æŸ¥é˜…Discordå’ŒStack Overflow
4. **æŒç»­æ›´æ–°**: æœ¬ä¹¦ä¼šå®šæœŸæ›´æ–°èµ„æºå’Œé“¾æ¥

---

**æœ‰é—®é¢˜?æŸ¥çœ‹ [é™„å½•B: æ•…éšœæ’æŸ¥æŒ‡å—](appendix-b-troubleshooting.md)**
