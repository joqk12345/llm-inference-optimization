# ç¬¬4ç« : ç¯å¢ƒæ­å»º

> **ğŸ’° å•†ä¸šåŠ¨æœº**: æ­£ç¡®çš„ç¯å¢ƒé…ç½®å¯ä»¥é¿å…80%çš„éƒ¨ç½²é—®é¢˜ã€‚æ ¹æ®è¡Œä¸šæ•°æ®,ç¯å¢ƒä¸å½“å¯¼è‡´çš„æ•…éšœå¹³å‡æ’æŸ¥æ—¶é—´ä¸º4-8å°æ—¶,è€Œæ­£ç¡®é…ç½®å¯ä»¥åœ¨30åˆ†é’Ÿå†…å®Œæˆéƒ¨ç½²ã€‚

## ç®€ä»‹

åœ¨æ·±å…¥ä¼˜åŒ–æŠ€æœ¯ä¹‹å‰,æˆ‘ä»¬éœ€è¦å…ˆæ­å»ºä¸€ä¸ªå¯é çš„å¼€å‘ç¯å¢ƒã€‚å¾ˆå¤šå·¥ç¨‹å¸ˆåœ¨è¿™ä¸€æ­¥èŠ±è´¹äº†å¤ªå¤šæ—¶é—´â€”â€”CUDAç‰ˆæœ¬å†²çªã€Dockeræƒé™é—®é¢˜ã€ä¾èµ–åŒ…ç‰ˆæœ¬ä¸åŒ¹é…â€¦â€¦è¿™äº›é—®é¢˜éƒ½ä¼šé˜»ç¢ä½ çš„è¿›åº¦ã€‚

æœ¬ç« å°†å¸®ä½ :
- ç†è§£ä¸ºä»€ä¹ˆä½¿ç”¨ Docker è¿›è¡Œç¯å¢ƒéš”ç¦»
- ä»é›¶æ­å»ºå®Œæ•´çš„ LLM æ¨ç†ç¯å¢ƒ
- å¿«é€Ÿå¯åŠ¨ä½ çš„ç¬¬ä¸€ä¸ª vLLM æ¨ç†æœåŠ¡
- æŒæ¡å®¹å™¨åŒ–éƒ¨ç½²çš„æœ€ä½³å®è·µ
- å­¦ä¼šæ’æŸ¥å¸¸è§çš„ç¯å¢ƒé—®é¢˜

**å­¦å®Œæœ¬ç« ,ä½ å°†æ‹¥æœ‰ä¸€ä¸ªå¯ä»¥ç«‹å³ç”¨äºç”Ÿäº§çš„æ¨ç†ç¯å¢ƒã€‚**

---

## 4.1 å¼€å‘ç¯å¢ƒæ¦‚è§ˆ

### 4.1.1 ä¸ºä»€ä¹ˆä½¿ç”¨ Docker

ä½ å¯èƒ½å¬è¿‡è¿™æ ·çš„è¯:"åœ¨æˆ‘æœºå™¨ä¸Šèƒ½è¿è¡Œ,ä¸ºä»€ä¹ˆåœ¨ä½ é‚£å°±ä¸è¡Œ?"

**ä¼ ç»Ÿæ–¹å¼çš„é—®é¢˜**:
```
å·¥ç¨‹å¸ˆ A çš„æœºå™¨:
- Ubuntu 20.04
- CUDA 11.8
- Python 3.9
- PyTorch 2.0.1

å·¥ç¨‹å¸ˆ B çš„æœºå™¨:
- Ubuntu 22.04
- CUDA 12.1
- Python 3.10
- PyTorch 2.1.0

ç»“æœ:
â†’ åŒæ ·çš„ä»£ç ,ä¸åŒçš„ç»“æœ
â†’ éš¾ä»¥å¤ç° bug
â†’ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å™©æ¢¦
```

**Docker çš„è§£å†³æ–¹æ¡ˆ**:
```
Docker å®¹å™¨:
- å›ºå®šçš„åŸºç¡€é•œåƒ
- ï¿½è£…çš„ CUDA ç‰ˆæœ¬
- é”å®šçš„ä¾èµ–ç‰ˆæœ¬
- æ ‡å‡†çš„è¿è¡Œç¯å¢ƒ

ç»“æœ:
â†’ ä»»ä½•æœºå™¨,åŒæ ·çš„è¡Œä¸º
â†’ æ˜“äºå¤ç°å’Œè°ƒè¯•
â†’ ä¸€é”®éƒ¨ç½²åˆ°ç”Ÿäº§
```

**å•†ä¸šä»·å€¼**:
- å‡å°‘ 80% çš„ç¯å¢ƒç›¸å…³ bug
- æ–°äººä¸Šæ‰‹æ—¶é—´ä» 2 å¤©é™åˆ° 30 åˆ†é’Ÿ
- éƒ¨ç½²æ—¶é—´ä»æ•°å°æ—¶é™åˆ°æ•°åˆ†é’Ÿ

---

### 4.1.2 ç¯å¢ƒä¸€è‡´æ€§: æœ¬åœ° vs ç”Ÿäº§

**ä¸‰å±‚ç¯å¢ƒä¸€è‡´æ€§**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¼€å‘ç¯å¢ƒ (Development)                  â”‚
â”‚  - ä½ çš„ç¬”è®°æœ¬ç”µè„‘                        â”‚
â”‚  - å¿«é€Ÿè¿­ä»£,é¢‘ç¹é‡å¯                    â”‚
â”‚  - ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Docker é•œåƒå¤ç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æµ‹è¯•ç¯å¢ƒ (Staging)                     â”‚
â”‚  - ä¸ç”Ÿäº§ç›¸åŒçš„é…ç½®                     â”‚
â”‚  - çœŸå®è´Ÿè½½æµ‹è¯•                         â”‚
â”‚  - éªŒè¯æ€§èƒ½å’Œç¨³å®šæ€§                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ åŒä¸€ä¸ª Docker é•œåƒ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç”Ÿäº§ç¯å¢ƒ (Production)                  â”‚
â”‚  - äº‘ç«¯ GPU å®ä¾‹                        â”‚
â”‚  - é«˜å¯ç”¨éƒ¨ç½²                           â”‚
â”‚  - ç›‘æ§å’Œå‘Šè­¦                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®åŸåˆ™**:
1. **å¼€å‘å®¹å™¨åŒ–**: ä»ç¬¬ä¸€å¤©å¼€å§‹å°±ç”¨ Docker
2. **ç‰ˆæœ¬é”å®š**: ä½¿ç”¨ `requirements.txt` æˆ– `pyproject.toml` é”å®šä¾èµ–
3. **é…ç½®å¤–éƒ¨åŒ–**: ç¯å¢ƒå˜é‡ã€é…ç½®æ–‡ä»¶ä¸è¦ç¡¬ç¼–ç 
4. **æœ€å°æƒé™**: ç”Ÿäº§å®¹å™¨ä¸è¦åŒ…å«å¼€å‘å·¥å…·

---

### 4.1.3 å®Œæ•´æŠ€æœ¯æ ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åº”ç”¨å±‚ (Application Layer)                        â”‚
â”‚  - FastAPI / Flask (API æœåŠ¡)                     â”‚
â”‚  - vLLM / SGLang (æ¨ç†å¼•æ“)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ¡†æ¶å±‚ (Framework Layer)                          â”‚
â”‚  - PyTorch / TensorFlow (æ·±åº¦å­¦ä¹ æ¡†æ¶)            â”‚
â”‚  - Transformers (æ¨¡å‹åº“)                           â”‚
â”‚  - Hugging Face Hub (æ¨¡å‹ä¸‹è½½)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è¿è¡Œæ—¶å±‚ (Runtime Layer)                          â”‚
â”‚  - Python 3.8+                                     â”‚
â”‚  - CUDA 12.x                                       â”‚
â”‚  - cuDNN / cuBLAS (CUDA åŠ é€Ÿåº“)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é©±åŠ¨å±‚ (Driver Layer)                             â”‚
â”‚  - NVIDIA Driver (525+)                            â”‚
â”‚  - GPU ç¡¬ä»¶ (A100 / H100 / RTX 4090)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å®¹å™¨å±‚ (Container Layer)                          â”‚
â”‚  - Docker                                          â”‚
â”‚  - NVIDIA Container Toolkit                       â”‚
â”‚  - Docker Compose                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ¯ä¸€å±‚éƒ½å¾ˆé‡è¦**:
- åº”ç”¨å±‚: ä½ çš„ä¸šåŠ¡é€»è¾‘
- æ¡†æ¶å±‚: æ¨ç†å¼•æ“çš„åŸºç¡€
- è¿è¡Œæ—¶å±‚: Python å’Œ CUDA çš„ç‰ˆæœ¬å…¼å®¹æ€§
- é©±åŠ¨å±‚: å¿…é¡»ä¸ GPU ç¡¬ä»¶åŒ¹é…
- å®¹å™¨å±‚: éš”ç¦»å’Œå¯ç§»æ¤æ€§

---

## 4.2 åŸºç¡€ç¯å¢ƒå®‰è£…

### 4.2.1 NVIDIA é©±åŠ¨å®‰è£…

**æ£€æŸ¥å½“å‰é©±åŠ¨ç‰ˆæœ¬**:

```bash
nvidia-smi
```

ä½ åº”è¯¥çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è¾“å‡º:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05   Driver Version: 535.104.05   CUDA Version: 12.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:00:04.0 Off |                    0 |
| N/A   32C    P0    54W / 400W |  18939MiB / 81920MiB |     28%      Default |
+-------------------------------+----------------------+----------------------+
```

**å…³é”®ä¿¡æ¯**:
- **Driver Version**: è‡³å°‘ 525+ (æ¨è 535+)
- **CUDA Version**: è¿™æ˜¯æœ€é«˜çš„ CUDA ç‰ˆæœ¬æ”¯æŒ,ä¸ä¸€å®šæ˜¯å·²å®‰è£…çš„ç‰ˆæœ¬

**å¦‚æœé©±åŠ¨ç‰ˆæœ¬è¿‡ä½æˆ–æœªå®‰è£…**:

**Ubuntu/Debian**:
```bash
# æ·»åŠ  NVIDIA ä»“åº“
sudo apt-get update
sudo apt-get install -y ca-certificates curl gnupg

distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

# å®‰è£…é©±åŠ¨
sudo apt-get update
sudo apt-get install -y nvidia-driver-535

# é‡å¯
sudo reboot
```

**CentOS/RHEL**:
```bash
# æ·»åŠ  NVIDIA ä»“åº“
sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm
sudo yum install -y https://nvidia.github.io/libnvidia-container/rhel8/nvidia-container-toolkit.repo

# å®‰è£…é©±åŠ¨
sudo yum install -y nvidia-driver

# é‡å¯
sudo reboot
```

**äº‘å¹³å° (AWS/GCP/Azure)**:
- é€šå¸¸å·²ç»é¢„è£… NVIDIA é©±åŠ¨
- ä½¿ç”¨å®˜æ–¹çš„ GPU ä¼˜åŒ– AMI/Image

---

### 4.2.2 CUDA Toolkit é…ç½®

**é‡è¦è¯´æ˜**: Docker å®¹å™¨ä¸­çš„ CUDA ä¸éœ€è¦å®¿ä¸»æœºå®‰è£… CUDA Toolkit!

**ä¸ºä»€ä¹ˆ?**
```
å®¿ä¸»æœº:
- åªéœ€è¦ NVIDIA é©±åŠ¨
- é©±åŠ¨æä¾› GPU è®¿é—®èƒ½åŠ›

Docker å®¹å™¨:
- åŒ…å« CUDA Toolkit
- åŒ…å« CUDA è¿è¡Œæ—¶åº“
- éš”ç¦»çš„ CUDA ç‰ˆæœ¬
```

**æœ€ä½³å®è·µ**:
- âœ… å®¿ä¸»æœº: åªå®‰è£… NVIDIA é©±åŠ¨
- âœ… Docker å®¹å™¨: ä½¿ç”¨å¸¦ CUDA çš„åŸºç¡€é•œåƒ
- âŒ é¿å…åœ¨å®¿ä¸»æœºå®‰è£…å¤šä¸ª CUDA ç‰ˆæœ¬

**å¦‚æœä½ ç¡®å®éœ€è¦åœ¨å®¿ä¸»æœºå®‰è£… CUDA** (ä¾‹å¦‚æœ¬åœ°å¼€å‘):

```bash
# ä» NVIDIA å®˜ç½‘ä¸‹è½½ CUDA Toolkit
# https://developer.nvidia.com/cuda-downloads

# å®‰è£… (Ubuntu ç¤ºä¾‹)
wget https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux.run
sudo sh cuda_12.2.0_535.54.03_linux.run --toolkit --silent

# é…ç½®ç¯å¢ƒå˜é‡
echo 'export PATH=/usr/local/cuda-12.2/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# éªŒè¯
nvcc --version
```

---

### 4.2.3 Docker ä¸ NVIDIA Container Toolkit

**å®‰è£… Docker**:

```bash
# Ubuntu/Debian
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# å°†å½“å‰ç”¨æˆ·æ·»åŠ åˆ° docker ç»„
sudo usermod -aG docker $USER

# é‡æ–°ç™»å½•æˆ–è¿è¡Œ
newgrp docker

# éªŒè¯
docker --version
docker run hello-world
```

**å®‰è£… NVIDIA Container Toolkit**:

```bash
# æ·»åŠ  NVIDIA ä»“åº“
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

# å®‰è£…
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# é…ç½® Docker
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# éªŒè¯
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

å¦‚æœæˆåŠŸ,ä½ åº”è¯¥çœ‹åˆ° `nvidia-smi` çš„è¾“å‡ºã€‚

---

### 4.2.4 Python ç¯å¢ƒç®¡ç†

**æ¨èæ–¹å¼**: ä½¿ç”¨ pyenv æˆ– conda ç®¡ç†å¤šä¸ª Python ç‰ˆæœ¬

**ä½¿ç”¨ pyenv** (æ¨è):

```bash
# å®‰è£… pyenv
curl https://pyenv.run | bash

# æ·»åŠ åˆ° shell
echo 'export PYENV_ROOT="$HOME/.pyenv"' >> ~/.bashrc
echo '[[ -d $PYENV_ROOT/bin ]] && export PATH="$PYENV_ROOT/bin:$PATH"' >> ~/.bashrc
echo 'eval "$(pyenv init -)"' >> ~/.bashrc
source ~/.bashrc

# å®‰è£… Python 3.10
pyenv install 3.10.12
pyenv global 3.10.12

# éªŒè¯
python --version
```

**ä½¿ç”¨ conda** (å¯é€‰):

```bash
# ä¸‹è½½ Miniconda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n llm-inference python=3.10
conda activate llm-inference
```

**ä½¿ç”¨ venv** (Docker å†…æ¨è):

```bash
# åœ¨ Dockerfile ä¸­
python3 -m venv /opt/venv
source /opt/venv/bin/activate
pip install --upgrade pip
```

---

## 4.3 vLLM å¿«é€Ÿå…¥é—¨

### 4.3.1 ä»€ä¹ˆæ˜¯ vLLM

**vLLM** æ˜¯ç›®å‰æœ€æµè¡Œçš„å¼€æº LLM æ¨ç†å¼•æ“ä¹‹ä¸€,ç”± UC Berkeley çš„å›¢é˜Ÿå¼€å‘ã€‚

**æ ¸å¿ƒç‰¹æ€§**:
- âš¡ **é«˜æ€§èƒ½**: PagedAttention ç®—æ³•,ååé‡æ¯” HuggingFace Transformers é«˜ 24 å€
- ğŸš€ **è¿ç»­æ‰¹å¤„ç†**: Continuous Batching,æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡
- ğŸ¯ **æ˜“ç”¨æ€§**: å…¼å®¹ OpenAI API,ä¸€è¡Œä»£ç å¯åŠ¨æœåŠ¡
- ğŸ”§ **çµæ´»æ€§**: æ”¯æŒå¤šç§é‡åŒ–æ ¼å¼ã€æŠ•æœºè§£ç ã€å‰ç¼€ç¼“å­˜

**é€‚ç”¨åœºæ™¯**:
- âœ… é«˜ååé‡æ¨ç†æœåŠ¡
- âœ… å¤šæ¨¡å‹å¹¶å‘éƒ¨ç½²
- âœ… éœ€è¦ä½å»¶è¿Ÿçš„å®æ—¶åº”ç”¨
- âœ… ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

**ä¸é€‚ç”¨åœºæ™¯**:
- âŒ ç ”ç©¶å’Œå®éªŒ (å»ºè®®ä½¿ç”¨ Transformers)
- âŒ éœ€è¦æœ€å¤§åŒ–çš„æ¨¡å‹çµæ´»æ€§
- âŒ è¶…å¤§æ¨¡å‹çš„æ¨¡å‹å¹¶è¡Œ (vLLM æ”¯æŒæœ‰é™)

---

### 4.3.2 vLLM vs å…¶ä»–æ¨ç†æ¡†æ¶

| ç‰¹æ€§ | vLLM | SGLang | TensorRT-LLM | Transformers |
|------|------|--------|--------------|--------------|
| **æ€§èƒ½** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­ |
| **æ˜“ç”¨æ€§** | â­â­â­â­â­ | â­â­â­â­ | â­â­ | â­â­â­â­â­ |
| **ç”Ÿæ€** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **OpenAI API** | âœ… | âœ… | âŒ | âŒ |
| **ç”Ÿäº§å°±ç»ª** | âœ… | âš ï¸ | âœ… | âŒ |
| **å­¦ä¹ æ›²çº¿** | ä½ | ä¸­ | é«˜ | ä½ |

**é€‰æ‹©å»ºè®®**:
- **vLLM**: å¤§å¤šæ•°åœºæ™¯çš„é¦–é€‰,æ€§èƒ½ä¸æ˜“ç”¨æ€§çš„æœ€ä½³å¹³è¡¡
- **SGLang**: éœ€è¦ç»“æ„åŒ–ç”Ÿæˆæˆ–é«˜çº§è°ƒåº¦åŠŸèƒ½
- **TensorRT-LLM**: æè‡´æ€§èƒ½è¦æ±‚,æ„¿æ„æŠ•å…¥æ—¶é—´ä¼˜åŒ–
- **Transformers**: å¿«é€ŸåŸå‹,å­¦ä¹ ç ”ç©¶

---

### 4.3.3 å®‰è£… vLLM

**æ–¹å¼ 1: pip å®‰è£…** (æ¨èç”¨äºå¼€å‘):

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£… vLLM
pip install vllm

# éªŒè¯å®‰è£…
python -c "import vllm; print(vllm.__version__)"
```

**æ–¹å¼ 2: ä»æºç å®‰è£…** (ç”¨äºå¼€å‘æˆ–æœ€æ–°åŠŸèƒ½):

```bash
git clone https://github.com/vllm-project/vllm.git
cd vllm

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£… vLLM (å¯ç¼–è¾‘æ¨¡å¼)
pip install -e .
```

**æ–¹å¼ 3: Docker é•œåƒ** (æ¨èç”¨äºç”Ÿäº§):

```bash
# æ‹‰å–å®˜æ–¹é•œåƒ
docker pull vllm/vllm-openai:latest

# æˆ–è€…ä½¿ç”¨æœ¬ä¹¦æä¾›çš„é•œåƒ
docker pull your-registry/llm-inference-book:latest
```

---

### 4.3.4 å¯åŠ¨ç¬¬ä¸€ä¸ªæ¨ç†æœåŠ¡

**æœ€ç®€å•çš„å¯åŠ¨æ–¹å¼**:

```bash
# OpenAI API å…¼å®¹æœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --host 0.0.0.0 \
    --port 8000
```

**ä½¿ç”¨ Docker**:

```bash
docker run --gpus all \
    --shm-size 10g \
    -p 8000:8000 \
    vllm/vllm-openai:latest \
    --model meta-llama/Llama-2-7b-chat-hf
```

**æµ‹è¯•æ¨ç†æœåŠ¡**:

```bash
# ä½¿ç”¨ curl
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ]
  }'

# ä½¿ç”¨ Python
import openai

# é…ç½®æœ¬åœ°ç«¯ç‚¹
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "dummy"  # vLLM ä¸éªŒè¯ key

response = openai.ChatCompletion.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[
        {"role": "user", "content": "Hello, how are you?"}
    ]
)

print(response.choices[0].message.content)
```

**é‡è¦å¯åŠ¨å‚æ•°**:

```bash
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \  # æ¨¡å‹åç§°æˆ–è·¯å¾„
    --tensor-parallel-size 2 \                # å¼ é‡å¹¶è¡Œåº¦ (å¤šGPU)
    --gpu-memory-utilization 0.9 \            # GPU å†…å­˜åˆ©ç”¨ç‡ (0-1)
    --max-model-len 4096 \                    # æœ€å¤§åºåˆ—é•¿åº¦
    --dtype half \                            # æ•°æ®ç±»å‹ (half, bfloat16)
    --quantization awq \                      # é‡åŒ–æ ¼å¼ (awq, gptq, squeezellm)
    --host 0.0.0.0 \                          # ç›‘å¬åœ°å€
    --port 8000                               # ç›‘å¬ç«¯å£
```

---

## 4.4 Docker å®¹å™¨åŒ–éƒ¨ç½²

### 4.4.1 Dockerfile ç¼–å†™

**åŸºç¡€ç‰ˆ Dockerfile**:

```dockerfile
# åŸºç¡€é•œåƒ: åŒ…å« CUDA 12.1
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

# å®‰è£… Python å’ŒåŸºç¡€å·¥å…·
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£… vLLM
RUN pip3 install --no-cache-dir vllm

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", \
     "--host", "0.0.0.0", \
     "--port", "8000"]
```

**ç”Ÿäº§çº§ Dockerfile** (å¤šé˜¶æ®µæ„å»º):

```dockerfile
# ==========================================
# é˜¶æ®µ 1: æ„å»ºé˜¶æ®µ
# ==========================================
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS builder

# å®‰è£…æ„å»ºä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
RUN python3 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# å‡çº§ pip
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# ==========================================
# é˜¶æ®µ 2: è¿è¡Œé˜¶æ®µ
# ==========================================
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

# åªå®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-venv \
    && rm -rf /var/lib/apt/lists/*

# ä»æ„å»ºé˜¶æ®µå¤åˆ¶è™šæ‹Ÿç¯å¢ƒ
COPY --from=builder /opt/venv /opt/venv

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PATH="/opt/venv/bin:$PATH"
ENV PYTHONUNBUFFERED=1
ENV TF_CPP_MIN_LOG_LEVEL=3

# åˆ›å»ºé root ç”¨æˆ·
RUN useradd -m -u 1000 appuser

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY --chown=appuser:appuser . .

# åˆ‡æ¢åˆ°é root ç”¨æˆ·
USER appuser

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", \
     "--host", "0.0.0.0", \
     "--port", "8000", \
     "--model", "${MODEL_PATH}"]
```

**requirements.txt**:

```txt
vllm==0.6.0
torch==2.3.0
transformers==4.41.0
accelerate==0.30.0
fastapi==0.111.0
uvicorn[standard]==0.29.0
pydantic==2.7.0
```

---

### 4.4.2 Docker Compose é…ç½®

**docker-compose.yml**:

```yaml
version: '3.8'

services:
  vllm-server:
    build:
      context: .
      dockerfile: Dockerfile
    image: llm-inference:latest
    container_name: vllm-server

    # GPU é…ç½®
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # ç¯å¢ƒå˜é‡
    environment:
      - MODEL_PATH=meta-llama/Llama-2-7b-chat-hf
      - GPU_MEMORY_UTILIZATION=0.9
      - MAX_MODEL_LEN=4096
      - NUM_GPU=1

    # ç«¯å£æ˜ å°„
    ports:
      - "8000:8000"

    # å…±äº«å†…å­˜
    shm_size: '10g'

    # æ•°æ®å·
    volumes:
      - model-cache:/root/.cache/huggingface
      - logs:/app/logs

    # ç½‘ç»œ
    networks:
      - llm-network

    # é‡å¯ç­–ç•¥
    restart: unless-stopped

    # å¥åº·æ£€æŸ¥
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    # æ—¥å¿—
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # å¯é€‰: Nginx åå‘ä»£ç†
  nginx:
    image: nginx:alpine
    container_name: nginx-proxy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    networks:
      - llm-network
    depends_on:
      - vllm-server
    restart: unless-stopped

  # å¯é€‰: Prometheus ç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    networks:
      - llm-network
    restart: unless-stopped

  # å¯é€‰: Grafana å¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    networks:
      - llm-network
    restart: unless-stopped

# æ•°æ®å·
volumes:
  model-cache:
  logs:
  prometheus-data:
  grafana-data:

# ç½‘ç»œ
networks:
  llm-network:
    driver: bridge
```

**å¯åŠ¨æœåŠ¡**:

```bash
# æ„å»ºå¹¶å¯åŠ¨
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f vllm-server

# åœæ­¢æœåŠ¡
docker-compose down

# åœæ­¢å¹¶åˆ é™¤æ•°æ®å·
docker-compose down -v
```

---

### 4.4.3 å¤šé˜¶æ®µæ„å»ºä¼˜åŒ–

**ä¸ºä»€ä¹ˆè¦å¤šé˜¶æ®µæ„å»º?**

```
å•é˜¶æ®µæ„å»º:
â”œâ”€â”€ åŸºç¡€é•œåƒ: 5GB
â”œâ”€â”€ æ„å»ºå·¥å…·: 2GB
â”œâ”€â”€ æºä»£ç : 500MB
â”œâ”€â”€ ç¼–è¯‘äº§ç‰©: 3GB
â””â”€â”€ æœ€ç»ˆé•œåƒ: 10.5GB âŒ

å¤šé˜¶æ®µæ„å»º:
â”Œâ”€ æ„å»ºé˜¶æ®µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åŸºç¡€é•œåƒ: 5GB              â”‚
â”‚ æ„å»ºå·¥å…·: 2GB              â”‚
â”‚ æºä»£ç : 500MB              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ åªå¤åˆ¶ç¼–è¯‘äº§ç‰©
â”Œâ”€ è¿è¡Œé˜¶æ®µ â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åŸºç¡€é•œåƒ: 5GB              â”‚
â”‚ ç¼–è¯‘äº§ç‰©: 3GB              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ æœ€ç»ˆé•œåƒ: 8GB âœ…           â”‚
```

**ä¼˜åŠ¿**:
- âœ… æ›´å°çš„é•œåƒä½“ç§¯ (èŠ‚çœå­˜å‚¨å’Œä¼ è¾“)
- âœ… æ›´é«˜çš„å®‰å…¨æ€§ (ä¸åŒ…å«æºä»£ç å’Œæ„å»ºå·¥å…·)
- âœ… æ›´å¿«çš„éƒ¨ç½²é€Ÿåº¦

---

### 4.4.4 æ•°æ®å·ç®¡ç†

**ä¸‰ç§æŒ‚è½½æ–¹å¼**:

```yaml
volumes:
  # 1. å‘½åå· (Docker ç®¡ç†)
  - model-cache:/root/.cache/huggingface

  # 2. ç»‘å®šæŒ‚è½½ (å®¿ä¸»æœºè·¯å¾„)
  - /path/on/host:/path/in/container

  # 3. ä¸´æ—¶å· (tmpfs)
  - tmpfs-data:/tmp:rw,size=1g
```

**æœ€ä½³å®è·µ**:

```yaml
volumes:
  # æ¨¡å‹ç¼“å­˜ (æŒä¹…åŒ–)
  - model-cache:/root/.cache/huggingface

  # æ—¥å¿— (æŒä¹…åŒ–)
  - ./logs:/app/logs

  # é…ç½®æ–‡ä»¶ (åªè¯»)
  - ./config:/app/config:ro

  # ä¸´æ—¶æ–‡ä»¶ (å†…å­˜)
  - /tmp:rw,size=1g
```

---

## 4.5 åŸºç¡€æ¨ç†ç¤ºä¾‹

### 4.5.1 å•æ¬¡æ¨ç†

**Python API**:

```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    gpu_memory_utilization=0.9,
    max_model_len=4096,
)

# é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=256,
)

# è¾“å…¥æ–‡æœ¬
prompts = [
    "Hello, my name is",
    "The future of AI is",
]

# æ¨ç†
outputs = llm.generate(prompts, sampling_params)

# æ‰“å°ç»“æœ
for i, output in enumerate(outputs):
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt {i}: {prompt}")
    print(f"Generated: {generated_text}\n")
```

**OpenAI API**:

```python
import openai

# é…ç½®æœ¬åœ°ç«¯ç‚¹
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "dummy"

response = openai.ChatCompletion.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ],
    temperature=0.7,
    max_tokens=100,
)

print(response.choices[0].message.content)
```

---

### 4.5.2 æ‰¹é‡æ¨ç†

**Python API**:

```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=2,  # ä½¿ç”¨ 2 ä¸ª GPU
)

# æ‰¹é‡è¾“å…¥
prompts = [
    "Write a short story about a robot.",
    "Explain quantum computing.",
    "What is the meaning of life?",
    "Describe the perfect day.",
    "How does the internet work?",
]

# é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    n=1,  # æ¯ä¸ª prompt ç”Ÿæˆ 1 ä¸ªç»“æœ
    temperature=0.8,
    top_p=0.95,
    max_tokens=256,
)

# æ‰¹é‡æ¨ç†
outputs = llm.generate(prompts, sampling_params)

# ä¿å­˜ç»“æœ
results = []
for output in outputs:
    results.append({
        "prompt": output.prompt,
        "generated": output.outputs[0].text,
        "tokens": len(output.outputs[0].token_ids),
    })

# æ‰“å°ç»Ÿè®¡
import json
print(json.dumps(results, indent=2, ensure_ascii=False))
```

**æ€§èƒ½ä¼˜åŒ–å»ºè®®**:
- âœ… ä½¿ç”¨æ›´å¤§çš„ batch size æé«˜ååé‡
- âœ… é¢„å¤„ç† prompt,å‡å°‘è¿è¡Œæ—¶å¼€é”€
- âœ… ä½¿ç”¨å¼‚æ­¥ API å¤„ç†å¤§é‡è¯·æ±‚

---

### 4.5.3 æµå¼è¾“å‡º

**æœåŠ¡å™¨ç«¯é…ç½®**:

```python
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.sampling_params import SamplingParams

# å¯ç”¨æµå¼è¾“å‡º
engine_args = AsyncEngineArgs(
    model="meta-llama/Llama-2-7b-chat-hf",
    enable_prefix_caching=True,
)

engine = AsyncLLMEngine.from_engine_args(engine_args)
```

**å®¢æˆ·ç«¯ä½¿ç”¨**:

```python
import asyncio
from openai import AsyncOpenAI

async def stream_chat():
    client = AsyncOpenAI(
        base_url="http://localhost:8000/v1",
        api_key="dummy",
    )

    stream = await client.chat.completions.create(
        model="meta-llama/Llama-2-7b-chat-hf",
        messages=[
            {"role": "user", "content": "Tell me a long story."}
        ],
        stream=True,
        max_tokens=500,
    )

    async for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)

# è¿è¡Œ
asyncio.run(stream_chat())
```

**curl ç¤ºä¾‹**:

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": true
  }'
```

---

### 4.5.4 æ€§èƒ½åŸºå‡†æµ‹è¯•

**ç®€å•çš„åŸºå‡†æµ‹è¯•è„šæœ¬**:

```python
import time
import numpy as np
from vllm import LLM, SamplingParams

def benchmark(llm, prompts, sampling_params, num_iterations=10):
    latencies = []

    for _ in range(num_iterations):
        start_time = time.time()

        outputs = llm.generate(prompts, sampling_params)

        end_time = time.time()
        latencies.append(end_time - start_time)

    # ç»Ÿè®¡
    latencies = np.array(latencies)
    print(f"å¹³å‡å»¶è¿Ÿ: {np.mean(latencies):.3f} ç§’")
    print(f"P50 å»¶è¿Ÿ: {np.percentile(latencies, 50):.3f} ç§’")
    print(f"P99 å»¶è¿Ÿ: {np.percentile(latencies, 99):.3f} ç§’")
    print(f"ååé‡: {len(prompts) / np.mean(latencies):.2f} è¯·æ±‚/ç§’")

# è¿è¡ŒåŸºå‡†æµ‹è¯•
llm = LLM(model="meta-llama/Llama-2-7b-chat-hf")

sampling_params = SamplingParams(
    temperature=0.8,
    max_tokens=128,
)

prompts = ["Hello, world!"] * 32  # æ‰¹é‡ 32 ä¸ªè¯·æ±‚

benchmark(llm, prompts, sampling_params)
```

**ä½¿ç”¨ Apache Bench**:

```bash
# å®‰è£… ab
sudo apt-get install apache2-utils

# è¿è¡ŒåŸºå‡†æµ‹è¯•
ab -n 1000 -c 10 -T 'application/json' \
  -p request.json \
  http://localhost:8000/v1/chat/completions
```

**request.json**:
```json
{
  "model": "meta-llama/Llama-2-7b-chat-hf",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ]
}
```

---

## 4.6 å¼€å‘å·¥å…·æ¨è

### 4.6.1 ä»£ç ç¼–è¾‘å™¨é…ç½®

**VS Code** (æ¨è):

**æ¨èæ’ä»¶**:
- Python
- Pylance
- Docker
- Jupyter
- GitLens
- Thunder Client (API æµ‹è¯•)

**VS Code é…ç½®** (`.vscode/settings.json`):

```json
{
  "python.defaultInterpreterPath": "/opt/venv/bin/python",
  "python.linting.enabled": true,
  "python.linting.pylintEnabled": true,
  "python.formatting.provider": "black",
  "python.testing.pytestEnabled": true,
  "editor.formatOnSave": true,
  "editor.codeActionsOnSave": {
    "source.organizeImports": true
  },
  "files.exclude": {
    "**/__pycache__": true,
    "**/*.pyc": true
  }
}
```

**PyCharm**:
- å†…ç½®å¼ºå¤§çš„ Python æ”¯æŒ
- Docker é›†æˆ
- æ€§èƒ½åˆ†æå·¥å…·

---

### 4.6.2 è°ƒè¯•å·¥å…·

**Python è°ƒè¯•å™¨**:

```python
# ä½¿ç”¨ pdb
import pdb; pdb.set_trace()

# ä½¿ç”¨ ipdb (æ›´å‹å¥½)
import ipdb; ipdb.set_trace()

# VS Code è°ƒè¯•é…ç½®
# .vscode/launch.json
{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Python: Current File",
      "type": "debugpy",
      "request": "launch",
      "program": "${file}",
      "console": "integratedTerminal",
      "env": {
        "CUDA_VISIBLE_DEVICES": "0"
      }
    }
  ]
}
```

**NVIDIA Nsight** (GPU æ€§èƒ½åˆ†æ):
```bash
# å®‰è£… Nsight Systems
sudo apt-get install nsight-systems

# åˆ†æ GPU æ€§èƒ½
nsys profile python your_script.py

# æŸ¥çœ‹ç»“æœ
nsys stats report.nsys-rep
```

---

### 4.6.3 æ€§èƒ½åˆ†æå·¥å…·

**nvtop** (GPU ç›‘æ§):

```bash
# å®‰è£…
sudo apt-get install nvtop

# è¿è¡Œ
nvtop
```

**GPUtil** (Python):

```python
import GPUtil
GPUtil.showUtilization()
```

**è‡ªå®šä¹‰ç›‘æ§è„šæœ¬**:

```python
import time
import pynvml

pynvml.nvmlInit()
handle = pynvml.nvmlDeviceGetHandleByIndex(0)

while True:
    # GPU åˆ©ç”¨ç‡
    util = pynvml.nvmlDeviceGetUtilizationRates(handle)
    print(f"GPU åˆ©ç”¨ç‡: {util.gpu}%")

    # å†…å­˜ä½¿ç”¨
    info = pynvml.nvmlDeviceGetMemoryInfo(handle)
    print(f"å†…å­˜: {info.used / 1024**3:.2f}GB / {info.total / 1024**3:.2f}GB")

    # æ¸©åº¦
    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
    print(f"æ¸©åº¦: {temp}Â°C")

    # åŠŸè€—
    power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000
    print(f"åŠŸè€—: {power}W")

    print("-" * 40)
    time.sleep(1)
```

---

### 4.6.4 å¯è§†åŒ–å·¥å…·

**TensorBoard**:

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter()

# è®°å½•æŒ‡æ ‡
writer.add_scalar('Latency', latency, step)
writer.add_scalar('Throughput', throughput, step)
writer.add_scalar('GPU_Memory', gpu_memory, step)

# å¯åŠ¨ TensorBoard
# tensorboard --logdir runs
```

**Grafana + Prometheus**:

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'vllm'
    static_configs:
      - targets: ['localhost:8000']
```

---

## 4.7 å¸¸è§é—®é¢˜æ’æŸ¥

### 4.7.1 CUDA ç‰ˆæœ¬ä¸å…¼å®¹

**é—®é¢˜**: `CUDA_ERROR_INVALID_DEVICE`

**åŸå› **: é©±åŠ¨ç‰ˆæœ¬ä¸ CUDA ç‰ˆæœ¬ä¸åŒ¹é…

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. æ£€æŸ¥é©±åŠ¨ç‰ˆæœ¬
nvidia-smi

# 2. æ£€æŸ¥å®¹å™¨å†… CUDA ç‰ˆæœ¬
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# 3. ä½¿ç”¨å…¼å®¹çš„ Docker é•œåƒ
# CUDA 12.1 éœ€è¦ Driver >= 525
# CUDA 11.8 éœ€è¦ Driver >= 450
```

**ç‰ˆæœ¬å…¼å®¹è¡¨**:
| CUDA ç‰ˆæœ¬ | æœ€ä½é©±åŠ¨ç‰ˆæœ¬ |
|-----------|-------------|
| 12.x      | 525+        |
| 11.x      | 450+        |
| 10.x      | 410+        |

---

### 4.7.2 Docker GPU è®¿é—®é—®é¢˜

**é—®é¢˜**: `could not select device driver`

**åŸå› **: NVIDIA Container Toolkit é…ç½®ä¸æ­£ç¡®

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. æ£€æŸ¥ Docker è¿è¡Œæ—¶
docker info | grep nvidia

# 2. é‡æ–°é…ç½®
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# 3. æµ‹è¯•
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# 4. å¦‚æœè¿˜ä¸è¡Œ,æ£€æŸ¥é»˜è®¤è¿è¡Œæ—¶
# ç¼–è¾‘ /etc/docker/daemon.json
{
  "default-runtime": "nvidia",
  "runtimes": {
    "nvidia": {
      "path": "nvidia-container-runtime",
      "runtimeArgs": []
    }
  }
}

sudo systemctl restart docker
```

---

### 4.7.3 ç«¯å£å†²çªå¤„ç†

**é—®é¢˜**: `port is already allocated`

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. æŸ¥çœ‹å ç”¨ç«¯å£çš„è¿›ç¨‹
sudo lsof -i :8000

# 2. æ€æ‰å ç”¨ç«¯å£çš„è¿›ç¨‹
sudo kill -9 <PID>

# 3. æˆ–è€…ä½¿ç”¨å…¶ä»–ç«¯å£
docker run --gpus all -p 8001:8000 vllm/vllm-openai:latest
```

---

### 4.7.4 ä¾èµ–å®‰è£…å¤±è´¥

**é—®é¢˜**: pip å®‰è£…å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. å‡çº§ pip
pip install --upgrade pip

# 2. æ¸…ç†ç¼“å­˜
pip cache purge

# 3. ä½¿ç”¨é¢„ç¼–è¯‘åŒ…
pip install --only-binary :all: vllm

# 4. å¦‚æœè¿˜æ˜¯å¤±è´¥,ä½¿ç”¨ conda
conda install -c conda-forge vllm

# 5. æ£€æŸ¥ Python ç‰ˆæœ¬ (éœ€è¦ 3.8+)
python --version
```

---

## âœ… ç« èŠ‚æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬ç« å,ä½ åº”è¯¥èƒ½å¤Ÿ:

- [ ] ç†è§£ä¸ºä»€ä¹ˆä½¿ç”¨ Docker è¿›è¡Œç¯å¢ƒéš”ç¦»
- [ ] åœ¨æœ¬åœ°æ­å»ºå®Œæ•´çš„ LLM æ¨ç†ç¯å¢ƒ
- [ ] ä½¿ç”¨ vLLM å¯åŠ¨æ¨ç†æœåŠ¡
- [ ] ç¼–å†™ç”Ÿäº§çº§çš„ Dockerfile å’Œ docker-compose.yml
- [ ] ä½¿ç”¨ OpenAI API å…¼å®¹çš„æ¥å£è¿›è¡Œæ¨ç†
- [ ] æ’æŸ¥å¸¸è§çš„ç¯å¢ƒé—®é¢˜

---

## ğŸ“š åŠ¨æ‰‹ç»ƒä¹ 

**ç»ƒä¹  4.1**: ä»é›¶æ­å»º vLLM å¼€å‘ç¯å¢ƒ

1. å®‰è£… Docker å’Œ NVIDIA Container Toolkit
2. æ‹‰å– vLLM Docker é•œåƒ
3. å¯åŠ¨ Llama-2-7b æ¨ç†æœåŠ¡
4. ä½¿ç”¨ curl å‘é€æµ‹è¯•è¯·æ±‚
5. éªŒè¯æœåŠ¡æ­£å¸¸å·¥ä½œ

**ç»ƒä¹  4.2**: Docker åŒ–ä¸€ä¸ªæ¨ç†æœåŠ¡

1. ç¼–å†™ Dockerfile,æ„å»ºè‡ªå®šä¹‰ vLLM é•œåƒ
2. é…ç½® docker-compose.yml,åŒ…å«:
   - vLLM æœåŠ¡
   - Nginx åå‘ä»£ç†
   - åŸºæœ¬çš„ç›‘æ§
3. å¯åŠ¨å®Œæ•´çš„æœåŠ¡æ ˆ
4. æµ‹è¯•æœåŠ¡çš„å¯ç”¨æ€§
5. æ¸…ç†æ‰€æœ‰èµ„æº

---

## ğŸ¯ æ€»ç»“

**å…³é”®è¦ç‚¹**:
- Docker æ˜¯ç¡®ä¿ç¯å¢ƒä¸€è‡´æ€§çš„æœ€ä½³æ–¹å¼
- ä»å®¿ä¸»æœºåˆ°ç”Ÿäº§ç¯å¢ƒä½¿ç”¨åŒä¸€ä¸ª Docker é•œåƒ
- vLLM æä¾›é«˜æ€§èƒ½ã€æ˜“ç”¨çš„æ¨ç†æœåŠ¡
- Docker Compose ç®€åŒ–å¤šæœåŠ¡ç¼–æ’
- æŒæ¡åŸºæœ¬çš„è°ƒè¯•å’Œç›‘æ§å·¥å…·

**ä¸‹ä¸€ç« **: ç¬¬5ç«  vLLM æ·±å…¥â€”â€”ç†è§£ PagedAttention å’Œ Continuous Batching çš„åŸç†ã€‚

---

**æœ‰é—®é¢˜?åŠ å…¥ [ç¬¬4ç«  Discord é¢‘é“](https://discord.gg/TODO) è®¨è®º!**
