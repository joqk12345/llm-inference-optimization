# ç¬¬10ç« : ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

> "åœ¨å¼€å‘ç¯å¢ƒèƒ½è¿è¡Œæ˜¯è¿æ°”,åœ¨ç”Ÿäº§ç¯å¢ƒç¨³å®šè¿è¡Œæ‰æ˜¯æœ¬äº‹ã€‚" - ä½šå

## ç®€ä»‹

ä»å¼€å‘ç¯å¢ƒåˆ°ç”Ÿäº§ç¯å¢ƒ,è¿™ä¸æ˜¯ä¸€ä¸ªç®€å•çš„"å¤åˆ¶ç²˜è´´"è¿‡ç¨‹,è€Œæ˜¯ä¸€æ¬¡è´¨çš„é£è·ƒã€‚æ ¹æ®è¡Œä¸šæ•°æ®,ç¯å¢ƒä¸å½“å¯¼è‡´çš„æ•…éšœå¹³å‡æ’æŸ¥æ—¶é—´ä¸º4-8å°æ—¶,è€Œæ­£ç¡®é…ç½®å¯ä»¥åœ¨30åˆ†é’Ÿå†…å®Œæˆéƒ¨ç½²ã€‚

**ğŸ’° æˆæœ¬å½±å“**(åŸºäºè¡Œä¸šæ•°æ®)
- **å¯ç”¨æ€§æå‡**: ä»99%æå‡åˆ°99.9%,æ•…éšœæˆæœ¬é™ä½10å€
- **ç›‘æ§ROI**: åŠæ—¶å‘ç°é—®é¢˜,é¿å…èµ„æºæµªè´¹
- **æˆæœ¬ä¼˜åŒ–**: é€šè¿‡Spotå®ä¾‹ç­‰ç­–ç•¥å¯èŠ‚çœ60-80%äº‘GPUæˆæœ¬

åœ¨æœ¬ç« ä¸­,ä½ å°†å­¦ä¹ :
- ç”Ÿäº§ç¯å¢ƒä¸å¼€å‘ç¯å¢ƒçš„å…³é”®å·®å¼‚
- å¦‚ä½•è®¾è®¡é«˜å¯ç”¨çš„éƒ¨ç½²æ¶æ„
- Kuberneteséƒ¨ç½²æœ€ä½³å®è·µ
- ç›‘æ§ä¸å¯è§‚æµ‹æ€§ä½“ç³»å»ºè®¾
- æ€§èƒ½è°ƒä¼˜çš„å®Œæ•´æµç¨‹
- æˆæœ¬ä¼˜åŒ–ç­–ç•¥ä¸ROIç›‘æ§
- å®‰å…¨æ€§ä¸ç¾å¤‡æ–¹æ¡ˆ

æœ¬ç« ç»“æŸå,ä½ å°†èƒ½å¤Ÿ:
- âœ… è®¾è®¡å¹¶éƒ¨ç½²é«˜å¯ç”¨çš„LLMæœåŠ¡
- âœ… æ­å»ºå®Œæ•´çš„ç›‘æ§ä½“ç³»
- âœ… å®æ–½æœ‰æ•ˆçš„æˆæœ¬ä¼˜åŒ–ç­–ç•¥
- âœ… å¤„ç†ç”Ÿäº§ç¯å¢ƒçš„å¸¸è§é—®é¢˜

---

## 10.1 ç”Ÿäº§ç¯å¢ƒ vs å¼€å‘ç¯å¢ƒ

### 10.1.1 å…³é”®å·®å¼‚

| ç»´åº¦ | å¼€å‘ç¯å¢ƒ | ç”Ÿäº§ç¯å¢ƒ |
|------|---------|---------|
| **å¯ç”¨æ€§è¦æ±‚** | å¯ä»¥æ¥å—åœæœº | 99.9%+ SLA |
| **è´Ÿè½½ç‰¹å¾** | ä½å¹¶å‘,æµ‹è¯•æµé‡ | é«˜å¹¶å‘,çœŸå®ç”¨æˆ· |
| **ç›‘æ§** | åŸºæœ¬æ—¥å¿—å³å¯ | å®Œæ•´å¯è§‚æµ‹æ€§ä½“ç³» |
| **å®‰å…¨** | å®½æ¾ | ä¸¥æ ¼çš„è®¤è¯æˆæƒ |
| **æˆæœ¬ä¼˜åŒ–** | ä¸å…³å¿ƒ | å¿…é¡»ä¼˜åŒ– |
| **æ•…éšœæ¢å¤** | æ‰‹åŠ¨é‡å¯ | è‡ªåŠ¨æ¢å¤ |
| **å®¹é‡è§„åˆ’** | çŒœä¼° | åŸºäºæ•°æ®çš„é¢„æµ‹ |

### 10.1.2 ç”Ÿäº§ç¯å¢ƒçš„ç‰¹æ®Šè¦æ±‚

**1. é«˜å¯ç”¨æ€§(High Availability)**

```yaml
# å•ç‚¹æ•…éšœæ˜¯ç”Ÿäº§ç¯å¢ƒçš„å¤§å¿Œ
æ¶æ„è®¾è®¡:
  - å¤šå‰¯æœ¬éƒ¨ç½²(è‡³å°‘2ä¸ªå®ä¾‹)
  - è·¨å¯ç”¨åŒºåˆ†å¸ƒ(AZåˆ†å¸ƒ)
  - è‡ªåŠ¨æ•…éšœè½¬ç§»
  - å¥åº·æ£€æŸ¥æœºåˆ¶
```

**2. å¯è§‚æµ‹æ€§(Observability)**

ç”Ÿäº§ç¯å¢ƒéœ€è¦ä¸‰å¤§æ”¯æŸ±:

```python
# Metrics(æŒ‡æ ‡)
- è¯·æ±‚å»¶è¿Ÿ(P50, P95, P99)
- ååé‡(tokens/s)
- GPUåˆ©ç”¨ç‡
- é”™è¯¯ç‡

# Logs(æ—¥å¿—)
- ç»“æ„åŒ–æ—¥å¿—(JSONæ ¼å¼)
- æ—¥å¿—åˆ†çº§(ERROR, WARN, INFO)
- è¯·æ±‚è¿½è¸ªID

# Traces(è¿½è¸ª)
- ç«¯åˆ°ç«¯è¯·æ±‚è¿½è¸ª
- ä¾èµ–å…³ç³»å¯è§†åŒ–
- æ€§èƒ½ç“¶é¢ˆå®šä½
```

**3. å¼¹æ€§ä¼¸ç¼©(Elasticity)**

```yaml
è‡ªåŠ¨ä¼¸ç¼©ç­–ç•¥:
  - åŸºäºCPU/GPUåˆ©ç”¨ç‡
  - åŸºäºè¯·æ±‚é˜Ÿåˆ—é•¿åº¦
  - åŸºäºæ—¶é—´çª—å£(ä¸šåŠ¡é«˜å³°)
  - Spotå®ä¾‹è‡ªåŠ¨æ›¿æ¢
```

### 10.1.3 SLAå®šä¹‰

**SLA(Service Level Agreement)æ˜¯ç”Ÿäº§ç¯å¢ƒçš„æ‰¿è¯º**

| æŒ‡æ ‡ | å®šä¹‰ | ç›®æ ‡å€¼ | ç›‘æ§æ–¹å¼ |
|------|------|--------|---------|
| **å¯ç”¨æ€§** | æœåŠ¡æ­£å¸¸è¿è¡Œæ—¶é—´æ¯”ä¾‹ | 99.9% | å¥åº·æ£€æŸ¥ + å‘Šè­¦ |
| **å»¶è¿Ÿ** | è¯·æ±‚å“åº”æ—¶é—´ | TTFT < 2s | Prometheus |
| **ååé‡** | æ¯ç§’å¤„ç†çš„tokenæ•° | >1000 tok/s | æŒ‡æ ‡ç›‘æ§ |
| **é”™è¯¯ç‡** | å¤±è´¥è¯·æ±‚æ¯”ä¾‹ | <0.1% | æ—¥å¿—åˆ†æ |

**å¯ç”¨æ€§ä¸æˆæœ¬çš„å…³ç³»**:

```
99%   å¯ç”¨æ€§ = 3.65å¤©/å¹´åœæœº   â†’ æˆæœ¬: 1x
99.9% å¯ç”¨æ€§ = 8.76å°æ—¶/å¹´åœæœº  â†’ æˆæœ¬: 2x
99.99%å¯ç”¨æ€§ = 52.56åˆ†é’Ÿ/å¹´åœæœº â†’ æˆæœ¬: 5x

å¯¹äºå¤§å¤šæ•°LLMæœåŠ¡,99.9%æ˜¯åˆç†çš„å¹³è¡¡ç‚¹
```

---

## 10.2 éƒ¨ç½²æ¶æ„è®¾è®¡

### 10.2.1 å•æœºéƒ¨ç½²

**é€‚ç”¨åœºæ™¯**:
- å¼€å‘æµ‹è¯•ç¯å¢ƒ
- å°è§„æ¨¡å†…éƒ¨å·¥å…·
- ä½å¹¶å‘åœºæ™¯(<10 QPS)

```bash
# å•æœºéƒ¨ç½²ç¤ºä¾‹
vllm serve meta-llama/Llama-3.1-8B \
  --tensor-parallel-size 1 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 8192 \
  --port 8000
```

**æ¶æ„å›¾**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         å•æœåŠ¡å™¨ (RTX 4090)          â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ vLLM    â”‚  â”‚ vLLM    â”‚  å¤šå‰¯æœ¬  â”‚
â”‚  â”‚ :8000   â”‚  â”‚ :8001   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â”‚
â”‚       â”‚            â”‚               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚             â–¼                      â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚       â”‚ Nginx    â”‚  è´Ÿè½½å‡è¡¡       â”‚
â”‚       â”‚ :80      â”‚                 â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.2.2 å¤šæœºéƒ¨ç½²(æ¨¡å‹å¹¶è¡Œ)

**é€‚ç”¨åœºæ™¯**:
- å¤§æ¨¡å‹(70B+)æ— æ³•æ”¾å…¥å•å¡
- éœ€è¦æ›´é«˜ååé‡
- ç”Ÿäº§ç¯å¢ƒé«˜å¯ç”¨

```bash
# ä½¿ç”¨Rayå¯åŠ¨å¤šèŠ‚ç‚¹é›†ç¾¤
# åœ¨å¤´èŠ‚ç‚¹(head node)
ray start --head --port=6379

# åœ¨å·¥ä½œèŠ‚ç‚¹(worker nodes)
ray start --address=<head-node-ip>:6379

# å¯åŠ¨vLLMæœåŠ¡(è‡ªåŠ¨åˆ†å¸ƒå¼)
vllm serve meta-llama/Llama-3.1-70B \
  --tensor-parallel-size 4 \
  --pipeline-parallel-size 2 \
  --distributed-executor-backend ray
```

**æ¶æ„å›¾**:

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Load Balancer  â”‚
          â”‚   (Nginx/ALB)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Node 1 â”‚    â”‚ Node 2 â”‚    â”‚ Node 3 â”‚
â”‚ GPU 0-3â”‚    â”‚ GPU 0-3â”‚    â”‚ GPU 0-3â”‚
â”‚ TP=4   â”‚    â”‚ TP=4   â”‚    â”‚ TP=4   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.2.3 è´Ÿè½½å‡è¡¡ç­–ç•¥

**1. è½®è¯¢(Round Robin)**

```nginx
upstream vllm_backend {
    server 10.0.1.10:8000;
    server 10.0.1.11:8000;
    server 10.0.1.12:8000;
}

server {
    listen 80;
    location /v1/chat/completions {
        proxy_pass http://vllm_backend;
    }
}
```

**2. æœ€å°‘è¿æ¥(Least Connections)**

```nginx
upstream vllm_backend {
    least_conn;
    server 10.0.1.10:8000;
    server 10.0.1.11:8000;
    server 10.0.1.12:8000;
}
```

**3. Session-Awareè·¯ç”±**(é‡è¦!)

```python
# å¯¹äºæœ‰çŠ¶æ€æœåŠ¡(LLMå¯¹è¯),éœ€è¦session stickiness
# ä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸Œç¡®ä¿åŒä¸€è¯·æ±‚è·¯ç”±åˆ°åŒä¸€èŠ‚ç‚¹

import hashlib

def get_worker_id(session_id: str, num_workers: int) -> int:
    """ä¸€è‡´æ€§å“ˆå¸Œè·¯ç”±"""
    hash_val = int(hashlib.md5(session_id.encode()).hexdigest(), 16)
    return hash_val % num_workers
```

**ä¸ºä»€ä¹ˆéœ€è¦Session-Awareè·¯ç”±?**

```
é—®é¢˜åœºæ™¯:
- ç”¨æˆ·Açš„ç¬¬ä¸€è½®å¯¹è¯ â†’ Node 1
- ç”¨æˆ·Açš„ç¬¬äºŒè½®å¯¹è¯ â†’ Node 2 (ä¸åŒçš„èŠ‚ç‚¹!)
- Node 2æ²¡æœ‰KV Cache â†’ éœ€è¦é‡æ–°prefillæ•´ä¸ªå†å²

è§£å†³æ–¹æ¡ˆ:
- ä½¿ç”¨session_idè¿›è¡Œä¸€è‡´æ€§å“ˆå¸Œ
- ç¡®ä¿åŒä¸€sessionçš„è¯·æ±‚è·¯ç”±åˆ°åŒä¸€èŠ‚ç‚¹
- KV Cacheå¯ä»¥å¤ç”¨,TTFTé™ä½40-60%
```

### 10.2.4 é«˜å¯ç”¨æ¶æ„

**å®Œæ•´çš„é«˜å¯ç”¨æ¶æ„**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CDN/WAF                        â”‚
â”‚              (CloudFlare/AWS WAF)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Load Balancer (ALB/SLB)              â”‚
â”‚              Health Checks + Auto Failover        â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                              â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚  AZ 1      â”‚              â”‚   AZ 2     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”‚              â”‚   â”Œâ”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚Node1 â”‚  â”‚              â”‚   â”‚Node3 â”‚ â”‚
â”‚  â”‚Node2 â”‚  â”‚              â”‚   â”‚Node4 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â”‚              â”‚   â””â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  å…±äº«å­˜å‚¨   â”‚              â”‚   å…±äº«å­˜å‚¨  â”‚
â”‚  (EFS/S3)  â”‚              â”‚  (EFS/S3)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ç›‘æ§å‘Šè­¦ç³»ç»Ÿ                  â”‚
â”‚  (Prometheus + Grafana + AlertMgr)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10.3 Kuberneteséƒ¨ç½²

### 10.3.1 K8såŸºç¡€æ¦‚å¿µ

**æ ¸å¿ƒæ¦‚å¿µæ˜ å°„**:

| K8sæ¦‚å¿µ | LLMæœåŠ¡å¯¹åº” |
|---------|-----------|
| **Pod** | ä¸€ä¸ªvLLMå®ä¾‹ |
| **Deployment** | vLLMå‰¯æœ¬ç®¡ç† |
| **Service** | æœåŠ¡å‘ç°ä¸è´Ÿè½½å‡è¡¡ |
| **ConfigMap** | é…ç½®ç®¡ç†(æ¨¡å‹å‚æ•°) |
| **Secret** | æ•æ„Ÿä¿¡æ¯(APIå¯†é’¥) |
| **HPA** | æ°´å¹³è‡ªåŠ¨ä¼¸ç¼© |

### 10.3.2 éƒ¨ç½²vLLMåˆ°K8s

**Deploymenté…ç½®**:

```yaml
# vllm-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama3-8b
  labels:
    app: vllm
    model: llama3-8b
spec:
  replicas: 3  # 3ä¸ªå‰¯æœ¬
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        resources:
          limits:
            nvidia.com/gpu: 1  # æ¯ä¸ªPod 1ä¸ªGPU
        env:
        - name: MODEL_NAME
          value: "meta-llama/Llama-3.1-8B"
        - name: GPU_MEMORY_UTILIZATION
          value: "0.9"
        - name: MAX_MODEL_LEN
          value: "8192"
        ports:
        - containerPort: 8000
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 5
```

**Serviceé…ç½®**:

```yaml
# vllm-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: vllm-service
spec:
  selector:
    app: vllm
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer  # æˆ– ClusterIP
```

**éƒ¨ç½²å‘½ä»¤**:

```bash
# åº”ç”¨é…ç½®
kubectl apply -f vllm-deployment.yaml
kubectl apply -f vllm-service.yaml

# æŸ¥çœ‹çŠ¶æ€
kubectl get pods -w
kubectl logs -f deployment/vllm-llama3-8b

# æ‰©ç¼©å®¹
kubectl scale deployment vllm-llama3-8b --replicas=5
```

### 10.3.3 é…ç½®ç®¡ç†

**ä½¿ç”¨ConfigMapç®¡ç†é…ç½®**:

```yaml
# vllm-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-config
data:
  MODEL_NAME: "meta-llama/Llama-3.1-8B"
  GPU_MEMORY_UTILIZATION: "0.9"
  MAX_MODEL_LEN: "8192"
  DTYPE: "half"
  KV_CACHE_DTYPE: "fp8"
```

**å¼•ç”¨ConfigMap**:

```yaml
envFrom:
- configMapRef:
    name: vllm-config
```

### 10.3.4 èµ„æºè°ƒåº¦ä¸GPUå…±äº«

**GPUå…±äº«(NVIDIA GPU Operator)**:

```yaml
# ä½¿ç”¨æ—¶é—´åˆ‡ç‰‡å…±äº«GPU
apiVersion: v1
kind: Pod
metadata:
  name: vllm-shared-gpu
spec:
  containers:
  - name: vllm
    image: vllm/vllm-openai:latest
    resources:
      limits:
        nvidia.com/gpu: 1  # è¯·æ±‚1ä¸ªGPU
        nvidia.com/mig-1g.5gb: 2  # æˆ–ä½¿ç”¨MIGåˆ†åŒº
```

**èŠ‚ç‚¹é€‰æ‹©ä¸äº²å’Œæ€§**:

```yaml
# ç¡®ä¿Podè°ƒåº¦åˆ°GPUèŠ‚ç‚¹
spec:
  containers:
  - name: vllm
    resources:
      limits:
        nvidia.com/gpu: 1
  nodeSelector:
    gpu-type: nvidia-a100  # é€‰æ‹©ç‰¹å®šGPUç±»å‹
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
```

**ä¼˜å…ˆçº§è°ƒåº¦**(ä¿è¯å…³é”®ä»»åŠ¡):

```yaml
# priority-class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority-vllm
value: 1000
globalDefault: false
description: "é«˜ä¼˜å…ˆçº§vLLMæœåŠ¡"
```

---

## 10.4 ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### 10.4.1 å…³é”®ç›‘æ§æŒ‡æ ‡

**ä¸šåŠ¡æŒ‡æ ‡**:

```python
# 1. å»¶è¿ŸæŒ‡æ ‡
æŒ‡æ ‡:
  - TTFT (Time To First Token)
  - TPOT (Time Per Output Token)
  - ç«¯åˆ°ç«¯å»¶è¿Ÿ

ç›®æ ‡:
  - TTFT < 2s (P95)
  - TPOT < 100ms (P95)
```

```python
# 2. ååé‡æŒ‡æ ‡
æŒ‡æ ‡:
  - tokens/second
  - requests/second
  - GPUåˆ©ç”¨ç‡

ç›®æ ‡:
  - >1000 tokens/s/GPU (Llama-3-8B)
  - GPUåˆ©ç”¨ç‡ > 60%
```

```python
# 3. è´¨é‡æŒ‡æ ‡
æŒ‡æ ‡:
  - é”™è¯¯ç‡
  - è¶…æ—¶ç‡
  - OOMé¢‘ç‡

ç›®æ ‡:
  - é”™è¯¯ç‡ < 0.1%
  - è¶…æ—¶ç‡ < 1%
```

**ç³»ç»ŸæŒ‡æ ‡**:

```bash
# GPUæŒ‡æ ‡
nvidia-smi dmon -s u -c 1  # GPUåˆ©ç”¨ç‡
nvidia-smi dmon -s m -c 1  # æ˜¾å­˜ä½¿ç”¨

# ç³»ç»ŸæŒ‡æ ‡
top -bn1 | grep "Cpu(s)"  # CPUä½¿ç”¨ç‡
free -h                    # å†…å­˜ä½¿ç”¨
df -h                      # ç£ç›˜ä½¿ç”¨
```

### 10.4.2 Prometheus + Grafana

**vLLMå†…ç½®Prometheusæ”¯æŒ**:

```bash
# å¯åŠ¨vLLMæ—¶å¯ç”¨metrics
vllm serve meta-llama/Llama-3.1-8B \
  --metrics-port 8000 \
  --enable-prometheus
```

**Prometheusé…ç½®**:

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'vllm'
    static_configs:
      - targets: ['vllm-service:8000']
    metrics_path: /metrics
```

**Grafanaä»ªè¡¨ç›˜JSONç‰‡æ®µ**:

```json
{
  "dashboard": {
    "title": "vLLM Performance Dashboard",
    "panels": [
      {
        "title": "TTFT (P95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(vllm:ttft_seconds_bucket[5m]))"
          }
        ]
      },
      {
        "title": "GPU Utilization",
        "targets": [
          {
            "expr": "nvidia_gpu_utilization"
          }
        ]
      },
      {
        "title": "Tokens per Second",
        "targets": [
          {
            "expr": "rate(vllm:tokens_total[5m])"
          }
        ]
      }
    ]
  }
}
```

**å…³é”®PromQLæŸ¥è¯¢**:

```promql
# TTFT P95
histogram_quantile(0.95, rate(vllm_ttft_seconds_bucket[5m]))

# ååé‡
rate(vllm_tokens_total[5m])

# GPUåˆ©ç”¨ç‡
nvidia_gpu_utilization

# è¯·æ±‚é”™è¯¯ç‡
rate(vllm_requests_failed_total[5m]) / rate(vllm_requests_total[5m])

# KV Cacheå‘½ä¸­ç‡
vllm_kv_cache_hit_rate
```

### 10.4.3 æ—¥å¿—æ”¶é›†ä¸åˆ†æ

**ç»“æ„åŒ–æ—¥å¿—é…ç½®**:

```python
# vllm_logging_config.json
{
  "version": 1,
  "formatters": {
    "json": {
      "class": "pythonjsonlogger.jsonlogger.JsonFormatter",
      "format": "%(asctime)s %(name)s %(levelname)s %(message)s"
    }
  },
  "handlers": {
    "console": {
      "class": "logging.StreamHandler",
      "formatter": "json",
      "stream": "ext://sys.stdout"
    }
  },
  "root": {
    "level": "INFO",
    "handlers": ["console"]
  }
}
```

**ä½¿ç”¨ELK Stackæ”¶é›†æ—¥å¿—**:

```yaml
# filebeat.yml
filebeat.inputs:
- type: container
  paths:
    - /var/log/containers/vllm*.log
  processors:
  - add_kubernetes_metadata:

output.elasticsearch:
  hosts: ["elasticsearch:9200"]
```

**å…³é”®æ—¥å¿—å­—æ®µ**:

```json
{
  "timestamp": "2025-01-31T10:00:00Z",
  "level": "INFO",
  "request_id": "req-123456",
  "session_id": "sess-789",
  "model": "Llama-3-8B",
  "input_tokens": 150,
  "output_tokens": 50,
  "ttft_ms": 1200,
  "tpot_ms": 80,
  "gpu_memory_mb": 16384,
  "cache_hit": true
}
```

### 10.4.4 åˆ†å¸ƒå¼è¿½è¸ª

**ä½¿ç”¨OpenTelemetry**:

```python
# å®‰è£…
pip install opentelemetry-api opentelemetry-sdk

# è¿½è¸ªä»£ç 
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider

trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

def generate_request(prompt: str):
    with tracer.start_as_current_span("generate_request") as span:
        span.set_attribute("prompt_length", len(prompt))

        with tracer.start_as_current_span("prefill"):
            # Prefillé˜¶æ®µ
            pass

        with tracer.start_as_current_span("decode"):
            # Decodeé˜¶æ®µ
            pass
```

**Jaeger UIæŸ¥çœ‹è¿½è¸ª**:

```bash
# å¯åŠ¨Jaeger
docker run -d --name jaeger \
  -p 16686:16686 \
  -p 14250:14250 \
  jaegertracing/all-in-one:latest

# è®¿é—®UI
open http://localhost:16686
```

---

## 10.5 æ€§èƒ½è°ƒä¼˜å®æˆ˜

### 10.5.1 è°ƒä¼˜æµç¨‹

**å®Œæ•´çš„è°ƒä¼˜æµç¨‹**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: å»ºç«‹åŸºçº¿                                   â”‚
â”‚  - ä½¿ç”¨benchmark_serving.pyæµ‹è¯•                     â”‚
â”‚  - è®°å½•TTFTã€TPOTã€ååé‡                           â”‚
â”‚  - è®°å½•GPUåˆ©ç”¨ç‡                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: è¯†åˆ«ç“¶é¢ˆ                                   â”‚
â”‚  - GPUåˆ©ç”¨ç‡ä½ â†’ å†…å­˜/CPUç“¶é¢ˆ?                      â”‚
â”‚  - GPUåˆ©ç”¨ç‡é«˜ä½†æ…¢ â†’ è®¡ç®—ç“¶é¢ˆ?                      â”‚
â”‚  - ä½¿ç”¨Nsight Systemsåˆ†æ                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: å®æ–½ä¼˜åŒ–                                   â”‚
â”‚  - è°ƒæ•´batch size                                  â”‚
â”‚  - å¯ç”¨Prefix Caching                              â”‚
â”‚  - é‡åŒ–(FP16â†’INT8)                                 â”‚
â”‚  - è°ƒæ•´max_model_len                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: éªŒè¯æ•ˆæœ                                   â”‚
â”‚  - é‡æ–°è¿è¡Œbenchmark                               â”‚
â”‚  - å¯¹æ¯”ä¼˜åŒ–å‰åæŒ‡æ ‡                                â”‚
â”‚  - ç¡®è®¤æ²¡æœ‰regression                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.5.2 ç“¶é¢ˆå®šä½æ–¹æ³•

**ä½¿ç”¨vLLMå†…ç½®benchmark**:

```bash
# è¿è¡Œbenchmark
python benchmark_serving.py \
  --model meta-llama/Llama-3.1-8B \
  --dataset-name sharegpt \
  --num-prompts 1000 \
  --request-rate 10

# è¾“å‡ºå…³é”®æŒ‡æ ‡
# TTFT: 1.2s
# TPOT: 80ms
# Throughput: 1234 tokens/s
# GPU Util: 65%
```

**ä½¿ç”¨Nsight Systems**:

```bash
# 1. å®‰è£…Nsight Systems
# https://developer.nvidia.com/nsight-systems

# 2. é‡‡é›†trace
nsys profile -o report.qdrep \
  python your_vllm_app.py

# 3. åˆ†æç»“æœ
nsys-ui report.qdrep

# æŸ¥çœ‹æŒ‡æ ‡:
# - GPUåˆ©ç”¨ç‡æ˜¯å¦è¾¾åˆ°é¢„æœŸ?
# - Memory bandwidthæ˜¯å¦é¥±å’Œ?
# - CPU overheadæ˜¯å¦è¿‡é«˜?
```

**è¯Šæ–­å†³ç­–æ ‘**:

```
GPUåˆ©ç”¨ç‡ < 60%?
  â”œâ”€ æ˜¯ â†’ å†…å­˜ä½¿ç”¨ç‡é«˜?
  â”‚   â”œâ”€ æ˜¯ â†’ å†…å­˜å—é™
  â”‚   â”‚   è§£å†³: å‡å°‘batch size, é‡åŒ–
  â”‚   â””â”€ å¦ â†’ CPU/IOå—é™
  â”‚       è§£å†³: æ£€æŸ¥CPUã€ç£ç›˜ã€ç½‘ç»œ
  â””â”€ å¦ â†’ è®¡ç®—å—é™
      è§£å†³: æ›´å¥½çš„GPU, tensor parallelism
```

### 10.5.3 å¸¸è§æ€§èƒ½é—®é¢˜

**é—®é¢˜1: TTFTè¿‡é•¿**

```yaml
ç—‡çŠ¶: é¦–ä¸ªtokenè¿”å›æ—¶é—´ > 3s

åŸå› :
  - KV Cacheæœªå‘½ä¸­
  - Promptå¤ªé•¿
  - å†…å­˜å¸¦å®½ä¸è¶³

è§£å†³æ–¹æ¡ˆ:
  # 1. å¯ç”¨Prefix Caching
  vllm serve ... --enable-prefix-caching

  # 2. ä½¿ç”¨Chunked Prefill
  vllm serve ... --max-model-len 32768

  # 3. ä¼˜åŒ–prompt
  - ç§»é™¤å†—ä½™å†…å®¹
  - å‹ç¼©ç³»ç»Ÿæç¤ºè¯
```

**é—®é¢˜2: ååé‡ä½**

```yaml
ç—‡çŠ¶: tokens/s < é¢„æœŸå€¼çš„50%

åŸå› :
  - Batch sizeå¤ªå°
  - GPUåˆ©ç”¨ç‡ä½
  - é¢‘ç¹çš„OOM

è§£å†³æ–¹æ¡ˆ:
  # 1. å¢åŠ batch size
  vllm serve ... --max-num-seqs 256

  # 2. è°ƒæ•´GPUå†…å­˜åˆ©ç”¨ç‡
  vllm serve ... --gpu-memory-utilization 0.95

  # 3. å¯ç”¨continuous batching
  vllm serve ... --enable-chunked-context
```

**é—®é¢˜3: OOMé¢‘ç¹**

```yaml
ç—‡çŠ¶: CUDA out of memoryé”™è¯¯

åŸå› :
  - max_model_lenå¤ªå¤§
  - KV Cacheå ç”¨è¿‡å¤š
  - æ‰¹æ¬¡å¤§å°è¿‡å¤§

è§£å†³æ–¹æ¡ˆ:
  # 1. å‡å°‘max_model_len
  vllm serve ... --max-model-len 4096

  # 2. KV Cacheé‡åŒ–
  vllm serve ... --kv-cache-dtype fp8

  # 3. å‡å°‘å¹¶å‘è¯·æ±‚æ•°
  vllm serve ... --max-num-batched-tokens 8192
```

### 10.5.4 è°ƒä¼˜å‚æ•°å‚è€ƒ

| å‚æ•° | é»˜è®¤å€¼ | æ¨èèŒƒå›´ | è¯´æ˜ |
|------|--------|---------|------|
| **gpu-memory-utilization** | 0.9 | 0.85-0.95 | è¿‡é«˜å¯èƒ½å¯¼è‡´OOM |
| **max-num-seqs** | 256 | 64-512 | å¹¶å‘è¯·æ±‚æ•° |
| **max-model-len** | æ¨¡å‹max | 2048-8192 | æ ¹æ®å®é™…éœ€æ±‚ |
| **dtype** | auto | half/bf16 | FP16/BF16 |
| **kv-cache-dtype** | auto | fp8/int8 | KVç¼“å­˜é‡åŒ– |

---

## 10.5.5 æ€§èƒ½åˆ†æå·¥å…·

> **å·¥å…·åˆ†ç±»**: Profilingå·¥å…·(å®šä½å†…æ ¸çº§ç“¶é¢ˆ) vs Benchmarkå·¥å…·(ç«¯åˆ°ç«¯æ€§èƒ½è¯„ä¼°)

### 10.5.5.1 PyTorch Profiler

**å¿«é€Ÿè¯Šæ–­Python/CUDAç“¶é¢ˆ**:

```python
import torch
from vllm import LLM, SamplingParams

# å¯ç”¨profiler
with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],
    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),
    on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'),
    record_shapes=True,
    profile_memory=True,
    with_stack=True
) as prof:
    llm = LLM(model="meta-llama/Llama-3.1-8B")
    params = SamplingParams(max_tokens=100)

    for i in range(5):
        llm.generate(["Hello world"] * 10, params)
        prof.step()

# æŸ¥çœ‹ç»“æœ
# tensorboard --logdir=./logs
```

**æŸ¥çœ‹Chrome Trace**:

```bash
# æ‰“å¼€Chrome://tracing
# åŠ è½½trace.json
# æŸ¥çœ‹CUDA kernelæ—¶é—´çº¿
```

### 10.5.5.2 Nsight Systems

**ç³»ç»Ÿçº§æ€§èƒ½åˆ†æ**:

```bash
# é‡‡é›†trace
nsys profile -y 30 -o vllm_report \
  --force-overwrite=true \
  python your_vllm_app.py

# æŸ¥çœ‹GUI
nsys-ui vllm_report.qdrep

# æˆ–å¯¼å‡ºæŠ¥å‘Š
nsys stats vllm_report.qdrep --report csv > stats.csv
```

**å…³é”®æŒ‡æ ‡è§£è¯»**:

```yaml
GPU Utilization:
  - ç†æƒ³å€¼: >80%
  - <60% â†’ å†…å­˜æˆ–CPUç“¶é¢ˆ

Memory Bandwidth:
  - H100å³°å€¼: 3.35 TB/s
  - A100å³°å€¼: 2.0 TB/s
  - RTX 4090: ~1 TB/s
  - è¾¾åˆ°>50%å³°å€¼ = å†…å­˜å¸¦å®½å—é™

Compute Throughput:
  - Tensor Coreåˆ©ç”¨ç‡
  - ç†æƒ³å€¼: >60%

CUDA Kernel Duration:
  - Top kernelså ç”¨>50%æ—¶é—´
  - ä¼˜åŒ–slow kernels

CPU Overhead:
  - ç†æƒ³å€¼: <10%æ€»æ—¶é—´
  - è¿‡é«˜ â†’ ä¼˜åŒ–Pythonä»£ç 
```

### 10.5.5.3 Nsight Compute

**Kernelçº§æ·±åº¦åˆ†æ**:

```bash
# åˆ†æç‰¹å®škernel
ncu --set full \
  --target-processes all \
  -o output_report \
  python your_vllm_app.py

# æŸ¥çœ‹æŠ¥å‘Š
ncu-ui output_report.ncu-rep

# å…³é”®æŒ‡æ ‡:
# - Memory Workload: è¯»å†™åå
# - Compute Throughput: FLOPsåˆ©ç”¨ç‡
# - Occupancy: Warpå¹¶è¡Œåº¦
# - Warp Efficiency: åˆ†æ”¯åˆ†æ­§ç¨‹åº¦
```

### 10.5.5.4 vLLMå†…ç½®æ€§èƒ½åˆ†æ

```bash
# vLLM 0.6.0+å†…ç½®profiling
VLLM_USE_TRACING=1 vllm serve meta-llama/Llama-3.1-8B

# æŸ¥çœ‹trace
# ç”Ÿæˆçš„chrome traceæ–‡ä»¶: /tmp/vllm_trace.json
```

### 10.5.5.5 æ€§èƒ½ä¼˜åŒ–checklist

**Step 1: åŸºçº¿æµ‹è¯•**
- ä½¿ç”¨`benchmark_serving.py`å»ºç«‹æ€§èƒ½åŸºçº¿
- è®°å½•å…³é”®æŒ‡æ ‡: throughput (tokens/s), TTFT, TPOT, GPUåˆ©ç”¨ç‡

**Step 2: PyTorch Profilerå¿«é€Ÿè¯Šæ–­**
- æ‰¾å‡ºtop CUDA time operators
- æ£€æŸ¥æ˜¯å¦æœ‰unexpectedçš„CPU overhead

**Step 3: Nsight Systemsç³»ç»Ÿçº§åˆ†æ**
- éªŒè¯GPUåˆ©ç”¨ç‡æ˜¯å¦åˆç†
- å®šä½å†…å­˜å¸¦å®½ç“¶é¢ˆ
- åˆ†æCPU-GPU overlap

**Step 4: Nsight Compute kernelä¼˜åŒ–**(å¦‚éœ€è¦)
- é’ˆå¯¹slow kernelè¿›è¡Œæ·±åº¦åˆ†æ
- ä¼˜åŒ–memory access pattern
- è°ƒæ•´block/gridé…ç½®

**Step 5: éªŒè¯ä¼˜åŒ–æ•ˆæœ**
- é‡æ–°è¿è¡Œbenchmark
- å¯¹æ¯”ä¼˜åŒ–å‰åçš„æŒ‡æ ‡
- ç¡®è®¤æ²¡æœ‰regression

### 10.5.5.6 LLMæ€§èƒ½æµ‹è¯•å·¥å…·

> **å·¥å…·å®šä½**: é™¤äº†profilingå·¥å…·,è¿˜éœ€è¦ç«¯åˆ°ç«¯çš„benchmarkå·¥å…·æ¥è¯„ä¼°LLMæ¨ç†æ€§èƒ½ã€‚

**GuideLLM** (Intel)

- **é¡¹ç›®åœ°å€**: https://github.com/intel/guidellm
- **æ ¸å¿ƒåŠŸèƒ½**:
  - ç«¯åˆ°ç«¯LLMæ¨ç†æ€§èƒ½æµ‹è¯•
  - æ”¯æŒå¤šç§ç¡¬ä»¶: Intel Gaudi2ã€Habanaã€Xeonã€NVIDIA GPU
  - æ ‡å‡†åŒ–benchmark: MMLUã€GSM8Kã€HumanEvalç­‰
- **å…³é”®ç‰¹æ€§**:
  - ç¡¬ä»¶å¯¹æ¯”æµ‹è¯•
  - æ¨ç†æ¡†æ¶é€‰å‹è¯„ä¼°
  - æ¨¡å‹æ€§èƒ½éªŒè¯

```bash
# å®‰è£…
pip install guidellm

# è¿è¡Œbenchmark
guidellm benchmark \
  --model meta-llama/Llama-3.1-8B \
  --framework vllm \
  --dataset mmlu \
  --output results.json
```

**EvalScope** (ModelScope)

- **é¡¹ç›®åœ°å€**: https://github.com/modelscope/evalscope
- **æ ¸å¿ƒåŠŸèƒ½**:
  - ç»¼åˆLLMè¯„ä¼°æ¡†æ¶
  - æ€§èƒ½+å‡†ç¡®ç‡æµ‹è¯•
  - æ”¯æŒç¦»çº¿è¯„ä¼°å’Œåœ¨çº¿æ¨ç†
- **å…³é”®ç‰¹æ€§**:
  - å¤šç»´åº¦è¯„ä¼°(æ€§èƒ½ã€å‡†ç¡®ç‡ã€é²æ£’æ€§)
  - ç”Ÿäº§ç¯å¢ƒæ€§èƒ½éªŒè¯
  - A/Bæµ‹è¯•æ”¯æŒ

**llm-bench** (vLLMå†…ç½®)

```bash
# vLLMå®˜æ–¹benchmarkå·¥å…·
python benchmark_serving.py \
  --model meta-llama/Llama-3.1-8B \
  --dataset-name sharegpt \
  --num-prompts 1000 \
  --request-rate 10

# è¾“å‡º:
# - TTFT (Time To First Token)
# - TPOT (Time Per Output Token)
# - Throughput (tokens/s)
# - GPUåˆ©ç”¨ç‡
```

**å®Œæ•´æ€§èƒ½æµ‹è¯•å·¥ä½œæµ**:

```bash
# Step 1: å¿«é€Ÿè¯„ä¼°(EvalScope)
evalscope eval \
  --model meta-llama/Llama-3.1-8B \
  --datasets mmlu,gsm8k

# Step 2: æ¨ç†æ€§èƒ½(llm-bench)
python benchmark_serving.py \
  --model meta-llama/Llama-3.1-8B \
  --num-prompts 1000

# Step 3: ç¡¬ä»¶å¯¹æ¯”(GuideLLM)
guidellm benchmark \
  --model meta-llama/Llama-3.1-8B \
  --framework vllm

# Step 4: vLLMä¸“ç”¨ä¼˜åŒ–
# ä½¿ç”¨vLLMå†…ç½®benchmark_serving.py
# éªŒè¯ç‰¹å®šä¼˜åŒ–æ•ˆæœ
```

---

## 10.6 æˆæœ¬ä¼˜åŒ–

### 10.6.1 äº‘GPUé€‰æ‹©ç­–ç•¥

**æˆæœ¬vsæ€§èƒ½æƒè¡¡**:

| GPU | æˆæœ¬/å°æ—¶ | æ€§èƒ½ | é€‚ç”¨åœºæ™¯ |
|-----|----------|------|---------|
| RTX 4090 | $1-2 | ä¸­ | å¼€å‘ã€å°æ¨¡å‹ |
| A100 (40GB) | $3-5 | é«˜ | ç”Ÿäº§ç¯å¢ƒ |
| A100 (80GB) | $5-7 | å¾ˆé«˜ | å¤§æ¨¡å‹ |
| H100 | $8-12 | é¡¶çº§ | é«˜æ€§èƒ½éœ€æ±‚ |

**é€‰æ‹©å†³ç­–æ ‘**:

```
æ¨¡å‹å¤§å° < 30B?
  â”œâ”€ æ˜¯ â†’ é¢„ç®— < $500/æœˆ?
  â”‚   â”œâ”€ æ˜¯ â†’ RTX 4090 (è‡ªå»ºæˆ–Lambda Labs)
  â”‚   â””â”€ å¦ â†’ A100 40GB (äº‘GPU)
  â””â”€ å¦ â†’ é¢„ç®— < $2000/æœˆ?
      â”œâ”€ æ˜¯ â†’ A100 80GB
      â””â”€ å¦ â†’ H100
```

### 10.6.2 Spotå®ä¾‹ä½¿ç”¨

**ğŸ’° æˆæœ¬èŠ‚çœ**: 60-80%

**ä»€ä¹ˆæ˜¯Spotå®ä¾‹?**
- äº‘å‚å•†çš„é—²ç½®GPUèµ„æº
- ä»·æ ¼æ¯”æŒ‰éœ€å®ä¾‹ä½60-80%
- å¯èƒ½è¢«éšæ—¶å›æ”¶

**ä½¿ç”¨ç­–ç•¥**:

```python
# ä½¿ç”¨Ray Autoscalerè‡ªåŠ¨ç®¡ç†Spotå®ä¾‹
# cluster.yaml

cluster_name: vllm-spot-cluster

provider:
  type: aws
  region: us-west-2

available_node_types:
  spot_head:
    resources:
      CPU: 8
      GPU: 0
    node_config:
      InstanceType: m5.xlarge
      InstanceMarketOptions:
        MarketType: spot

  spot_worker:
    resources:
      CPU: 32
      GPU: 4
    node_config:
      InstanceType: p4d.24xlarge
      InstanceMarketOptions:
        MarketType: spot
        SpotOptions:
          InstanceInterruptionBehavior: terminate

autoscaling_mode: default
autoscaler_options:
  idle_timeout_minutes: 10
```

**å¤„ç†ä¸­æ–­**:

```python
# æ£€æµ‹Spotä¸­æ–­
import requests

def check_spot_interruption():
    """AWS Spotä¸­æ–­æ£€æµ‹"""
    try:
        resp = requests.get(
            "http://169.254.169.254/latest/meta-data/spot/instance-action",
            timeout=1
        )
        return resp.json().get("action") == "terminate"
    except requests.exceptions.RequestException:
        return False

# ä¼˜é›…é™çº§
def graceful_shutdown():
    # 1. åœæ­¢æ¥å—æ–°è¯·æ±‚
    # 2. ç­‰å¾…ç°æœ‰è¯·æ±‚å®Œæˆ
    # 3. ä¿å­˜checkpoint
    # 4. è‡ªåŠ¨é‡å¯åˆ°æ–°èŠ‚ç‚¹
    pass
```

### 10.6.3 è‡ªåŠ¨ä¼¸ç¼©

**åŸºäºè´Ÿè½½çš„è‡ªåŠ¨ä¼¸ç¼©**:

```yaml
# Kubernetes HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vllm-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-llama3-8b
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: nvidia.com/gpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 120
```

**åŸºäºæ—¶é—´çš„ä¼¸ç¼©**:

```python
# ä¸šåŠ¡é«˜å³°æœŸæå‰æ‰©å®¹
from apscheduler.schedulers.background import BackgroundScheduler

def scale_up_before_peak():
    """åœ¨ä¸šåŠ¡é«˜å³°å‰æ‰©å®¹"""
    os.system("kubectl scale deployment vllm --replicas=10")

def scale_down_after_peak():
    """ä¸šåŠ¡é«˜å³°åç¼©å®¹"""
    os.system("kubectl scale deployment vllm --replicas=2")

scheduler = BackgroundScheduler()
scheduler.add_job(scale_up_before_peak, 'cron', hour=8)  # æ—©ä¸Š8ç‚¹
scheduler.add_job(scale_down_after_peak, 'cron', hour=20)  # æ™šä¸Š8ç‚¹
scheduler.start()
```

### 10.6.4 æˆæœ¬ç›‘æ§å·¥å…·

**AWS Cost Explorer**:

```bash
# è®¾ç½®æˆæœ¬å‘Šè­¦
aws budgets create-budget \
  --account-id <account-id> \
  --budget '{
    "BudgetName": "vLLM-GPU-Budget",
    "BudgetLimit": {"Amount": "1000", "Unit": "USD"},
    "TimeUnit": "MONTHLY",
    "BudgetType": "COST"
  }'
```

**è‡ªå®šä¹‰æˆæœ¬è¿½è¸ª**:

```python
import psutil
import pynvml
from datetime import datetime

def calculate_cost_per_token():
    """è®¡ç®—æ¯1000 tokensçš„æˆæœ¬"""

    # GPUå°æ—¶æˆæœ¬(å‡è®¾A100 $3/å°æ—¶)
    gpu_cost_per_hour = 3.0

    # è·å–GPUæ•°é‡å’Œåˆ©ç”¨ç‡
    pynvml.nvmlInit()
    gpu_count = pynvml.nvmlDeviceGetCount()
    total_util = 0
    for i in range(gpu_count):
        handle = pynvml.nvmlDeviceGetHandleByIndex(i)
        util = pynvml.nvmlDeviceGetUtilizationRates(handle)
        total_util += util.gpu

    avg_util = total_util / gpu_count / 100

    # å®é™…GPUä½¿ç”¨é‡
    effective_gpus = gpu_count * avg_util

    # æ¯å°æ—¶æˆæœ¬
    cost_per_hour = effective_gpus * gpu_cost_per_hour

    # æ¯ç§’æˆæœ¬
    cost_per_second = cost_per_hour / 3600

    return cost_per_second

# è®°å½•æ¯ä¸ªè¯·æ±‚çš„æˆæœ¬
def log_request_cost(tokens: int, time_seconds: float):
    cost = calculate_cost_per_token() * time_seconds
    cost_per_1k_tokens = (cost / tokens) * 1000

    print(f"Cost: ${cost_per_1k_tokens:.6f} per 1K tokens")
```

### 10.6.5 Agentç³»ç»Ÿçš„æˆæœ¬ä¼˜åŒ–ç­–ç•¥

> **æ¥æº**: [Manus - Context Engineering for AI Agents](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus)
>
> **æ ¸å¿ƒè§‚ç‚¹**: å›´ç»•KV-Cacheè®¾è®¡Agentç³»ç»Ÿâ€”â€”è¿™æ˜¯æˆæœ¬ä¼˜åŒ–çš„"é“¶å¼¹"

#### 10.6.5.1 æˆæœ¬å¯¹æ¯”:Cached vs Uncached

**Claude Sonnetå®šä»·**(2025):
- Cached tokens: **$0.30/MTok**
- Uncached tokens: **$3.00/MTok**
- **10å€å·®å¼‚!**

**Agentç³»ç»Ÿçš„æˆæœ¬æ”¾å¤§æ•ˆåº”**:
```
å…¸å‹Agentä»»åŠ¡: 50æ­¥tool calls
æ¯æ­¥contextå¢é•¿: ~500 tokens
æ€»tokenæ•°: 25,000 tokens (å¤§éƒ¨åˆ†æ˜¯prefill)

æ— ä¼˜åŒ–æˆæœ¬: 25K Ã— $3/MTok = $0.075/ä»»åŠ¡
ä¼˜åŒ–åæˆæœ¬: prefix cached â†’ ~$0.01/ä»»åŠ¡
èŠ‚çœ: 7.5å€
```

#### 10.6.5.2 å››å¤§ä¼˜åŒ–æ‰‹æ®µ

**ä¼˜åŒ–1: ç§»é™¤åŠ¨æ€å†…å®¹**

```python
# âŒ Before: æ¯æ¬¡è¯·æ±‚éƒ½ä¸åŒ
system_prompt = f"""
You are Manus AI assistant.
Current time: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
Today's date: {datetime.now().date()}
User ID: {user_id}
Session ID: {session_id}
...
"""

# âœ… After: å›ºå®šå‰ç¼€
system_prompt = """
You are Manus AI assistant.
Current time: {{current_time}}
Today's date: {{today_date}}
"""

# åŠ¨æ€å†…å®¹æ”¾åœ¨æœ€å
request = system_prompt + fixed_tools + dynamic_content
```

**ä¼˜åŒ–2: ä½¿ç”¨ç¨³å®šçš„JSONåºåˆ—åŒ–**

```python
# âŒ Before: æ— åºåºåˆ—åŒ–
import json
prompt_json = json.dumps(tools_definition)

# âœ… After: æœ‰åºåºåˆ—åŒ–
prompt_json = json.dumps(tools_definition, sort_keys=True)
```

**ä¼˜åŒ–3: ä½¿ç”¨appendè€Œémodify**

```python
# âŒ Before: ä¿®æ”¹æ•´ä¸ªprompt
for tool_call in tool_calls:
    prompt += f"\nTool result: {tool_call.result}"

# âœ… After: appendæ–°å†…å®¹
for tool_call in tool_calls:
    cache_manager.append(tool_call.result)
```

**ä¼˜åŒ–4: Session-Afulè·¯ç”±**

```python
class SessionAwareRouter:
    """ç¡®ä¿åŒä¸€sessionè·¯ç”±åˆ°åŒä¸€èŠ‚ç‚¹"""

    def __init__(self, num_workers: int):
        self.num_workers = num_workers
        self.worker_cache = {}

    def get_worker(self, session_id: str) -> int:
        if session_id in self.worker_cache:
            return self.worker_cache[session_id]

        # ä¸€è‡´æ€§å“ˆå¸Œ
        worker_id = hash(session_id) % self.num_workers
        self.worker_cache[session_id] = worker_id
        return worker_id
```

**æ•ˆæœ**:
- Prefix cacheå¤ç”¨ç‡æå‡
- TTFTé™ä½40-60%
- ååé‡æå‡2-3å€

#### 10.6.5.3 æˆæœ¬ä¼˜åŒ–Checklist

**åŸºçº¿æµ‹é‡**:
- [ ] æµ‹é‡å½“å‰KV-cache hit rate
- [ ] è®¡ç®—å¹³å‡æ¯ä¸ªä»»åŠ¡çš„tokenæ•°
- [ ] ç»Ÿè®¡prefill vs decodeæ¯”ä¾‹
- [ ] è®°å½•æ¯1000ä¸ªä»»åŠ¡çš„cost

**å¿«é€Ÿä¼˜åŒ–**(1å¤©å†…):
- [ ] ç§»é™¤promptä¸­çš„timestampç­‰åŠ¨æ€å†…å®¹
- [ ] æ£€æŸ¥JSONåºåˆ—åŒ–æ˜¯å¦ä½¿ç”¨`sort_keys=True`
- [ ] ç¡®ä¿promptç»“æ„æ˜¯"å›ºå®šprefix + åŠ¨æ€suffix"
- [ ] å¯ç”¨Prefix Caching

**ä¸­æœŸä¼˜åŒ–**(1å‘¨å†…):
- [ ] å®ç°Session-Afulè·¯ç”±
- [ ] æ·»åŠ file system fallbackæœºåˆ¶
- [ ] ç›‘æ§cache hit rateæŒ‡æ ‡

**é•¿æœŸä¼˜åŒ–**(æŒç»­):
- [ ] å»ºç«‹æˆæœ¬ç›‘æ§dashboard
- [ ] A/Bæµ‹è¯•ä¸åŒcontextç­–ç•¥
- [ ] ä¼˜åŒ–å·¥å…·è°ƒç”¨é¢‘ç‡
- [ ] å®æ–½contextå‹ç¼©ç­–ç•¥

#### 10.6.5.4 å®æˆ˜æ¡ˆä¾‹å¯¹æ¯”

| åœºæ™¯ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | èŠ‚çœ |
|------|--------|--------|------|
| ç®€å•ä»»åŠ¡(10æ­¥) | $0.02 | $0.005 | 75% |
| ä¸­ç­‰ä»»åŠ¡(30æ­¥) | $0.05 | $0.015 | 70% |
| å¤æ‚ä»»åŠ¡(50æ­¥) | $0.075 | $0.025 | 67% |
| è¶…é•¿ä»»åŠ¡(100æ­¥) | $0.15 | $0.06 | 60% |

**å…³é”®æ´å¯Ÿ**: ä»»åŠ¡è¶Šå¤æ‚,ä¼˜åŒ–æ•ˆæœè¶Šæ˜æ˜¾â€”â€”å› ä¸ºcontextç´¯ç§¯æ›´å¤šã€‚

### 10.6.6 è½»é‡çº§å‚è€ƒå®ç°:Mini-SGLang

> **ğŸ’¡ æ·±åº¦æ¥æº**: [Mini-SGLang Blog](https://lmsys.org/blog/2025-12-17-minisgl/)
>
> **æ ¸å¿ƒä»·å€¼**: 5kè¡Œä»£ç å®ç°å®Œæ•´æ¨ç†å¼•æ“,é€‚åˆå­¦ä¹ å’Œç ”ç©¶åŸå‹
>
> **é€‚ç”¨åœºæ™¯**: æ•™è‚²å­¦ä¹ ã€å¿«é€Ÿç ”ç©¶éªŒè¯ã€å†…æ ¸å¼€å‘è°ƒè¯•

#### 10.6.6.1 ä¸ºä»€ä¹ˆéœ€è¦è½»é‡çº§å®ç°?

**é—®é¢˜**:
- **vLLMä»£ç è§„æ¨¡**: 300k+è¡ŒPythonä»£ç 
  - æ–°æ‰‹å­¦ä¹ æ›²çº¿é™¡å³­
  - ä¿®æ”¹é£é™©é«˜(ç ´åéšå¼ä¸å˜é‡)
  - ç ”ç©¶åŸå‹éš¾ä»¥å¿«é€ŸéªŒè¯

- **SGLangä»£ç è§„æ¨¡**: 300kè¡ŒPythonä»£ç 
  - åŠŸèƒ½å®Œæ•´,ä½†å¤æ‚åº¦é«˜
  - ä¸é€‚åˆæ•™å­¦åœºæ™¯

**Mini-SGLangçš„ç­”æ¡ˆ**:
- **ä»…5kè¡ŒPythonä»£ç **(æ¯”vLLMç®€å•60å€)
- **ä¿ç•™æ ¸å¿ƒä¼˜åŒ–**:
  - Radix Attention (KV Cacheå¤ç”¨)
  - Overlap Scheduling (CPU-GPUå¹¶è¡Œ)
  - Chunked Prefill (å†…å­˜æ§åˆ¶)
  - Tensor Parallelism (åˆ†å¸ƒå¼æœåŠ¡)
  - JIT CUDA kernels (FlashAttention-3, FlashInfer)
- **æ€§èƒ½ç›¸å½“**: ä¸å®Œæ•´SGLangæ¥è¿‘

#### 10.6.6.2 5kè¡Œä»£ç å®ç°çš„æ ¸å¿ƒåŠŸèƒ½

**ä»£ç ç»“æ„**:
```
mini-sglang/
â”œâ”€â”€ server.py          # OpenAIå…¼å®¹API server
â”œâ”€â”€ tokenizer.py       # TokenizeræœåŠ¡
â”œâ”€â”€ scheduler.py       # è°ƒåº¦å™¨(å«Overlap Scheduling)
â”œâ”€â”€ radix_cache.py     # Radix Cacheå®ç°
â”œâ”€â”€ model_runner.py    # æ¨¡å‹æ‰§è¡Œ(Tensor Parallelism)
â””â”€â”€ kernels/
    â”œâ”€â”€ flashattention.py    # FlashAttention-3é›†æˆ
    â””â”€â”€ flashinfer.py        # FlashInferé›†æˆ
```

**å¯åŠ¨ç¤ºä¾‹**:

```bash
# å®‰è£…
pip install mini-sglang

# å¯åŠ¨server
python -m mini_sglang.server \
  --model meta-llama/Llama-3.1-8B \
  --tp 1 \
  --port 8000

# OpenAIå…¼å®¹API
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

**å…³é”®è®¾è®¡å†³ç­–**:
- **ç®€æ´æ€§ä¼˜å…ˆ**: ç§»é™¤è¾¹ç¼˜caseå¤„ç†,ä¸“æ³¨æ ¸å¿ƒé€»è¾‘
- **æ•™è‚²å¯¼å‘**: ä»£ç æ³¨é‡Šä¸°å¯Œ,æ˜“äºç†è§£
- **ç ”ç©¶å‹å¥½**: æ˜“äºä¿®æ”¹å’Œå®éªŒæ–°æƒ³æ³•

#### 10.6.6.3 æ ¸å¿ƒç»„ä»¶è§£æ

**1. Radix Cache**(radix_cache.py)

```python
class RadixCache:
    """5kè¡Œå®ç°çš„Radix Tree"""

    def __init__(self):
        self.root = RadixNode()

    def lookup(self, tokens: List[int]) -> CacheHit:
        """æŸ¥æ‰¾æœ€é•¿å…±äº«å‰ç¼€"""
        node = self.root
        matched = 0

        for token in tokens:
            if token in node.children:
                node = node.children[token]
                matched += 1
            else:
                break

        return CacheHit(node, matched)

    def insert(self, tokens: List[int]) -> None:
        """æ’å…¥æ–°prompt"""
        node = self.root
        for token in tokens:
            if token not in node.children:
                node.children[token] = RadixNode()
            node = node.children[token]
```

**2. Overlap Scheduling**(scheduler.py)

```python
class OverlapScheduler:
    """CPU-GPUå¹¶è¡Œè°ƒåº¦å™¨"""

    def schedule(self):
        """Overlap CPUå‡†å¤‡å’ŒGPUæ‰§è¡Œ"""
        while True:
            # CPU: å‡†å¤‡ä¸‹ä¸€ä¸ªbatch
            next_batch = self.prepare_batch_async()

            # GPU: æ‰§è¡Œå½“å‰batch
            self.execute_batch(current_batch)

            # äº¤æ¢
            current_batch = next_batch
```

**3. Tensor Parallelism**(model_runner.py)

```python
class TensorParallelRunner:
    """ç®€åŒ–çš„TPå®ç°"""

    def __init__(self, model, tp_size):
        self.tp_size = tp_size
        # NCCLåˆå§‹åŒ–
        # GPU kernelå¯åŠ¨
```

#### 10.6.6.4 å­¦ä¹ ä»·å€¼

**ä¸vLLMå¯¹æ¯”**:

| ç»´åº¦ | vLLM | Mini-SGLang |
|------|------|-------------|
| ä»£ç è¡Œæ•° | 300k+ | 5k |
| å­¦ä¹ æ›²çº¿ | é™¡å³­ | å¹³ç¼“ |
| æ ¸å¿ƒåŠŸèƒ½ | âœ… | âœ… |
| ç”Ÿäº§å°±ç»ª | âœ… | âŒ (æ•™è‚²/ç ”ç©¶) |
| ä¿®æ”¹éš¾åº¦ | é«˜ | ä½ |
| é˜…è¯»æ—¶é—´ | æ•°å‘¨ | æ•°å°æ—¶ |

**é€‚ç”¨åœºæ™¯**:

âœ… **Mini-SGLangé€‚åˆ**:
- å­¦ä¹ LLMæ¨ç†åŸç†
- å¿«é€ŸéªŒè¯ç ”ç©¶æƒ³æ³•
- å¼€å‘æ–°çš„CUDAå†…æ ¸
- ç†è§£Radix Cacheå®ç°

âŒ **vLLM/SGLangé€‚åˆ**:
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- éœ€è¦å®Œæ•´åŠŸèƒ½
- éœ€è¦é•¿æœŸç»´æŠ¤

---

## 10.7 ROIç›‘æ§ä¸æˆæœ¬è¿½è¸ª

### 10.7.1 å¦‚ä½•è¿½è¸ªæ¨ç†æˆæœ¬

**å®Œæ•´çš„æˆæœ¬è¿½è¸ªç³»ç»Ÿ**:

```python
class CostTracker:
    """LLMæ¨ç†æˆæœ¬è¿½è¸ªå™¨"""

    def __init__(self):
        self.requests = []
        self.gpu_cost_per_hour = 3.0  # A100 $3/å°æ—¶

    def track_request(self,
                     request_id: str,
                     input_tokens: int,
                     output_tokens: int,
                     ttft_ms: float,
                     gpu_utilization: float):
        """è¿½è¸ªå•ä¸ªè¯·æ±‚çš„æˆæœ¬"""

        # è®¡ç®—å®é™…GPUæ—¶é—´
        gpu_time_hours = (ttft_ms / 1000) / 3600

        # è®¡ç®—æˆæœ¬(è€ƒè™‘GPUåˆ©ç”¨ç‡)
        effective_gpus = gpu_utilization / 100
        cost = gpu_time_hours * effective_gpus * self.gpu_cost_per_hour

        # è®°å½•
        self.requests.append({
            "request_id": request_id,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "cost": cost,
            "cost_per_1k_tokens": (cost / (input_tokens + output_tokens)) * 1000
        })

    def get_summary(self):
        """è·å–æˆæœ¬æ±‡æ€»"""
        total_cost = sum(r["cost"] for r in self.requests)
        total_tokens = sum(r["total_tokens"] for r in self.requests)

        return {
            "total_requests": len(self.requests),
            "total_cost": total_cost,
            "total_tokens": total_tokens,
            "avg_cost_per_1k_tokens": (total_cost / total_tokens) * 1000
        }

# ä½¿ç”¨ç¤ºä¾‹
tracker = CostTracker()
tracker.track_request(
    request_id="req-001",
    input_tokens=150,
    output_tokens=50,
    ttft_ms=1200,
    gpu_utilization=75
)

print(tracker.get_summary())
# {'total_cost': 0.00075, 'avg_cost_per_1k_tokens': 0.00375}
```

### 10.7.2 ä¼˜åŒ–æªæ–½çš„ROIè®¡ç®—

**ROIè®¡ç®—å…¬å¼**:

```python
def calculate_roi(
    optimization_cost: float,  # ä¼˜åŒ–æŠ•å…¥çš„æˆæœ¬($)
    before_cost_per_hour: float,  # ä¼˜åŒ–å‰æˆæœ¬($/å°æ—¶)
    after_cost_per_hour: float,   # ä¼˜åŒ–åæˆæœ¬($/å°æ—¶)
    hours_per_month: float = 730   # æ¯æœˆå°æ—¶æ•°
):
    """è®¡ç®—ROI"""

    # æ¯æœˆèŠ‚çœ
    monthly_savings = (before_cost_per_hour - after_cost_per_hour) * hours_per_month

    # å›æœ¬å‘¨æœŸ(æœˆ)
    payback_period = optimization_cost / monthly_savings

    # å¹´åŒ–ROI
    annual_savings = monthly_savings * 12
    annual_roi = (annual_savings - optimization_cost) / optimization_cost * 100

    return {
        "monthly_savings": monthly_savings,
        "payback_period_months": payback_period,
        "annual_roi_percent": annual_roi
    }

# ç¤ºä¾‹: å¯ç”¨Prefix Cachingçš„ROI
roi = calculate_roi(
    optimization_cost=2000,  # å¼€å‘æ—¶é—´æˆæœ¬
    before_cost_per_hour=10,  # ä¼˜åŒ–å‰
    after_cost_per_hour=3,    # ä¼˜åŒ–å(70%èŠ‚çœ)
    hours_per_month=730
)

print(roi)
# {'monthly_savings': 5110, 'payback_period_months': 0.39, 'annual_roi_percent': 2960%}
```

**ROIä»ªè¡¨ç›˜**:

```python
import matplotlib.pyplot as plt

def plot_roi_dashboard():
    """ç»˜åˆ¶ROIä»ªè¡¨ç›˜"""

    # æ•°æ®
    optimizations = [
        "Prefix Caching",
        "INT8é‡åŒ–",
        "Spotå®ä¾‹",
        "è‡ªåŠ¨ä¼¸ç¼©"
    ]

    investment = [2000, 1000, 500, 1500]
    monthly_savings = [5110, 3640, 4380, 2190]
    payback_months = [i / s * 30 for i, s in zip(investment, monthly_savings)]

    # ç»˜å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    # æŠ•èµ„vsèŠ‚çœ
    ax1.bar(optimizations, investment, label='æŠ•èµ„($)')
    ax1.bar(optimizations, monthly_savings, label='æœˆèŠ‚çœ($)')
    ax1.set_title('æŠ•èµ„ vs æœˆèŠ‚çœ')
    ax1.legend()

    # å›æœ¬å‘¨æœŸ
    ax2.bar(optimizations, payback_months)
    ax2.set_title('å›æœ¬å‘¨æœŸ(å¤©)')
    ax2.set_ylabel('å¤©æ•°')

    plt.tight_layout()
    plt.savefig('roi_dashboard.png')
```

### 10.7.3 æŒç»­ä¼˜åŒ–æµç¨‹

**PDCAå¾ªç¯**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Plan: åˆ¶å®šä¼˜åŒ–è®¡åˆ’                              â”‚
â”‚  - åˆ†ææˆæœ¬æ•°æ®                                  â”‚
â”‚  - è¯†åˆ«ä¼˜åŒ–æœºä¼š                                  â”‚
â”‚  - ä¼°ç®—ROI                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Do: å®æ–½ä¼˜åŒ–                                    â”‚
â”‚  - ä»£ç å®ç°                                      â”‚
â”‚  - ç°åº¦å‘å¸ƒ                                      â”‚
â”‚  - ç›‘æ§æŒ‡æ ‡                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Check: éªŒè¯æ•ˆæœ                                 â”‚
â”‚  - å¯¹æ¯”ä¼˜åŒ–å‰åæˆæœ¬                              â”‚
â”‚  - è®¡ç®—å®é™…ROI                                   â”‚
â”‚  - æ£€æŸ¥æ˜¯å¦æœ‰å‰¯ä½œç”¨                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Act: æ ‡å‡†åŒ–æˆ–è°ƒæ•´                               â”‚
â”‚  - ROIè¾¾æ ‡ â†’ å…¨é¢æ¨å¹¿                            â”‚
â”‚  - ROIä¸è¾¾æ ‡ â†’ åˆ†æåŸå› ,è°ƒæ•´ç­–ç•¥                 â”‚
â”‚  - æ–‡æ¡£åŒ–æœ€ä½³å®è·µ                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¼˜åŒ–ä¼˜å…ˆçº§çŸ©é˜µ**:

```
é«˜ROI, ä½éš¾åº¦ â†’ ä¼˜å…ˆå®æ–½
  - å¯ç”¨Prefix Caching
  - ç§»é™¤promptåŠ¨æ€å†…å®¹

é«˜ROI, é«˜éš¾åº¦ â†’ ä¸­æœŸè§„åˆ’
  - INT4é‡åŒ–
  - è‡ªå®šä¹‰kernelä¼˜åŒ–

ä½ROI, ä½éš¾åº¦ â†’ å¡«å……å®æ–½
  - æ—¥å¿—ä¼˜åŒ–
  - ç›‘æ§å®Œå–„

ä½ROI, é«˜éš¾åº¦ â†’ æš‚ç¼“å®æ–½
  - è‡ªç ”æ¨ç†æ¡†æ¶
  - ç¡¬ä»¶å®šåˆ¶
```

---

## 10.8 å®‰å…¨æ€§è€ƒè™‘

### 10.8.1 APIè®¤è¯ä¸æˆæƒ

**API Keyè®¤è¯**:

```python
from fastapi import FastAPI, Header, HTTPException

app = FastAPI()

VALID_API_KEYS = {
    "sk-1234567890": "user1",
    "sk-0987654321": "user2"
}

@app.post("/v1/chat/completions")
async def chat_completions(
    authorization: str = Header(None)
):
    # éªŒè¯API Key
    if not authorization:
        raise HTTPException(status_code=401, detail="Missing API key")

    api_key = authorization.replace("Bearer ", "")
    if api_key not in VALID_API_KEYS:
        raise HTTPException(status_code=403, detail="Invalid API key")

    # å¤„ç†è¯·æ±‚
    user = VALID_API_KEYS[api_key]
    # ...
```

**é€Ÿç‡é™åˆ¶**:

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/v1/chat/completions")
@limiter.limit("10/minute")  # æ¯åˆ†é’Ÿ10æ¬¡è¯·æ±‚
async def chat_completions(request: Request):
    # ...
```

**åŸºäºTokençš„é™æµ**:

```python
class TokenBucketRateLimiter:
    """åŸºäºtokençš„é™æµå™¨"""

    def __init__(self, rate: int, capacity: int):
        self.rate = rate  # tokens/ç§’
        self.capacity = capacity  # æ¡¶å®¹é‡
        self.tokens = capacity
        self.last_time = time.time()

    def consume(self, tokens: int) -> bool:
        """æ¶ˆè´¹tokens"""
        now = time.time()
        elapsed = now - self.last_time

        # è¡¥å……tokens
        self.tokens = min(
            self.capacity,
            self.tokens + elapsed * self.rate
        )
        self.last_time = now

        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿtokens
        if self.tokens >= tokens:
            self.tokens -= tokens
            return True
        return False

# ä½¿ç”¨ç¤ºä¾‹
rate_limiter = TokenBucketRateLimiter(rate=100, capacity=1000)

@app.post("/v1/generate")
async def generate(prompt: str, max_tokens: int):
    if not rate_limiter.consume(max_tokens):
        raise HTTPException(status_code=429, detail="Rate limit exceeded")
    # ...
```

### 10.8.2 å†…å®¹å®‰å…¨è¿‡æ»¤

**è¾“å…¥è¿‡æ»¤**:

```python
import re

def validate_input(prompt: str):
    """éªŒè¯è¾“å…¥å®‰å…¨æ€§"""

    # æ£€æŸ¥æ¶æ„prompt
    malicious_patterns = [
        r"å¿½ç•¥.*æŒ‡ä»¤",
        r"ignore.*instruction",
        r"<\|.*\|>",  # ç‰¹æ®Štokenæ³¨å…¥
    ]

    for pattern in malicious_patterns:
        if re.search(pattern, prompt, re.IGNORECASE):
            raise ValueError("Malicious input detected")

    # æ£€æŸ¥prompté•¿åº¦
    if len(prompt) > 100000:
        raise ValueError("Prompt too long")

    return True
```

**è¾“å‡ºè¿‡æ»¤**:

```python
from transformers import pipeline

# åŠ è½½å®‰å…¨åˆ†ç±»å™¨
safety_classifier = pipeline("text-classification",
                             model="distilbert-base-uncased")

def filter_output(text: str) -> str:
    """è¿‡æ»¤ä¸å®‰å…¨è¾“å‡º"""

    result = safety_classifier(text)[0]

    if result["label"] == "UNSAFE" and result["score"] > 0.8:
        return "[å†…å®¹å·²è¢«è¿‡æ»¤]"

    return text
```

### 10.8.3 æ•°æ®éšç§

**æ•æ„Ÿæ•°æ®è„±æ•**:

```python
import re

def mask_pii(text: str) -> str:
    """è„±æ•æ•æ„Ÿä¿¡æ¯"""

    # Email
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
                  '[EMAIL]', text)

    # Phone
    text = re.sub(r'\b\d{3}-\d{3}-\d{4}\b',
                  '[PHONE]', text)

    # Credit Card
    text = re.sub(r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',
                  '[CARD]', text)

    # SSN
    text = re.sub(r'\b\d{3}-\d{2}-\d{4}\b',
                  '[SSN]', text)

    return text
```

**æ•°æ®åŠ å¯†å­˜å‚¨**:

```python
from cryptography.fernet import Fernet

class EncryptedStorage:
    """åŠ å¯†å­˜å‚¨"""

    def __init__(self, key: bytes):
        self.cipher = Fernet(key)

    def encrypt(self, data: str) -> bytes:
        return self.cipher.encrypt(data.encode())

    def decrypt(self, encrypted: bytes) -> str:
        return self.cipher.decrypt(encrypted).decode()

# ä½¿ç”¨ç¤ºä¾‹
storage = EncryptedStorage(Fernet.generate_key())
encrypted = storage.encrypt("sensitive data")
decrypted = storage.decrypt(encrypted)
```

### 10.8.4 å®¡è®¡æ—¥å¿—

**å®Œæ•´çš„å®¡è®¡æ—¥å¿—**:

```python
import logging
from datetime import datetime

class AuditLogger:
    """å®¡è®¡æ—¥å¿—è®°å½•å™¨"""

    def __init__(self):
        self.logger = logging.getLogger("audit")
        handler = logging.FileHandler("audit.log")
        handler.setFormatter(logging.Formatter('%(message)s'))
        self.logger.addHandler(handler)

    def log_request(self,
                   user_id: str,
                   request_id: str,
                   prompt: str,
                   response: str,
                   tokens_used: int,
                   cost: float):
        """è®°å½•è¯·æ±‚å®¡è®¡ä¿¡æ¯"""

        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "user_id": user_id,
            "request_id": request_id,
            "prompt_length": len(prompt),
            "response_length": len(response),
            "tokens_used": tokens_used,
            "cost": cost
        }

        self.logger.info(json.dumps(log_entry))

# ä½¿ç”¨ç¤ºä¾‹
audit = AuditLogger()
audit.log_request(
    user_id="user123",
    request_id="req-001",
    prompt="Hello",
    response="Hi there!",
    tokens_used=5,
    cost=0.0001
)
```

---

## 10.9 ç¾å¤‡ä¸å®¹é”™

### 10.9.1 å¤±è´¥åœºæ™¯åˆ†æ

**å¸¸è§å¤±è´¥åœºæ™¯**:

| å¤±è´¥ç±»å‹ | æ¦‚ç‡ | å½±å“ | æ£€æµ‹æ–¹å¼ |
|---------|------|------|---------|
| GPUç¡¬ä»¶æ•…éšœ | ä¸­ | é«˜ | NVIDIAå¥åº·æ£€æŸ¥ |
| OOM | é«˜ | ä¸­ | ç›‘æ§æ˜¾å­˜ä½¿ç”¨ |
| ç½‘ç»œåˆ†åŒº | ä½ | é«˜ | å¿ƒè·³æ£€æµ‹ |
| Spotå›æ”¶ | é«˜ | ä½ | AWSå…ƒæ•°æ®æœåŠ¡ |
| è¿›ç¨‹å´©æºƒ | ä¸­ | é«˜ | å¥åº·æ£€æŸ¥ |

### 10.9.2 å¥åº·æ£€æŸ¥

**Liveness Probe**(å­˜æ´»æ£€æŸ¥):

```yaml
# Kubernetes liveness probe
livenessProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 60
  periodSeconds: 10
  failureThreshold: 3
```

**Readiness Probe**(å°±ç»ªæ£€æŸ¥):

```yaml
# Kubernetes readiness probe
readinessProbe:
  httpGet:
    path: /ready
    port: 8000
  initialDelaySeconds: 30
  periodSeconds: 5
  failureThreshold: 3
```

**è‡ªå®šä¹‰å¥åº·æ£€æŸ¥ç«¯ç‚¹**:

```python
from fastapi import FastAPI
import pynvml

app = FastAPI()

@app.get("/health")
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""

    try:
        # æ£€æŸ¥GPUçŠ¶æ€
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)

        # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        if mem_info.used / mem_info.total > 0.98:
            return {"status": "unhealthy", "reason": "OOM"}

        # æ£€æŸ¥æ¸©åº¦
        temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
        if temp > 90:
            return {"status": "unhealthy", "reason": "Overheating"}

        return {"status": "healthy"}

    except Exception as e:
        return {"status": "unhealthy", "reason": str(e)}

@app.get("/ready")
def readiness_check():
    """å°±ç»ªæ£€æŸ¥ç«¯ç‚¹"""

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½å®Œæˆ
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿèµ„æºæ¥å—æ–°è¯·æ±‚

    return {"status": "ready"}
```

### 10.9.3 è‡ªåŠ¨é‡å¯ç­–ç•¥

**Kubernetesé‡å¯ç­–ç•¥**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    spec:
      restartPolicy: Always
```

**è‡ªåŠ¨æ¢å¤è„šæœ¬**:

```python
import time
import subprocess
import requests

def monitor_and_restart():
    """ç›‘æ§å¹¶è‡ªåŠ¨é‡å¯"""

    while True:
        try:
            # æ£€æŸ¥å¥åº·çŠ¶æ€
            resp = requests.get("http://localhost:8000/health", timeout=5)

            if resp.json()["status"] != "healthy":
                print("Unhealthy detected, restarting...")
                subprocess.run(["docker-compose", "restart"])

        except Exception as e:
            print(f"Health check failed: {e}")
            subprocess.run(["docker-compose", "restart"])

        time.sleep(10)

if __name__ == "__main__":
    monitor_and_restart()
```

### 10.9.4 é™çº§æ–¹æ¡ˆ

**ä¼˜é›…é™çº§ç­–ç•¥**:

```python
class DegradationManager:
    """é™çº§ç®¡ç†å™¨"""

    def __init__(self):
        self.current_level = 0  # 0=æ­£å¸¸, 1=è½»åº¦é™çº§, 2=é‡åº¦é™çº§

    def check_and_degrade(self):
        """æ£€æŸ¥ç³»ç»ŸçŠ¶æ€å¹¶é™çº§"""

        # æ£€æŸ¥GPUå¯ç”¨æ•°é‡
        gpu_count = get_available_gpu_count()

        if gpu_count < 2:
            self.current_level = 2
            # é‡åº¦é™çº§: æ‹’ç»æ–°è¯·æ±‚
            return {"action": "reject_new", "reason": "Insufficient GPUs"}

        elif gpu_count < 4:
            self.current_level = 1
            # è½»åº¦é™çº§: å‡å°‘max_model_len
            return {"action": "reduce_context", "new_max_len": 4096}

        else:
            self.current_level = 0
            return {"action": "normal"}

    def should_reject_request(self) -> bool:
        """æ˜¯å¦åº”è¯¥æ‹’ç»è¯·æ±‚"""
        return self.current_level == 2

# ä½¿ç”¨ç¤ºä¾‹
degradation = DegradationManager()

@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é™çº§
    action = degradation.check_and_degrade()

    if action["action"] == "reject_new":
        raise HTTPException(status_code=503, detail="Service overloaded")

    if action["action"] == "reduce_context":
        # è°ƒæ•´è¯·æ±‚å‚æ•°
        request.max_tokens = min(request.max_tokens, 1000)

    # å¤„ç†è¯·æ±‚
    # ...
```

---

## 10.10 RLç³»ç»Ÿéƒ¨ç½² âš ï¸ å¼€æºç”Ÿæ€ç¼ºå¤±

> **ğŸ’¡ 2025å¹´æŠ€æœ¯è¶‹åŠ¿**(æ¥æº:2025"é’ç¨"AIå˜‰å¹´å - æœ±å­æ—@è´¨æœ´ã€æœ±ç«‹è€•@NVIDIA)
>
> RL(å¼ºåŒ–å­¦ä¹ )ç³»ç»Ÿçš„éƒ¨ç½²é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜:
> - Trainingå’ŒRolloutçš„åˆ†ç¦»
> - å¼‚æ„GPUçš„ååŒ
> - å¼¹æ€§èµ„æºåˆ†é…
> - ä½å»¶è¿Ÿçš„inference serving

### 10.10.1 RLç³»ç»Ÿçš„ç‰¹æ®Šéœ€æ±‚

**ä¸æ™®é€šæ¨ç†çš„åŒºåˆ«**:

| ç»´åº¦ | æ™®é€šæ¨ç† | RLç³»ç»Ÿ |
|------|---------|--------|
| **å·¥ä½œè´Ÿè½½** | ä»…æ¨ç† | Training + Rollout |
| **å»¶è¿Ÿè¦æ±‚** | ç§’çº§ | æ¯«ç§’çº§(Rollout) |
| **ååé‡** | é‡è¦ | æå…¶é‡è¦ |
| **GPUç±»å‹** | åŒæ„ | å¸¸å¼‚æ„(è®­ç»ƒ+æ¨ç†) |
| **è°ƒåº¦** | ç®€å• | å¤æ‚(PDåˆ†ç¦») |

### 10.10.2 å¼€æºé¡¹ç›®ç°çŠ¶

**å½“å‰çŠ¶å†µ**:
- âœ… **Ray/RLlib**: è®­ç»ƒæ¡†æ¶æˆç†Ÿ
- âŒ **RolloutæœåŠ¡**: å¼€æºç”Ÿæ€ç¼ºå¤±
- âŒ **ç»Ÿä¸€æ¡†æ¶**: ç”Ÿäº§çº§æ–¹æ¡ˆå°‘

**ä¸»è¦é¡¹ç›®**:

- **slime** (è´¨æœ´ç§‘æŠ€)
  - GitHub: https://github.com/zizai/slime
  - **å®šä½**: RLè®­ç»ƒå’Œæ¨ç†çš„ç»Ÿä¸€æ¡†æ¶
  - **ç‰¹ç‚¹**:
    - Trainingå’ŒRolloutå…±äº«GPU
    - æ”¯æŒå¼‚æ„GPU(H100+H200)
    - å¼¹æ€§èµ„æºåˆ†é…

### 10.10.3 å…³é”®æŒ‘æˆ˜

**æŒ‘æˆ˜1: Training vs Rolloutçš„èµ„æºç«äº‰**

```python
# é—®é¢˜: Trainingå’ŒRolloutç«äº‰GPUèµ„æº
# è§£å†³æ–¹æ¡ˆ: åŠ¨æ€èµ„æºåˆ†é…

class DynamicResourceManager:
    """åŠ¨æ€èµ„æºç®¡ç†å™¨"""

    def __init__(self, total_gpus: int):
        self.total_gpus = total_gpus
        self.training_gpus = 0
        self.rollout_gpus = 0

    def allocate(self, rollout_queue_length: int):
        """æ ¹æ®é˜Ÿåˆ—é•¿åº¦åŠ¨æ€åˆ†é…"""

        if rollout_queue_length > 100:
            # Rolloutå‹åŠ›å¤§,å¢åŠ èµ„æº
            self.rollout_gpus = min(self.total_gpus * 0.8, self.rollout_gpus + 1)
            self.training_gpus = self.total_gpus - self.rollout_gpus

        elif rollout_queue_length < 10:
            # Rolloutå‹åŠ›å°,å‡å°‘èµ„æº
            self.rollout_gpus = max(self.total_gpus * 0.2, self.rollout_gpus - 1)
            self.training_gpus = self.total_gpus - self.rollout_gpus

        return {
            "training": self.training_gpus,
            "rollout": self.rollout_gpus
        }
```

**æŒ‘æˆ˜2: å¼‚æ„GPUååŒ**

```python
# H100ç”¨äºtraining,H200ç”¨äºrollout

class HeterogeneousCluster:
    """å¼‚æ„é›†ç¾¤ç®¡ç†"""

    def __init__(self):
        self.h100_count = 8
        self.h200_count = 4

    def schedule_task(self, task_type: str):
        """è°ƒåº¦ä»»åŠ¡åˆ°åˆé€‚çš„GPU"""

        if task_type == "training":
            # Training â†’ H100(æ€§ä»·æ¯”é«˜)
            return "h100"

        elif task_type == "rollout":
            # Rollout â†’ H200(ä½å»¶è¿Ÿ)
            return "h200"
```

### 10.10.4 éƒ¨ç½²æ¶æ„

**å•æœºéƒ¨ç½²**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         å•æœåŠ¡å™¨ (H100)              â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Training   â”‚  â”‚   Rollout    â”‚ â”‚
â”‚  â”‚   (70%)     â”‚  â”‚    (30%)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**é€‚åˆ**: å°è§„æ¨¡å®éªŒ

**å¤šæœºéƒ¨ç½²**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Training Node  â”‚      â”‚  Rollout Nodes   â”‚
â”‚   (H100 x 8)     â”‚      â”‚  (H200 x 4)      â”‚
â”‚                  â”‚      â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   PPO     â”‚  â”‚      â”‚  â”‚  vLLM/SGL  â”‚  â”‚
â”‚  â”‚  Training  â”‚  â”‚      â”‚  â”‚  Serving   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Parameter     â”‚
              â”‚   Server       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**é€‚åˆ**: ç”Ÿäº§ç¯å¢ƒ

### 10.10.5 å®æˆ˜æ¡ˆä¾‹

**æ¡ˆä¾‹1: ä½¿ç”¨slimeéƒ¨ç½²ç®€å•RLä»»åŠ¡**

```bash
# å®‰è£…slime
pip install slime-rl

# å¯åŠ¨RLè®­ç»ƒ+rolloutæœåŠ¡
slime launch \
  --model meta-llama/Llama-3.1-8B \
  --task rlhf \
  --training-gpus 4 \
  --rollout-gpus 2 \
  --rollout-framework vllm
```

**æ¡ˆä¾‹2: å¼‚æ„GPUçš„RLéƒ¨ç½²(H100+H200)**

```python
# slimeé…ç½®æ–‡ä»¶
# slime_config.yaml

cluster:
  training_nodes:
    - type: H100
      count: 8
      use: training

  rollout_nodes:
    - type: H200
      count: 4
      use: rollout

training:
  framework: ppo
  batch_size: 512
  learning_rate: 1e-5

rollout:
  framework: vllm
  tensor_parallel_size: 4
  gpu_memory_utilization: 0.9
```

**æ¡ˆä¾‹3: å¤§è§„æ¨¡RLçš„å¼¹æ€§èµ„æºåˆ†é…**

```python
class ElasticRLScheduler:
    """å¼¹æ€§RLè°ƒåº¦å™¨"""

    def __init__(self):
        self.cloud_provider = AWS()
        self.spot_instances = []

    def scale_rollout_workers(self, demand: int):
        """æ ¹æ®éœ€æ±‚å¼¹æ€§æ‰©ç¼©å®¹"""

        current_workers = len(self.spot_instances)

        if demand > current_workers * 100:
            # éœ€è¦æ‰©å®¹
            new_instances = self.cloud_provider.launch_spot_instances(
                instance_type="p4d.24xlarge",
                count=(demand // 100) - current_workers
            )
            self.spot_instances.extend(new_instances)

        elif demand < current_workers * 50:
            # éœ€è¦ç¼©å®¹
            instances_to_terminate = self.spot_instances[(demand // 50):]
            for inst in instances_to_terminate:
                self.cloud_provider.terminate_instance(inst)
            self.spot_instances = self.spot_instances[:(demand // 50)]
```

---

## ğŸš« å¸¸è§è¯¯åŒº

### âŒ "ç”Ÿäº§ç¯å¢ƒåªéœ€è¦æ›´å¤šGPU"

**å®é™…æƒ…å†µ**: æ¶æ„å’Œä¼˜åŒ–æ¯”ç¡¬ä»¶æ›´é‡è¦ã€‚

```
åœºæ™¯1: 8ä¸ªRTX 4090 vs 2ä¸ªH100
- RTXæ–¹æ¡ˆ: 8Ã—24GB=192GB, å¸¦å®½~8 TB/s
- H100æ–¹æ¡ˆ: 2Ã—80GB=160GB, å¸¦å®½~6 TB/s
- ç»“è®º: å–å†³äºæ¨¡å‹å¤§å°å’Œé€šä¿¡å¼€é”€

åœºæ™¯2: ä¼˜åŒ–å‰ vs ä¼˜åŒ–å
- ä¼˜åŒ–å‰: 4ä¸ªA100, 1000 tokens/s
- ä¼˜åŒ–å(å¯ç”¨Prefix Caching): 2ä¸ªA100, 1500 tokens/s
- ç»“è®º: ä¼˜åŒ–æ¯”å¢åŠ GPUæ›´æœ‰æ•ˆ
```

### âŒ "K8sèƒ½è‡ªåŠ¨å¤„ç†æ‰€æœ‰æ•…éšœ"

**å®é™…æƒ…å†µ**: K8såªæ˜¯å·¥å…·,éœ€è¦åˆç†é…ç½®ã€‚

```yaml
# âŒ é”™è¯¯é…ç½®
livenessProbe:
  initialDelaySeconds: 0  # å¤ªçŸ­,æ¨¡å‹è¿˜æœªåŠ è½½
  periodSeconds: 1        # å¤ªé¢‘ç¹,æµªè´¹èµ„æº

# âœ… æ­£ç¡®é…ç½®
livenessProbe:
  initialDelaySeconds: 60  # ç»™æ¨¡å‹åŠ è½½æ—¶é—´
  periodSeconds: 10        # åˆç†é—´éš”
  failureThreshold: 3      # å…è®¸å¶å°”å¤±è´¥
```

### âŒ "ç›‘æ§è¶Šè¯¦ç»†è¶Šå¥½"

**å®é™…æƒ…å†µ**: å…³æ³¨å…³é”®æŒ‡æ ‡,é¿å…ä¿¡æ¯è¿‡è½½ã€‚

```python
# âŒ ç›‘æ§æ‰€æœ‰æŒ‡æ ‡
metrics = [
    "cpu_usage",
    "memory_usage",
    "disk_io",
    "network_io",
    "gpu_temperature",
    "gpu_fan_speed",
    "gpu_power_usage",
    # ... 100+ æŒ‡æ ‡
]

# âœ… ç›‘æ§å…³é”®æŒ‡æ ‡
metrics = [
    "ttft_p95",         # é¦–tokenå»¶è¿Ÿ
    "tokens_per_second", # ååé‡
    "gpu_utilization",   # GPUåˆ©ç”¨ç‡
    "error_rate",        # é”™è¯¯ç‡
    "kv_cache_hit_rate"  # ç¼“å­˜å‘½ä¸­ç‡
]
```

### âŒ "Spotå®ä¾‹ä¸å¯é ,ä¸é€‚åˆç”Ÿäº§"

**å®é™…æƒ…å†µ**: åˆç†çš„è®¾è®¡å¯ä»¥å¯é ä½¿ç”¨Spotå®ä¾‹ã€‚

```python
# âœ… æœ€ä½³å®è·µ
1. ä½¿ç”¨æ··åˆå®ä¾‹(æŒ‰éœ€ + Spot)
   - æŒ‰éœ€: æœ€å°å®¹é‡
   - Spot: å¼¹æ€§æ‰©å®¹

2. æ£€æŸ¥ä¸­æ–­ä¿¡å·
   - AWS: http://169.254.169.254/latest/meta-data/spot/instance-action
   - ä¼˜é›…å…³é—­: ä¿å­˜checkpoint, å®Œæˆå½“å‰è¯·æ±‚

3. è‡ªåŠ¨æ›¿æ¢
   - Spotè¢«å›æ”¶ â†’ è‡ªåŠ¨å¯åŠ¨æ–°Spotå®ä¾‹
   - ä½¿ç”¨Autoscaling Group

4. åˆ†å¸ƒå¼è®­ç»ƒ
   - ä½¿ç”¨checkpointå®šæœŸä¿å­˜
   - ä»checkpointæ¢å¤è®­ç»ƒ
```

---

## âœ… ç« èŠ‚æ£€æŸ¥æ¸…å•

é˜…è¯»æœ¬ç« å,ä½ åº”è¯¥èƒ½å¤Ÿ:

- [ ] è§£é‡Šç”Ÿäº§ç¯å¢ƒä¸å¼€å‘ç¯å¢ƒçš„å…³é”®å·®å¼‚
- [ ] è®¾è®¡é«˜å¯ç”¨çš„éƒ¨ç½²æ¶æ„
- [ ] ç¼–å†™Kuberneteséƒ¨ç½²é…ç½®
- [ ] æ­å»ºPrometheus + Grafanaç›‘æ§ç³»ç»Ÿ
- [ ] ä½¿ç”¨profilingå·¥å…·å®šä½æ€§èƒ½ç“¶é¢ˆ
- [ ] å®æ–½æˆæœ¬ä¼˜åŒ–ç­–ç•¥(Spotå®ä¾‹ã€è‡ªåŠ¨ä¼¸ç¼©)
- [ ] è®¡ç®—ä¼˜åŒ–æªæ–½çš„ROI
- [ ] é…ç½®APIè®¤è¯å’Œé€Ÿç‡é™åˆ¶
- [ ] å®ç°å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨æ¢å¤
- [ ] é¿å…å¸¸è§çš„ç”Ÿäº§ç¯å¢ƒè¯¯åŒº

---

## ğŸ“š åŠ¨æ‰‹ç»ƒä¹ 

**ç»ƒä¹ 10.1**: éƒ¨ç½²vLLMåˆ°Kubernetes

ç›®æ ‡: å°†vLLMæœåŠ¡éƒ¨ç½²åˆ°K8sé›†ç¾¤

ä»»åŠ¡:
1. ç¼–å†™Deploymenté…ç½®æ–‡ä»¶
2. é…ç½®Serviceå’ŒLoadBalancer
3. è®¾ç½®å¥åº·æ£€æŸ¥æ¢é’ˆ
4. éªŒè¯éƒ¨ç½²æˆåŠŸ

éªŒæ”¶:
```bash
kubectl get pods  # 3ä¸ªPodè¿è¡Œä¸­
kubectl port-forward service/vllm-service 8000:80
curl http://localhost:8000/v1/models
```

---

**ç»ƒä¹ 10.2**: æ­å»ºå®Œæ•´çš„ç›‘æ§ç³»ç»Ÿ

ç›®æ ‡: ä½¿ç”¨Prometheus + Grafanaç›‘æ§vLLM

ä»»åŠ¡:
1. é…ç½®Prometheusé‡‡é›†vLLM metrics
2. åˆ›å»ºGrafanaä»ªè¡¨ç›˜
3. æ·»åŠ å…³é”®æŒ‡æ ‡(TTFTã€ååé‡ã€GPUåˆ©ç”¨ç‡)
4. é…ç½®å‘Šè­¦è§„åˆ™

éªŒæ”¶:
- Grafanaæ˜¾ç¤ºå®æ—¶æŒ‡æ ‡
- GPUåˆ©ç”¨ç‡è¶…è¿‡90%æ—¶å‘Šè­¦
- TTFT P95è¶…è¿‡3ç§’æ—¶å‘Šè­¦

---

**ç»ƒä¹ 10.3**: å»ºç«‹ROIç›‘æ§ä»ªè¡¨ç›˜

ç›®æ ‡: è¿½è¸ªæ¨ç†æˆæœ¬å’Œä¼˜åŒ–ROI

ä»»åŠ¡:
1. å®ç°CostTrackerç±»
2. è®°å½•æ¯ä¸ªè¯·æ±‚çš„æˆæœ¬
3. è®¡ç®—ä¼˜åŒ–æªæ–½çš„ROI
4. å¯è§†åŒ–æˆæœ¬è¶‹åŠ¿

éªŒæ”¶:
- æ˜¾ç¤ºæ¯1000 tokensçš„æˆæœ¬
- è®¡ç®—Prefix Cachingçš„ROI
- ç”Ÿæˆæœˆåº¦æˆæœ¬æŠ¥å‘Š

---

**ç»ƒä¹ 10.4**: ä½¿ç”¨slimeéƒ¨ç½²ç®€å•RLä»»åŠ¡ â­

ç›®æ ‡: éƒ¨ç½²ä¸€ä¸ªç®€å•çš„RLè®­ç»ƒ+rolloutç³»ç»Ÿ

ä»»åŠ¡:
1. å®‰è£…slimeæ¡†æ¶
2. é…ç½®è®­ç»ƒå’ŒrolloutèŠ‚ç‚¹
3. å¯åŠ¨RLHFä»»åŠ¡
4. ç›‘æ§è®­ç»ƒè¿›åº¦

éªŒæ”¶:
- TrainingèŠ‚ç‚¹æ­£å¸¸è¿è¡Œ
- RolloutæœåŠ¡å“åº”<100ms
- æ¨¡å‹rewardæ”¶æ•›

---

**ç»ƒä¹ 10.5**: å¼€å‘å¹¶éƒ¨ç½²vLLMè‡ªå®šä¹‰æ’ä»¶ â­â­

ç›®æ ‡: å®ç°ä¸€ä¸ªvLLMæ’ä»¶æ¥å®šåˆ¶è¡Œä¸º

ä»»åŠ¡:
1. ä½¿ç”¨vLLM Plugin System
2. å®ç°è‡ªå®šä¹‰è°ƒåº¦å™¨(å¦‚ä¼˜å…ˆçº§è°ƒåº¦)
3. æ·»åŠ è‡ªå®šä¹‰æ—¥å¿—å’Œç›‘æ§
4. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

éªŒæ”¶:
- æ’ä»¶æ­£ç¡®åŠ è½½
- ä¼˜å…ˆçº§è°ƒåº¦ç”Ÿæ•ˆ
- é«˜ä¼˜å…ˆçº§è¯·æ±‚TTFTé™ä½50%

---

## âœ… ç»ƒä¹ å‚è€ƒç­”æ¡ˆ

**ç»ƒä¹ 10.1: éƒ¨ç½²vLLMåˆ°Kubernetes**

```yaml
# vllm-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama3-8b
spec:
  replicas: 3
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        resources:
          limits:
            nvidia.com/gpu: 1
        env:
        - name: MODEL_NAME
          value: "meta-llama/Llama-3.1-8B"
        ports:
        - containerPort: 8000
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-service
spec:
  selector:
    app: vllm
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

**ç»ƒä¹ 10.2: æ­å»ºç›‘æ§ç³»ç»Ÿ**

```yaml
# prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
    scrape_configs:
    - job_name: 'vllm'
      static_configs:
        - targets: ['vllm-service:8000']
      metrics_path: /metrics
---
apiVersion: v1
kind: Pod
metadata:
  name: prometheus
spec:
  containers:
  - name: prometheus
    image: prom/prometheus:latest
    args:
    - '--config.file=/etc/prometheus/prometheus.yml'
    volumeMounts:
    - name: config
      mountPath: /etc/prometheus
  volumes:
  - name: config
    configMap:
      name: prometheus-config
```

**ç»ƒä¹ 10.3: ROIç›‘æ§**

```python
# cost_tracker.py
import time
import json
from typing import List, Dict

class CostTracker:
    def __init__(self, gpu_cost_per_hour: float = 3.0):
        self.requests: List[Dict] = []
        self.gpu_cost_per_hour = gpu_cost_per_hour

    def track_request(self,
                     request_id: str,
                     input_tokens: int,
                     output_tokens: int,
                     ttft_ms: float,
                     gpu_utilization: float):
        total_tokens = input_tokens + output_tokens
        gpu_time_hours = (ttft_ms / 1000) / 3600
        effective_gpus = gpu_utilization / 100
        cost = gpu_time_hours * effective_gpus * self.gpu_cost_per_hour

        self.requests.append({
            "request_id": request_id,
            "total_tokens": total_tokens,
            "cost": cost,
            "cost_per_1k_tokens": (cost / total_tokens) * 1000,
            "timestamp": time.time()
        })

    def get_summary(self):
        total_cost = sum(r["cost"] for r in self.requests)
        total_tokens = sum(r["total_tokens"] for r in self.requests)
        return {
            "total_requests": len(self.requests),
            "total_cost": total_cost,
            "total_tokens": total_tokens,
            "avg_cost_per_1k_tokens": (total_cost / total_tokens) * 1000 if total_tokens > 0 else 0
        }

    def save_to_file(self, filename: str):
        with open(filename, "w") as f:
            json.dump(self.requests, f, indent=2)
```

---

## ğŸ¯ æ€»ç»“

å…³é”®è¦ç‚¹:
- **ç”Ÿäº§ç¯å¢ƒâ‰ å¼€å‘ç¯å¢ƒ**: éœ€è¦é«˜å¯ç”¨ã€ç›‘æ§ã€å®‰å…¨ã€ç¾å¤‡
- **ç›‘æ§æ˜¯åŸºç¡€**: Metricsã€Logsã€Tracesä¸‰å¤§æ”¯æŸ±
- **ä¼˜åŒ–å…ˆäºæ‰©å®¹**: Prefix Cachingã€é‡åŒ–ã€è‡ªåŠ¨ä¼¸ç¼©
- **æˆæœ¬å¯æ§**: Spotå®ä¾‹ã€ROIç›‘æ§ã€æŒç»­ä¼˜åŒ–
- **å®‰å…¨ç¬¬ä¸€**: è®¤è¯ã€æˆæƒã€å®¡è®¡æ—¥å¿—
- **æœªé›¨ç»¸ç¼ª**: å¥åº·æ£€æŸ¥ã€è‡ªåŠ¨æ¢å¤ã€é™çº§æ–¹æ¡ˆ

**ä¸‹ä¸€æ­¥**: ç¬¬11ç« é«˜çº§è¯é¢˜(å¼‚æ„ç¡¬ä»¶ã€MoEã€æœªæ¥è¶‹åŠ¿)

---

**æœ‰é—®é¢˜?åŠ å…¥ [ç¬¬10ç«  Discordé¢‘é“](https://discord.gg/TODO) è®¨è®º!**
