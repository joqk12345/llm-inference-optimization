# ç¬¬5ç« : LLMæ¨ç†åŸºç¡€

> **ğŸ’¡ æ•™å­¦ç†å¿µ** (å‚è€ƒ: Hugging Face "Continuous batching from first principles")
>
> **æ ¸å¿ƒæ€è·¯**: ä»ç¬¬ä¸€æ€§åŸç†å‡ºå‘,ç†è§£LLMæ¨ç†çš„åŸºæœ¬æµç¨‹å’Œä¼˜åŒ–åŠ¨æœºã€‚
>
> **å­¦ä¹ è·¯å¾„**: Attention â†’ KV Cache â†’ Chunked Prefill â†’ Continuous Batching

## ç®€ä»‹

åœ¨æ·±å…¥ vLLM çš„å¤æ‚ä¼˜åŒ–æŠ€æœ¯ä¹‹å‰,æˆ‘ä»¬éœ€è¦å…ˆç†è§£ LLM æ¨ç†çš„åŸºç¡€åŸç†ã€‚å¾ˆå¤šå·¥ç¨‹å¸ˆç›´æ¥è·³åˆ°é«˜çº§ä¼˜åŒ–æŠ€å·§,å´å¿½ç•¥äº†åŸºç¡€çŸ¥è¯†â€”â€”è¿™å°±åƒåœ¨æ²¡å­¦ä¼šèµ°è·¯ä¹‹å‰å°±æƒ³è·‘æ­¥ã€‚

æœ¬ç« å°†å¸¦ä½ ä»é›¶å¼€å§‹,é€æ­¥ç†è§£:
- LLM å¦‚ä½•ç”Ÿæˆæ–‡æœ¬ (Prefill å’Œ Decode é˜¶æ®µ)
- Attention æœºåˆ¶çš„å·¥ä½œåŸç†å’Œè®¡ç®—å¤æ‚åº¦
- KV Cache å¦‚ä½•å°†å¤æ‚åº¦ä» O(nÂ²) é™åˆ° O(n)
- Chunked Prefill å¦‚ä½•å¤„ç†è¶…é•¿ prompt
- ä¸ºä»€ä¹ˆéœ€è¦ Continuous Batching
- vLLM çš„ä¸‰å±‚æ¶æ„å…¨æ™¯

**å­¦å®Œæœ¬ç« ,ä½ å°†èƒ½å¤Ÿè§£é‡Šä¸ºä»€ä¹ˆ vLLM æ¯”ä¼ ç»Ÿæ–¹æ³•å¿« 24 å€ã€‚**

---

## 5.1 LLM å¦‚ä½•ç”Ÿæˆæ–‡æœ¬

### 5.1.1 è‡ªå›å½’ç”Ÿæˆçš„åŸºæœ¬è¿‡ç¨‹

**LLM çš„æœ¬è´¨**: "Fancy next token predictors" (èŠ±å“¨çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹å™¨)

```
ç”¨æˆ·è¾“å…¥: "The capital of France is"

æ¨¡å‹æ€è€ƒ: å‰é¢æ˜¯"æ³•å›½çš„é¦–éƒ½æ˜¯",ä¸‹ä¸€ä¸ªè¯æœ€å¯èƒ½æ˜¯ä»€ä¹ˆ?

æ¨¡å‹è¾“å‡º: "Paris"
```

**ç”Ÿæˆè¿‡ç¨‹**:

```
æ­¥éª¤ 1: è¾“å…¥æ•´ä¸ª prompt
è¾“å…¥: "Hello, my name is"
â†“
æ¨¡å‹å¤„ç†æ•´ä¸ª prompt
â†“
ç”Ÿæˆç¬¬ 1 ä¸ª token: " John"

æ­¥éª¤ 2: æ·»åŠ æ–° token,å†æ¬¡é¢„æµ‹
è¾“å…¥: "Hello, my name is John"
â†“
æ¨¡å‹å¤„ç†æ•´ä¸ªåºåˆ— (åŒ…æ‹¬ä¹‹å‰æ‰€æœ‰çš„ token)
â†“
ç”Ÿæˆç¬¬ 2 ä¸ª token: " and"

æ­¥éª¤ 3: é‡å¤...
è¾“å…¥: "Hello, my name is John and"
â†“
ç”Ÿæˆç¬¬ 3 ä¸ª token: " I"
...

æ­¥éª¤ n: ç”Ÿæˆ <eos> (end of sequence)
åœæ­¢ç”Ÿæˆ
```

**å…³é”®è§‚å¯Ÿ**:
- ç¬¬ä¸€ä¸ª token å‡ºç°è¾ƒæ…¢ (éœ€è¦å¤„ç†æ•´ä¸ª prompt)
- åç»­ token é€ä¸ªå‡ºç° (æ¯æ¬¡åªç”Ÿæˆ 1 ä¸ª)
- æ¯æ¬¡ç”Ÿæˆæ–° token æ—¶,éƒ½éœ€è¦é‡æ–°è¯»å–ä¹‹å‰æ‰€æœ‰å†…å®¹

---

### 5.1.2 Prefill é˜¶æ®µ: å¹¶è¡Œå¤„ç† prompt

**å®šä¹‰**: å¤„ç†åˆå§‹ prompt,ç”Ÿæˆç¬¬ä¸€ä¸ª token çš„é˜¶æ®µ

```
è¾“å…¥: "Once upon a time, there was a"
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            10 ä¸ª tokens

æ¨¡å‹: ä¸€æ¬¡ forward pass å¤„ç†å…¨éƒ¨ 10 ä¸ª tokens
     å¹¶è¡Œè®¡ç®—æ‰€æœ‰ token çš„è¡¨ç¤º
â†“
è¾“å‡º: ç¬¬ä¸€ä¸ª token: " little"
```

**ç‰¹ç‚¹**:
- âœ… **è®¡ç®—å¯†é›†å‹**: å¤§é‡çŸ©é˜µä¹˜æ³•
- âœ… **å¯ä»¥å¹¶è¡Œå¤„ç†**: æ‰€æœ‰ token åŒæ—¶è®¡ç®—
- â±ï¸ **æ—¶é—´**: TTFT (Time To First Token), é¦–å­—å»¶è¿Ÿ

**ä¸ºä»€ä¹ˆ Prefill å¯ä»¥å¹¶è¡Œ?**
- æ‰€æœ‰è¾“å…¥ token æ˜¯å·²çŸ¥çš„
- ä¸éœ€è¦è€ƒè™‘å› æœå…³ç³»
- Attention çŸ©é˜µå¯ä»¥ä¸€æ¬¡æ€§è®¡ç®—

**ç¤ºä¾‹**:
```
Prompt: 100 tokens
GPU RTX 4090:
- Prefill æ—¶é—´: ~200ms (ä¸€æ¬¡å¤„ç† 100 ä¸ª tokens)
```

---

### 5.1.3 Decode é˜¶æ®µ: é€ token ç”Ÿæˆ

**å®šä¹‰**: é€ä¸ªç”Ÿæˆåç»­ token çš„é˜¶æ®µ

```
æ­¥éª¤ 1:
è¾“å…¥: "Once upon a time, there was a little"
      â†“ æ¨¡å‹å¤„ç†
è¾“å‡º: " girl"

æ­¥éª¤ 2:
è¾“å…¥: "Once upon a time, there was a little girl"
      â†“ æ¨¡å‹å¤„ç†
è¾“å‡º: " who"

æ­¥éª¤ 3:
è¾“å…¥: "Once upon a time, there was a little girl who"
      â†“ æ¨¡å‹å¤„ç†
è¾“å‡º: " lived"

...é‡å¤ 100 æ¬¡...
```

**ç‰¹ç‚¹**:
- ğŸŒ **å†…å­˜å¸¦å®½å¯†é›†å‹**: æ¯æ¬¡åªç”Ÿæˆ 1 ä¸ª token
- âŒ **æ— æ³•å¹¶è¡Œ**: å¿…é¡»é€ä¸ªç”Ÿæˆ (å› æœå…³ç³»)
- â±ï¸ **æ—¶é—´**: TBT (Time Between Tokens), å­—é—´å»¶è¿Ÿ

**ä¸ºä»€ä¹ˆ Decode ä¸èƒ½å¹¶è¡Œ?**
- æ¯ä¸ªæ–° token ä¾èµ–äºä¹‹å‰ç”Ÿæˆçš„ token
- å¿…é¡»ç­‰å¾…å‰ä¸€ä¸ª token ç”Ÿæˆå®Œæˆ
- è¿™æ˜¯è‡ªå›å½’æ¨¡å‹çš„æœ¬è´¨é™åˆ¶

**ç¤ºä¾‹**:
```
ç”Ÿæˆ 100 tokens:
- æ¯ä¸ª token: ~20ms
- æ€»æ—¶é—´: 100 Ã— 20ms = 2000ms
- æ˜¯ Prefill çš„ 10 å€!
```

---

### 5.1.4 å›¾è§£å®Œæ•´æµç¨‹

```
æ—¶é—´çº¿:
0ms      200ms   220ms   240ms   260ms   ...   2200ms
â”‚        â”‚       â”‚       â”‚       â”‚             â”‚
â””â”€â”€Prefillâ”¬â”€Decode1â”¬â”€Decode2â”¬â”€Decode3â”€ ... â”€Decode100â”€
         â”‚       â”‚       â”‚       â”‚
         â”‚       â”‚       â”‚       â””â”€ ç”Ÿæˆ "lived"
         â”‚       â”‚       â””â”€ ç”Ÿæˆ "who"
         â”‚       â””â”€ ç”Ÿæˆ "girl"
         â””â”€ ç”Ÿæˆ "little" (ç¬¬ä¸€ä¸ª token)

Prefill é˜¶æ®µ:
- è¾“å…¥: 10 tokens
- æ—¶é—´: 200ms
- è®¡ç®—: å¹¶è¡Œå¤„ç†

Decode é˜¶æ®µ:
- è¾“å…¥: æ¯æ¬¡åŠ  1 ä¸ª token
- æ—¶é—´: æ¯ä¸ª 20ms
- è®¡ç®—: ä¸²è¡Œç”Ÿæˆ 100 ä¸ª tokens

æ€»æ—¶é—´: 200ms + 100 Ã— 20ms = 2200ms
```

**ä¼˜åŒ–æ–¹å‘**:
- **Prefill**: ä¼˜åŒ–è®¡ç®— (æ›´å¿«çš„ GPU, æ›´å¥½çš„å†…æ ¸)
- **Decode**: ä¼˜åŒ–å†…å­˜å¸¦å®½ (KV Cache, PagedAttention)

---

## 5.2 Attention æœºåˆ¶è¯¦è§£

> **ğŸ’¡ ä¸ºä»€ä¹ˆé‡è¦**: Attention æ˜¯å”¯ä¸€è®©ä¸åŒ token äº§ç”Ÿäº¤äº’çš„åœ°æ–¹ã€‚ç†è§£ Attention,å°±ç†è§£äº† LLM çš„æ ¸å¿ƒã€‚

### 5.2.1 Token çš„è¡¨ç¤º: å‘é‡ä¸ hidden dimension

**Tokenization**: æ–‡æœ¬ â†’ token åºåˆ—

```
æ–‡æœ¬: "Hello, world!"
â†“ Tokenizer
Tokens: [15496, 11, 2159, 0]
       â”‚      â”‚   â”‚    â”‚
     Hello    ,  world  <eos>
```

**Embedding**: æ¯ä¸ª token â†’ d ç»´å‘é‡

```
Tokens: [15496, 11, 2159, 0]
       â”‚      â”‚    â”‚    â”‚
       â–¼      â–¼    â–¼    â–¼
Embeddings:
  Token 15496 â†’ [0.12, -0.34, 0.56, ..., 0.78]  (d=4096)
  Token 11    â†’ [-0.23, 0.45, -0.67, ..., 0.89]
  Token 2159  â†’ [0.34, -0.56, 0.78, ..., -0.12]
  Token 0     â†’ [-0.45, 0.67, -0.89, ..., 0.23]
```

**Tensor å½¢çŠ¶**: `[batch_size, sequence_length, hidden_dim]`

```python
import torch

# 7 ä¸ª tokens
input_ids = torch.randint(0, 32000, (1, 7))  # [batch=1, seq_len=7]
# å½¢çŠ¶: [1, 7, 4096] (å‡è®¾ hidden_dim=4096)
embeddings = model.embeddings(input_ids)
print(embeddings.shape)  # torch.Size([1, 7, 4096])
```

**Hidden Dimension**: æ¨¡å‹çš„"è¡¨ç¤ºèƒ½åŠ›"
- GPT-2: d = 768 æˆ– 1024
- Llama-2-7B: d = 4096
- Llama-2-70B: d = 8192

---

### 5.2.2 Queryã€Keyã€Value æŠ•å½±

**ä¸‰ä¸ªæƒé‡çŸ©é˜µ**: Wqã€Wkã€Wv

```python
# æ¯ä¸ª token çš„è¡¨ç¤º: x
x = embeddings[i]  # shape: [hidden_dim]

# æŠ•å½±åˆ° Qã€Kã€V
Q = x @ Wq  # Query: è¿™ä¸ª token æƒ³æ‰¾ä»€ä¹ˆ?
K = x @ Wk  # Key: è¿™ä¸ª token èƒ½æä¾›ä»€ä¹ˆ?
V = x @ Wv  # Value: è¿™ä¸ª token çš„å®é™…å†…å®¹

# Wq, Wk, Wv çš„å½¢çŠ¶: [hidden_dim, head_dim]
# Q, K, V çš„å½¢çŠ¶: [head_dim]
```

**ç‰©ç†æ„ä¹‰**:

```
Token: "apple"
- Query: "æˆ‘æ˜¯æ°´æœ,æˆ‘æƒ³æ‰¾ä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„ä¿¡æ¯"
- Key: "æˆ‘æ˜¯æ°´æœ,æˆ‘å¯ä»¥è¢«ä¸é£Ÿç‰©ç›¸å…³çš„æŸ¥è¯¢æ‰¾åˆ°"
- Value: "æˆ‘çš„å…·ä½“è¯­ä¹‰å†…å®¹æ˜¯'è‹¹æœ'"

Token: "company"
- Query: "æˆ‘æ˜¯ç»„ç»‡,æˆ‘æƒ³æ‰¾ä¸å•†ä¸šç›¸å…³çš„ä¿¡æ¯"
- Key: "æˆ‘æ˜¯ç»„ç»‡,æˆ‘å¯ä»¥è¢«ä¸å•†ä¸šç›¸å…³çš„æŸ¥è¯¢æ‰¾åˆ°"
- Value: "æˆ‘çš„å…·ä½“è¯­ä¹‰å†…å®¹æ˜¯'å…¬å¸'"
```

**å¤šå¤´ Attention** (Multi-Head Attention):

```python
# 32 ä¸ª attention heads
num_heads = 32
head_dim = hidden_dim // num_heads  # 4096 // 32 = 128

# æ¯ä¸ª head å­¦ä¹ ä¸åŒçš„å…³ç³»æ¨¡å¼
# Head 1: å…³æ³¨è¯­æ³•å…³ç³»
# Head 2: å…³æ³¨è¯­ä¹‰å…³ç³»
# Head 3: å…³æ³¨æŒ‡ä»£å…³ç³»
# ...
```

---

### 5.2.3 Attention è®¡ç®—: QK^T ä¸äºŒæ¬¡å¤æ‚åº¦

**è®¡ç®—æ­¥éª¤**:

```python
# è¾“å…¥: Q, K, V
# å½¢çŠ¶: [batch, num_heads, seq_len, head_dim]
Q, K, V = ...

# æ­¥éª¤ 1: è®¡ç®— Q @ K^T
# ç›¸ä¼¼åº¦çŸ©é˜µ: æ¯ä¸ª token å¯¹å…¶ä»– token çš„å…³æ³¨åº¦
scores = Q @ K.transpose(-2, -1)  # [batch, num_heads, seq_len, seq_len]

# æ­¥éª¤ 2: ç¼©æ”¾
scores = scores / (head_dim ** 0.5)

# æ­¥éª¤ 3: Softmax å½’ä¸€åŒ–
attn_weights = torch.softmax(scores, dim=-1)

# æ­¥éª¤ 4: åŠ æƒæ±‚å’Œ
output = attn_weights @ V  # [batch, num_heads, seq_len, head_dim]
```

**å¤æ‚åº¦åˆ†æ**:

```
Q @ K^T:
- Q: [n, d]
- K^T: [d, n]
- ç»“æœ: [n, n]
- è®¡ç®—: n Ã— d Ã— n = O(nÂ²Â·d)

Softmax:
- è¾“å…¥: [n, n]
- è®¡ç®—: O(nÂ²)

attn_weights @ V:
- attn_weights: [n, n]
- V: [n, d]
- ç»“æœ: [n, d]
- è®¡ç®—: n Ã— n Ã— d = O(nÂ²Â·d)

æ€»å¤æ‚åº¦: O(nÂ²Â·d)
```

**å…³é”®æ´å¯Ÿ**: Attention çš„äºŒæ¬¡å¤æ‚åº¦æ˜¯æ€§èƒ½ç“¶é¢ˆ!

```
åºåˆ—é•¿åº¦ n = 1000:
- Attention è®¡ç®—: 1000Â² Ã— 4096 = 4,096,000,000 æ¬¡è¿ç®—

åºåˆ—é•¿åº¦ n = 10000:
- Attention è®¡ç®—: 10000Â² Ã— 4096 = 409,600,000,000 æ¬¡è¿ç®— (100å€!)
```

---

### 5.2.4 Attention Mask: æ§åˆ¶ token äº¤äº’

**ä»€ä¹ˆæ˜¯ Mask**: å¸ƒå°”çŸ©é˜µ,å†³å®šå“ªäº› token å¯ä»¥äº¤äº’

```python
# Attention Mask
# shape: [seq_len, seq_len]
mask = torch.tensor([
    [True,  True,  True,  True],  # Token 0 å¯ä»¥ attend to 0,1,2,3
    [False, True,  True,  True],  # Token 1 å¯ä»¥ attend to 1,2,3
    [False, False, True,  True],  # Token 2 å¯ä»¥ attend to 2,3
    [False, False, False, True],  # Token 3 åªèƒ½ attend to 3
])
```

**ä½œç”¨**: Mask=False çš„ä½ç½®,attention æƒé‡ = 0

```python
# åº”ç”¨ mask
scores = Q @ K.T
scores = scores.masked_fill(~mask, float('-inf'))
attn_weights = torch.softmax(scores, dim=-1)
```

**å¯è§†åŒ–æ–¹æ³•**:
```
ç»¿è‰²æ–¹å— (True)  = å¯ä»¥äº¤äº’
ç™½è‰²æ–¹å— (False) = ä¸èƒ½äº¤äº’
```

---

### 5.2.5 Causal Mask: å› æœå…³ç³»çš„å¯è§†åŒ–

**å®šä¹‰**: æ¯ä¸ª token åªèƒ½ä¸ä¹‹å‰çš„ token äº¤äº’

**ç›´è§‰**: å› å¿…é¡»åœ¨æœä¹‹å‰

```
Tokenåºåˆ—: <bos>  I     am    sure

Token 0 (<bos>):
  å¯ä»¥ attend to: [<bos>]
  Mask:          [ âœ“ ]

Token 1 (I):
  å¯ä»¥ attend to: [<bos>, I]
  Mask:          [  âœ“ , âœ“ ]

Token 2 (am):
  å¯ä»¥ attend to: [<bos>, I, am]
  Mask:          [  âœ“ , âœ“ , âœ“ ]

Token 3 (sure):
  å¯ä»¥ attend to: [<bos>, I, am, sure]
  Mask:          [  âœ“ , âœ“ , âœ“ , âœ“ ]
```

**Mask å½¢çŠ¶**: ä¸‹ä¸‰è§’çŸ©é˜µ

```python
def generate_causal_mask(seq_len):
    """ç”Ÿæˆ causal mask"""
    mask = torch.tril(torch.ones(seq_len, seq_len, dtype=bool))
    return mask

# ç¤ºä¾‹: seq_len = 4
mask = generate_causal_mask(4)
print(mask)
# tensor([[ True, False, False, False],
#         [ True,  True, False, False],
#         [ True,  True,  True, False],
#         [ True,  True,  True,  True]])
```

**è¯» mask æ–¹æ³•**:
- **è¡Œ**: å½“å‰ token
- **åˆ—**: å†å² token
- **True**: å½“å‰ token å¯ä»¥ attend åˆ°è¯¥å†å² token

---

### 5.2.6 ä¸ºä»€ä¹ˆ Attention æ˜¯å”¯ä¸€è®© token äº¤äº’çš„åœ°æ–¹

**å…¶ä»–æ“ä½œ**: token-wise,æ¯ä¸ª token ç‹¬ç«‹å¤„ç†

```python
# Layer Normalization
x = LayerNorm(x)  # æ¯ä¸ª token ç‹¬ç«‹å½’ä¸€åŒ–

# æ¿€æ´»å‡½æ•°
x = GELU(x)  # æ¯ä¸ª token ç‹¬ç«‹æ¿€æ´»

# çŸ©é˜µä¹˜æ³• (é Attention)
x = x @ W  # æ¯ä¸ª token ç‹¬ç«‹æŠ•å½±
```

**Attention çš„ä½œç”¨**: è®© token ä¹‹é—´"äº¤æµ"

```python
# Attention
output = Attention(Q, K, V)  # token ä¹‹é—´èšåˆä¿¡æ¯!
```

**ç»“è®º**: ç†è§£äº† attention mask,å°±ç†è§£äº† LLM çš„ä¿¡æ¯æµ

---

## 5.3 ä»æœ´ç´ ç”Ÿæˆåˆ° KV Cache

### 5.3.1 æœ´ç´ æ–¹æ³•: æ¯æ¬¡é‡æ–°è®¡ç®— (O(nÂ²))

**é—®é¢˜åœºæ™¯**: ç”Ÿæˆç¬¬ n+1 ä¸ª token

**æœ´ç´ åšæ³•**:
```
å·²ç”Ÿæˆ: "Hello, my name is John" (7 tokens)
ç›®æ ‡:   ç”Ÿæˆç¬¬ 8 ä¸ª token

æœ´ç´ æ–¹æ³•:
1. å°†æ‰€æœ‰ 8 ä¸ª tokens é‡æ–°è¾“å…¥æ¨¡å‹
   ["Hello", ",", "my", "name", "is", "John", æ–°token]

2. é‡æ–°è®¡ç®—æ‰€æœ‰ token çš„ K å’Œ V

3. åªä½¿ç”¨æœ€åä¸€ä¸ª token çš„è¾“å‡º
```

**è®¡ç®—å¤æ‚åº¦**:
```
ç”Ÿæˆç¬¬ 1 ä¸ª token:  O(1Â²)
ç”Ÿæˆç¬¬ 2 ä¸ª token:  O(2Â²)
ç”Ÿæˆç¬¬ 3 ä¸ª token:  O(3Â²)
...
ç”Ÿæˆç¬¬ n ä¸ª token:  O(nÂ²)

æ€»å¤æ‚åº¦: O(1Â² + 2Â² + ... + nÂ²) = O(nÂ³)
```

**å¯è§†åŒ–æµªè´¹**:
```
ç¬¬ 2 æ­¥: é‡æ–°è®¡ç®— Token 1 çš„ Kã€V âŒ æµªè´¹
ç¬¬ 3 æ­¥: é‡æ–°è®¡ç®— Token 1, 2 çš„ Kã€V âŒ æµªè´¹
...
ç¬¬ n æ­¥: é‡æ–°è®¡ç®— Token 1, 2, ..., n-1 çš„ Kã€V âŒ æµªè´¹
```

---

### 5.3.2 é‡å¤è®¡ç®—é—®é¢˜çš„å¯è§†åŒ–

**å…³é”®è§‚å¯Ÿ**: æ–° token (å¦‚"will") ä¸å½±å“æ—§ token çš„ attention è®¡ç®—

```
Token åºåˆ—:
["I", "am", "sure", "I", "will"]

Token 4 ("will") ç”Ÿæˆå:

å¯¹äº Token 0 ("I"):
- è®¡ç®— attention æ—¶,åªä¼šçœ‹ Token 0-3
- Token 4 ä¸å½±å“ Token 0 çš„ attention
- åŸå› : Causal mask!

å¯¹äº Token 1 ("am"):
- è®¡ç®— attention æ—¶,åªä¼šçœ‹ Token 0-1
- Token 4 åŒæ ·ä¸å½±å“
```

**åŸå› **: Causal mask,æœªæ¥ token ä¸å½±å“è¿‡å»

```python
# Token 4 çš„ attention mask
mask = [False, False, False, False, True]
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”¬â”€â”€â”˜
      ä¸èƒ½ attend to ä¹‹å‰           åªèƒ½attend toè‡ªå·±
```

**å›¾ç¤º**:
```
æœ€åä¸€ä¸ª token åªå…³å¿ƒè‡ªå·±çš„é¢„æµ‹,
ä¸å½±å“å…¶ä»– token çš„ attention è®¡ç®—!
```

---

### 5.3.3 KV Cache çš„æ ¸å¿ƒæ€æƒ³

**æ ¸å¿ƒæ´å¯Ÿ**: æ—§ token çš„ Kã€V å·²ç»è®¡ç®—è¿‡,ç¼“å­˜èµ·æ¥!

**åšæ³•**:
```
Prefill é˜¶æ®µ:
è¾“å…¥: "Hello, my name is"
è®¡ç®—:
  Token 0: K0, V0
  Token 1: K1, V1
  Token 2: K2, V2
  Token 3: K3, V3
å­˜å‚¨: ç¼“å­˜æ‰€æœ‰ K, V

Decode é˜¶æ®µ - ç¬¬ 1 æ­¥:
è¾“å…¥: æ–° Token 4
è®¡ç®—:
  Token 4: K4, V4
å¤ç”¨: K0, K1, K2, K3 (ä»ç¼“å­˜è¯»å–)
ç»„åˆ: [K0, K1, K2, K3, K4]
      [V0, V1, V2, V3, V4]

Decode é˜¶æ®µ - ç¬¬ 2 æ­¥:
è¾“å…¥: æ–° Token 5
è®¡ç®—:
  Token 5: K5, V5
å¤ç”¨: K0, K1, K2, K3, K4 (ä»ç¼“å­˜è¯»å–)
ç»„åˆ: [K0, K1, K2, K3, K4, K5]
      [V0, V1, V2, V3, V4, V5]
```

**æ•ˆæœ**: é¿å…é‡å¤è®¡ç®—
- âœ… æ¯ä¸ª token çš„ Kã€V åªè®¡ç®—ä¸€æ¬¡
- âœ… å¤§å¹…å‡å°‘è®¡ç®—é‡

**ä»£ä»·**: æ˜¾å­˜å ç”¨ O(n)
- âŒ éœ€è¦å­˜å‚¨æ‰€æœ‰å†å² token çš„ Kã€V
- âŒ åºåˆ—è¶Šé•¿,æ˜¾å­˜å ç”¨è¶Šå¤§

---

### 5.3.4 è®¡ç®—å¤æ‚åº¦é™ä½: ä» O(nÂ²) åˆ° O(n)

**æ—  KV Cache**:
```
æ¯ä¸ª token: O(nÂ²)
æ€»å¤æ‚åº¦: O(nÂ³)
```

**æœ‰ KV Cache**:
```
ç¬¬ 1 ä¸ª token (Prefill): O(nÂ²)
ç¬¬ 2 ä¸ª token (Decode): O(n)  (åªè®¡ç®—æ–° token)
ç¬¬ 3 ä¸ª token (Decode): O(n)
...
ç¬¬ n ä¸ª token (Decode): O(n)

æ€»å¤æ‚åº¦: O(nÂ²) + (n-1) Ã— O(n) = O(nÂ²)
å¹³å‡å¤æ‚åº¦: O(n)
```

**åŠ é€Ÿæ•ˆæœ**: åºåˆ—è¶Šé•¿,åŠ é€Ÿè¶Šæ˜æ˜¾

```
åºåˆ—é•¿åº¦ n = 10:
- æ—  KV Cache: 10Â³ = 1000 æ¬¡è¿ç®—
- æœ‰ KV Cache: 10Â² + 9Ã—10 = 190 æ¬¡è¿ç®—
- åŠ é€Ÿæ¯”: 1000/190 = 5.26x

åºåˆ—é•¿åº¦ n = 100:
- æ—  KV Cache: 100Â³ = 1,000,000 æ¬¡è¿ç®—
- æœ‰ KV Cache: 100Â² + 99Ã—100 = 19,900 æ¬¡è¿ç®—
- åŠ é€Ÿæ¯”: 1,000,000/19,900 = 50.25x

åºåˆ—é•¿åº¦ n = 1000:
- åŠ é€Ÿæ¯”: ~500x!
```

---

### 5.3.5 æ˜¾å­˜ä»£ä»·: æ¯ä¸ª token éœ€è¦å¤šå°‘æ˜¾å­˜?

**å• token çš„ cache å¤§å°**:
```
Size = 2 Ã— L Ã— H Ã— A Ã— bytes

å…¶ä¸­:
- 2: K å’Œ V
- L: å±‚æ•° (å¦‚ 32)
- H: heads æ•° (å¦‚ 32)
- A: head dimension (å¦‚ 128)
- bytes: æ¯ä¸ªå…ƒç´ çš„å­—èŠ‚æ•° (FP16 = 2 bytes)
```

**ç¤ºä¾‹è®¡ç®—**:
```
Llama-2-7B:
- L = 32 å±‚
- H = 32 heads
- A = 128 head_dim
- bytes = 2 (FP16)

å• token cache:
= 2 Ã— 32 Ã— 32 Ã— 128 Ã— 2
= 524,288 bytes
â‰ˆ 0.5 MB/token

1000 tokens:
= 1000 Ã— 0.5 MB
= 500 MB

10000 tokens:
= 10000 Ã— 0.5 MB
= 5000 MB = 5 GB
```

**æƒè¡¡**: ç”¨æ˜¾å­˜æ¢è®¡ç®—
- âœ… è®¡ç®—: å¤§å¹…åŠ é€Ÿ
- âŒ æ˜¾å­˜: çº¿æ€§å¢é•¿

---

## 5.4 Chunked Prefill: å¤„ç†é•¿ prompt

### 5.4.1 é—®é¢˜: å¤§ prompt è¶…è¿‡æ˜¾å­˜

**åœºæ™¯**: Cursor æ·»åŠ æ•´ä¸ªä»£ç ä»“åº“åˆ° prompt

```
ä»£ç ä»“åº“: 10,000 è¡Œä»£ç 
Tokens:   ~100,000 tokens

é—®é¢˜: 100,000 ä¸ª tokens çš„æ¿€æ´»å€¼è¶…è¿‡ GPU æ˜¾å­˜!
```

**çº¦æŸ**: æ¯æ¬¡ forward pass æœ€å¤šå¤„ç† m ä¸ª token

```
GPU: RTX 4090 24GB
æ˜¾å­˜: 24 GB
çº¦æŸ: æ¯æ¬¡ ~4,096 tokens
```

---

### 5.4.2 è§£å†³æ–¹æ¡ˆ: åˆ†å—å¤„ç†

**æ€è·¯**: å°† n ä¸ª token çš„ prompt åˆ†æˆ âŒˆn/mâŒ‰ ä¸ª chunks

**ç¤ºä¾‹**: n=7, m=4 â†’ åˆ†æˆ 2 ä¸ª chunks

```
åŸå§‹: [t0, t1, t2, t3, t4, t5, t6]

Chunk 1: [t0, t1, t2, t3]
Chunk 2: [t4, t5, t6]
```

**å…³é”®**: å¦‚ä½•ä¿æŒä¿¡æ¯è¿ç»­æ€§?

---

### 5.4.3 KV Cache åœ¨ chunked prefill ä¸­çš„ä½œç”¨

**Chunk 1**:
```
è¾“å…¥: [t0, t1, t2, t3]
è®¡ç®—:
  K0, V0
  K1, V1
  K2, V2
  K3, V3
å­˜å‚¨: ç¼“å­˜ K0-3, V0-3
```

**Chunk 2**:
```
è¾“å…¥: [t4, t5, t6]
å¤ç”¨: K0-3, V0-3 (ä»ç¼“å­˜è¯»å–)
è®¡ç®—:
  K4, V4
  K5, V5
  K6, V6
æ‹¼æ¥: [K0, K1, K2, K3, K4, K5, K6]
      [V0, V1, V2, V3, V4, V5, V6]
```

**Attention mask è°ƒæ•´**: ç¡®ä¿è·¨ chunk çš„ token æ­£ç¡®äº¤äº’

```python
# Chunk 1 mask (4 tokens)
mask1 = [
    [True,  False, False, False],
    [True,  True,  False, False],
    [True,  True,  True,  False],
    [True,  True,  True,  True ],
]

# Chunk 2 mask (3 tokens, but can attend to chunk 1)
mask2 = [
    [True,  True,  True,  True,  True,  False, False],  # t4
    [True,  True,  True,  True,  True,  True,  False],  # t5
    [True,  True,  True,  True,  True,  True,  True ],  # t6
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      Chunk 1         Chunk 2
```

---

### 5.4.4 å›¾è§£åˆ†å—å¤„ç†æµç¨‹

**æ—  chunked prefill**:
```
è¾“å…¥: 100,000 tokens
é—®é¢˜: æ˜¾å­˜ä¸è¶³! ğŸ’¥
```

**æœ‰ chunked prefill**:
```
Chunk 1: å¤„ç† tokens 0-4095
  â†’ è®¡ç®— K0-4095, V0-4095
  â†’ ç¼“å­˜åˆ° GPU å†…å­˜

Chunk 2: å¤„ç† tokens 4096-8191
  â†’ å¤ç”¨ K0-4095, V0-4095
  â†’ è®¡ç®— K4096-8191, V4096-8191
  â†’ æ‹¼æ¥å®Œæ•´ cache

Chunk 3: å¤„ç† tokens 8192-12287
  â†’ å¤ç”¨ K0-8191, V0-8191
  â†’ è®¡ç®— K8192-12287, V8192-12287
  â†’ æ‹¼æ¥å®Œæ•´ cache

...é‡å¤ 25 æ¬¡ (100,000 / 4,096)

å®Œæˆ: æ•´ä¸ª 100,000 tokens çš„ KV cache
```

**çµæ´»æ€§**: å¯æ ¹æ®å†…å­˜çº¦æŸåŠ¨æ€è°ƒæ•´ chunk å¤§å°

```
GPU å†…å­˜å……è¶³: chunk_size = 8192
GPU å†…å­˜ç´§å¼ : chunk_size = 2048
```

---

## 5.5 æ‰¹å¤„ç†çš„æŒ‘æˆ˜: ä»é™æ€åˆ°åŠ¨æ€

### 5.5.1 é™æ€æ‰¹å¤„ç†

**ç›®æ ‡**: æé«˜ååé‡ (throughput)

**æ–¹æ³•**: å°†å¤šä¸ª prompt æ‰“åŒ…æˆä¸€ä¸ª batch

```python
# 3 ä¸ª prompt
prompt1 = "Hello"
prompt2 = "Hi there, how are you doing today?"
prompt3 = "Hey"

# é—®é¢˜: é•¿åº¦ä¸ä¸€è‡´!
```

**çº¦æŸ**: æ‰€æœ‰ prompt å¿…é¡»æœ‰ç›¸åŒé•¿åº¦

**è§£å†³æ–¹æ¡ˆ**: å·¦ä¾§ padding,å³ä¾§å¯¹é½

```python
# Padding
prompt1 = "<pad><pad><pad><pad><pad><pad><pad><pad>Hello"
prompt2 = "Hi there, how are you doing today?"
prompt3 = "<pad><pad><pad><pad><pad><pad><pad><pad><pad>Hey"

# ç»Ÿä¸€é•¿åº¦: max(len(prompt1, prompt2, prompt3)) = 36
```

---

### 5.5.2 Padding çš„é—®é¢˜: è®¡ç®—æµªè´¹

**Padding ä½ç½®**: å·¦ä¾§ (æ·»åŠ `<pad>` token)

**Attention mask**: padding ä½ç½®è®¾ä¸º False

```python
# Prompt 1 çš„ attention mask
mask1 = [
    [False, False, False, ..., True,  True],  # "H"
    [False, False, False, ..., True,  True,  True],  # "e"
    [False, False, False, ..., True,  True,  True,  True],  # "l"
    [False, False, False, ..., True,  True,  True,  True,  True],  # "l"
    [False, False, False, ..., True,  True,  True,  True,  True,  True],  # "o"
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     padding (æµªè´¹)          å®é™…å†…å®¹
]
```

**é—®é¢˜**: padding token å ç”¨äº†è®¡ç®—èµ„æº,ä½†æ²¡æœ‰å®é™…è´¡çŒ®

```python
# GPU ä»ç„¶è®¡ç®— padding tokens çš„ attention!
# è™½ç„¶ç»“æœè¢« mask æ‰,ä½†è®¡ç®—é‡æ²¡æœ‰å‡å°‘
```

---

### 5.5.3 ä¸åŒåºåˆ—é•¿åº¦çš„å›°å¢ƒ

**åœºæ™¯**: batch ä¸­æœ‰å¤šä¸ª prompt,é•¿åº¦å·®å¼‚å¤§

```python
batch = [
    "Hi",          # 2 tokens
    "Hello",       # 3 tokens
    "How are you?", # 12 tokens
]

# éœ€è¦å…¨éƒ¨ padding åˆ° 12 tokens
padded_batch = [
    "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>Hi",
    "<pad><pad><pad><pad><pad><pad><pad><pad><pad>Hello",
    "How are you?",
]
```

**é—®é¢˜ 1**: çŸ­ prompt å®Œæˆå,é•¿ prompt è¿˜åœ¨ç”Ÿæˆ

```
æ—¶é—´çº¿:
0ms    100ms   200ms   300ms   ...   1200ms
â”‚      â”‚       â”‚       â”‚             â”‚
Hi âœ“   Hi âœ“    Hi âœ“    ...          Hi âœ“  (å®Œæˆ)
       Hello âœ“ Hello âœ“ ...          Hello âœ“  (å®Œæˆ)
                How ...         How are you? âœ“  (å®Œæˆ)

é—®é¢˜: "Hi" å’Œ "Hello" å®Œæˆå,
      ä»åœ¨ batch ä¸­ç­‰å¾…"How are you?"å®Œæˆ
      â†’ GPU è®¡ç®—æµªè´¹åœ¨ padding ä¸Š!
```

**é—®é¢˜ 2**: åŠ¨æ€è°ƒåº¦å¼•å…¥å¤§é‡ padding

```
Decode é˜¶æ®µ:
- æ­£åœ¨ decode çš„ prompt æ¯æ¬¡åªåŠ  1 ä¸ª token
- æ–°åŠ å…¥çš„ prompt éœ€è¦ prefill å¾ˆå¤š tokens

Batch çŠ¶æ€:
  Request 1: å·²ç”Ÿæˆ 100 tokens (decode é˜¶æ®µ)
  Request 2-8: å·²ç”Ÿæˆ 50 tokens (decode é˜¶æ®µ)
  Request 9: æ–°åŠ å…¥,æœ‰ 1000 tokens çš„ prompt

é—®é¢˜:
  - Request 9 éœ€è¦ prefill 1000 tokens
  - Request 1-8 åªéœ€è¦ decode 1 ä¸ª token
  - å¦‚ä½•ç»„ç»‡ batch?
```

Padding æ•°é‡ = (n-1) Ã— (B-1)

---

### 5.5.4 ç¤ºä¾‹: ä¸ºä»€ä¹ˆ padding æˆæœ¬éš batch å’Œé•¿åº¦äºŒæ¬¡å¢é•¿

**å‚æ•°**:
```
B = 8   (batch ä¸­ 8 ä¸ª prompt åœ¨ decode)
n = 100 (æ–° prompt æœ‰ 100 ä¸ª token)
```

**è®¡ç®— padding**:
```
æ–° prompt (Request 9):
- éœ€è¦ prefill 100 tokens
- å…¶ä»– 7 ä¸ª request åªåŠ  1 ä¸ª token

Padding æ•°é‡:
= (100 - 1) Ã— (8 - 1)
= 99 Ã— 7
= 693 ä¸ª padding tokens!

å®é™…è®¡ç®—:
- Request 9: 100 tokens (å®é™…å†…å®¹)
- Request 1-8: æ¯ä¸ª 99 ä¸ª padding tokens + 1 ä¸ªå®é™… token
- æ€»è®¡: 100 + 8Ã—1 + 7Ã—99 = 791 tokens
- æœ‰æ•ˆ: 100 + 8 = 108 tokens
- æµªè´¹: 791 - 108 = 683 tokens (86%!)
```

**ç»“è®º**: åŠ¨æ€è°ƒåº¦ + ä¼ ç»Ÿ batching = ç¾éš¾

---

## 5.6 Continuous Batching å…¥é—¨ â­

> **ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ**: å»æ‰ batch ç»´åº¦,ç”¨ attention mask æ§åˆ¶ token äº¤äº’,è®© GPU æ—¶åˆ»æ»¡è½½ã€‚

### 5.6.1 æ ¸å¿ƒæ€æƒ³: å»æ‰ batch ç»´åº¦

**é—®é¢˜æ ¹æº**: batch ç»´åº¦å¼•å…¥äº† padding

**æ¿€è¿›æƒ³æ³•**: ä¸è¦ batch ç»´åº¦!

**æ›¿ä»£æ–¹æ¡ˆ**: æ‹¼æ¥æ‰€æœ‰ prompt

```python
# ä¼ ç»Ÿ batching
batch = [
    [token1, token2, token3],
    [token4, token5],
]
shape: [2, 3]  # batch_size=2, seq_len=3 (padding!)

# æ–°æ–¹æ³•: æ‹¼æ¥
sequence = [token1, token2, token3, token4, token5]
shape: [5]  # åªæœ‰ seq_len!
```

**æ–°é—®é¢˜**: å¦‚ä½•é˜²æ­¢ä¸åŒ prompt çš„ token äº’ç›¸å¹²æ‰°?

---

### 5.6.2 Ragged Batching: ç”¨ attention mask æ§åˆ¶äº¤äº’

**æ–¹æ³•**:
1. å°†å¤šä¸ª prompt æ‹¼æ¥æˆä¸€ä¸ªåºåˆ—
2. ç”¨ attention mask æ§åˆ¶ token äº¤äº’
3. Prompt A çš„ token ä¸èƒ½ attend to Prompt B çš„ token

**Mask å½¢çŠ¶**: å—å¯¹è§’çŸ©é˜µ (block-diagonal)

**å¯è§†åŒ–**:
```
Prompt A (3 tokens): [A1, A2, A3]
Prompt B (2 tokens): [B1, B2]

æ‹¼æ¥: [A1, A2, A3, B1, B2]

Attention Mask:
       A1   A2   A3   B1   B2
A1:   [âœ“]  [ ]  [ ]  [ ]  [ ]
A2:   [âœ“]  [âœ“]  [ ]  [ ]  [ ]
A3:   [âœ“]  [âœ“]  [âœ“]  [ ]  [ ]
B1:   [ ]  [ ]  [ ]  [âœ“]  [ ]
B2:   [ ]  [ ]  [ ]  [âœ“]  [âœ“]

å—å¯¹è§’ç»“æ„:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ A A â”‚     â”‚
â”‚ A A â”‚     â”‚
â”‚ A A â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚ B B â”‚
â”‚     â”‚ B B â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```

**ä¼˜åŠ¿**: æ—  padding,æ‰€æœ‰è®¡ç®—éƒ½æœ‰æ„ä¹‰

```python
# GPU è®¡ç®—çš„æ¯ä¸ª token éƒ½æ˜¯å®é™…éœ€è¦çš„!
# æ²¡æœ‰ padding æµªè´¹
```

---

### 5.6.3 Dynamic Scheduling: åŠ¨æ€æ›¿æ¢å®Œæˆçš„è¯·æ±‚

**åœºæ™¯**: æŸä¸ª prompt ç”Ÿæˆ `<eos>`

**åŠ¨ä½œ**:
```
æ­¥éª¤ 1: æ£€æµ‹åˆ° Request A å®Œæˆ
æ­¥éª¤ 2: ç«‹å³ä» batch ä¸­ç§»é™¤ Request A
æ­¥éª¤ 3: ç”¨ç­‰å¾…ä¸­çš„ Request C æ›¿æ¢
æ­¥éª¤ 4: é‡æ–°æ„å»º attention mask
```

**ç›®æ ‡**: ä¿æŒ GPU æ—¶åˆ»æ»¡è½½

**å…³é”®**: Ragged batching è®©æ›¿æ¢æˆæœ¬ä½

```
ä¼ ç»Ÿ batching (éœ€è¦é‡æ–° padding):
- ç§»é™¤ Request A
- é‡æ–°è®¡ç®—æœ€å¤§é•¿åº¦
- é‡æ–° padding æ‰€æœ‰è¯·æ±‚
- æˆæœ¬: é«˜!

Continuous Batching (åªéœ€é‡å»º mask):
- ç§»é™¤ Request A çš„ tokens
- è¿½åŠ  Request C çš„ tokens
- é‡å»º block-diagonal mask
- æˆæœ¬: ä½!
```

---

### 5.6.4 æ··åˆ Prefill å’Œ Decode: æœ€å¤§åŒ– throughput

**æŒ‘æˆ˜**:
```
Decode é˜¶æ®µçš„ prompt:
- æ¯æ¬¡åªåŠ  1 ä¸ª token
- å¿«é€Ÿ,å ç”¨å°‘é‡ GPU

æ–°åŠ å…¥çš„ prompt:
- éœ€è¦ prefill å¾ˆå¤š tokens
- æ…¢,å ç”¨å¤§é‡ GPU

å¦‚ä½•å¹³è¡¡?
```

**è°ƒåº¦ç®—æ³•**:
```
ç›®æ ‡: æ¯ä¸ª batch è¾¾åˆ° m ä¸ª token (memory budget)

æ­¥éª¤ 1: ç»Ÿè®¡å½“å‰ decode é˜¶æ®µçš„è¯·æ±‚æ•°
  decode_requests = 10
  decode_tokens = 10 Ã— 1 = 10

æ­¥éª¤ 2: è®¡ç®—å‰©ä½™ç©ºé—´
  remaining = m - decode_tokens
  remaining = 1000 - 10 = 990

æ­¥éª¤ 3: ç”¨ chunked prefill åŠ å…¥æ–°è¯·æ±‚
  new_request_tokens = 100
  if remaining >= new_request_tokens:
    # å¯ä»¥åŠ å…¥æ•´ä¸ª request
    add_request(new_request)
  else:
    # åª prefill ä¸€ä¸ª chunk
    chunk_size = remaining
    add_chunk(new_request, chunk_size)
```

**ç¤ºä¾‹**:
```
Memory budget: m = 1000

å½“å‰çŠ¶æ€:
- 10 ä¸ª decode requests â†’ 10 ä¸ª tokens

æ–°è¯·æ±‚:
- Request A: 100 tokens
- Request B: 200 tokens
- Request C: 500 tokens

è°ƒåº¦:
- åŠ å…¥ Request A: 10 + 100 = 110 tokens
- åŠ å…¥ Request B: 110 + 200 = 310 tokens
- åŠ å…¥ Request C (chunk 1): 310 + 500 = 810 tokens
- å‰©ä½™: 1000 - 810 = 190 tokens
- åŠ å…¥ Request C (chunk 2): 810 + 190 = 1000 tokens (æ»¡!)

GPU åˆ©ç”¨ç‡: 1000/1000 = 100% âœ…
```

---

### 5.6.5 å®Œæ•´çš„ Continuous Batching æµç¨‹å›¾

```
æ­¥éª¤ 1: åˆå§‹ batch
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request A (å·²ç”Ÿæˆ 50 tokens)        â”‚
â”‚ Request B (å·²ç”Ÿæˆ 30 tokens)        â”‚
â”‚ Request C (å·²ç”Ÿæˆ 20 tokens)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ­¥éª¤ 2: æŸä¸ªè¯·æ±‚å®Œæˆ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request A (ç”Ÿæˆ <eos>) âœ“            â”‚
â”‚ Request B (å·²ç”Ÿæˆ 31 tokens)        â”‚
â”‚ Request C (å·²ç”Ÿæˆ 21 tokens)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ­¥éª¤ 3: ç§»é™¤å®Œæˆçš„è¯·æ±‚,åŠ å…¥æ–°è¯·æ±‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request D (æ–°,éœ€è¦ prefill 100)     â”‚
â”‚ Request B (å·²ç”Ÿæˆ 32 tokens)        â”‚
â”‚ Request C (å·²ç”Ÿæˆ 22 tokens)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ­¥éª¤ 4: Chunked prefill + Decode
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request D (prefill chunk 1: 70)     â”‚
â”‚ Request B (decode: +1 token)        â”‚
â”‚ Request C (decode: +1 token)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ­¥éª¤ 5: Forward pass
â†’ GPU å¤„ç† 70 + 1 + 1 = 72 tokens
â†’ ç”Ÿæˆæ–° tokens

æ­¥éª¤ 6: å¾ªç¯å›åˆ°æ­¥éª¤ 2
```

---

### 5.6.6 Continuous Batching vs ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”

**Static Batching**:
```
ä¼˜ç‚¹:
  âœ… ç®€å•,æ˜“äºå®ç°
  âœ… é€‚åˆå›ºå®šé•¿åº¦çš„æ‰¹å¤„ç†

ç¼ºç‚¹:
  âŒ å¤§é‡ padding (50-90%)
  âŒ ååé‡ä½
  âŒ GPU åˆ©ç”¨ç‡ä½
```

**Dynamic Batching**:
```
ä¼˜ç‚¹:
  âœ… åŠ¨æ€è°ƒæ•´
  âœ… æ¯”é™æ€ batching çµæ´»

ç¼ºç‚¹:
  âŒ padding ä»ç„¶ä¸¥é‡
  âŒ é¢‘ç¹çš„é‡æ–° padding
  âŒ éš¾ä»¥ä¼˜åŒ–
```

**Continuous Batching** (vLLM):
```
ä¼˜ç‚¹:
  âœ… æ—  padding
  âœ… GPU åˆ©ç”¨ç‡æœ€é«˜ (å¯è¾¾ 95%+)
  âœ… ååé‡æå‡ 3-5 å€
  âœ… æ”¯æŒåŠ¨æ€è°ƒåº¦

ç¼ºç‚¹:
  âŒ å®ç°å¤æ‚
  âŒ éœ€è¦åŠ¨æ€ç®¡ç† attention mask
  âŒ CPU å¼€é”€è¾ƒé«˜
```

---

## 5.7 vLLM æ¶æ„å…¨æ™¯ â­â­â­

> **ğŸ’¡ æ¥æº**: [Berkeley EECS-2025-192 - Deconstructing vLLM](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-192.pdf)
>
> **æ ¸å¿ƒä»·å€¼**: ç³»ç»Ÿæ€§ç†è§£ vLLM çš„ä¸‰å±‚æ¶æ„â€”â€”Interfaceã€Model Authoringã€Runtime,ä¸ºåç»­ç« èŠ‚é“ºå«æ¶æ„çŸ¥è¯†ã€‚
>
> **ä¸ºä»€ä¹ˆé‡è¦**:
> - ä»"ä¼šç”¨ vLLM"åˆ°"ç†è§£ vLLM"çš„å…³é”®è½¬å˜
> - è°ƒè¯•é—®é¢˜ã€æ€§èƒ½ä¼˜åŒ–ã€æ‰©å±•å¼€å‘çš„åŸºç¡€
> - ä¸ºç¬¬6ç«  (KV Cache)ã€ç¬¬7ç«  (è°ƒåº¦)ã€ç¬¬10ç«  (éƒ¨ç½²) é“ºå«

### 5.7.1 vLLM çš„ä¸‰å±‚æ¶æ„

**Layer 1: Interfaces** (ç”¨æˆ·äº¤äº’å±‚)

```
User Request â†’ OpenAI Server â†’ API Server â†’ LLMEngine
```

- **LLMEngine**: æ ¸å¿ƒå¼•æ“
  - ä½œç”¨: åè°ƒæ‰€æœ‰ç»„ä»¶
  - èŒè´£: è¯·æ±‚ç®¡ç†ã€èµ„æºåˆ†é…ã€ç»“æœè¿”å›
  - æ¥å£: `generate()`, `encode()`

- **API Server**: HTTP æœåŠ¡
  - ä½œç”¨: æä¾› REST API
  - èŒè´£: è¯·æ±‚è·¯ç”±ã€è®¤è¯ã€é™æµ
  - åè®®: HTTP/REST

- **OpenAI-Compatible Server**: æ ‡å‡†æ¥å£
  - ä½œç”¨: å…¼å®¹ OpenAI API
  - èŒè´£: `/v1/chat/completions` ç­‰æ¥å£
  - ä»·å€¼: é›¶ä»£ç è¿ç§»

---

**Layer 2: Model Authoring** (æ¨¡å‹æŠ½è±¡å±‚)

```
LLMEngine â†’ ModelExecutor â†’ BlockManager + Scheduler
```

- **ModelExecutor**: æ¨¡å‹æ‰§è¡Œå™¨
  - ä½œç”¨: æ‰§è¡Œæ¨¡å‹ forward pass
  - æŠ½è±¡: æ”¯æŒä¸åŒæ¨¡å‹æ¶æ„
  - æ¥å£: `execute_model()`, `profile()`
  - è¯¦è§: ç¬¬10ç«  Model Authoring

- **BlockManager**: å†…å­˜å—ç®¡ç†
  - ä½œç”¨: ç®¡ç† KV Cache çš„ physical blocks
  - èŒè´£: åˆ†é…ã€é‡Šæ”¾ã€è¿ç§» blocks
  - æŠ½è±¡: Physical vs Logical blocks
  - è¯¦è§: ç¬¬6ç«  PagedAttention åŸç†

- **Scheduler**: è¯·æ±‚è°ƒåº¦å™¨
  - ä½œç”¨: å†³å®šå“ªäº›è¯·æ±‚å¯ä»¥æ‰§è¡Œ
  - ç­–ç•¥: FIFOã€Priorityã€SJF
  - è¾“å‡º: Scheduled requests
  - è¯¦è§: ç¬¬7ç«  vLLM çš„è°ƒåº¦å™¨å®ç°

---

**Layer 3: Runtime** (è¿è¡Œæ—¶å±‚)

```
Scheduler â†’ CacheEngine â†’ Worker (GPU)
```

- **CacheEngine**: KV ç¼“å­˜å¼•æ“
  - ä½œç”¨: ç®¡ç† KV Cache çš„ç‰©ç†å­˜å‚¨
  - æ•°æ®ç»“æ„: Block table
  - åŠŸèƒ½: Hash-based lookup
  - è¯¦è§: ç¬¬6ç«  å†…å­˜ç®¡ç†æ·±åº¦å‰–æ

- **Worker**: å·¥ä½œè¿›ç¨‹
  - ä½œç”¨: åœ¨ GPU ä¸Šæ‰§è¡Œè®¡ç®—
  - èŒè´£: æ¨¡å‹æ¨ç†ã€kernel æ‰§è¡Œ
  - é€šä¿¡: ä¸ä¸»è¿›ç¨‹é€šä¿¡

---

### 5.7.2 ç”¨æˆ·è¯·æ±‚çš„å®Œæ•´æµç¨‹

**æ­¥éª¤ 1: ç”¨æˆ·å‘é€è¯·æ±‚**

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

**æ­¥éª¤ 2: OpenAI Server æ¥æ”¶**
- è§£æè¯·æ±‚
- éªŒè¯å‚æ•°
- è½¬å‘ç»™ API Server

**æ­¥éª¤ 3: API Server å¤„ç†**
- è¯·æ±‚è·¯ç”±
- é™æµæ£€æŸ¥
- è°ƒç”¨ LLMEngine.generate()

**æ­¥éª¤ 4: LLMEngine è°ƒåº¦**
- åˆ›å»ºè¯·æ±‚å¯¹è±¡
- æäº¤ç»™ Scheduler
- ç­‰å¾…è°ƒåº¦ç»“æœ

**æ­¥éª¤ 5: Scheduler å†³ç­–**
- æ£€æŸ¥èµ„æº (GPU memoryã€compute)
- é€‰æ‹©å¯æ‰§è¡Œçš„è¯·æ±‚
- è¿”å› scheduled requests

**æ­¥éª¤ 6: ModelExecutor æ‰§è¡Œ**
- å‡†å¤‡ input data
- è°ƒç”¨ Worker.execute_model()
- ç­‰å¾… GPU è¿”å›ç»“æœ

**æ­¥éª¤ 7: Worker åœ¨ GPU ä¸Šæ‰§è¡Œ**
- åŠ è½½æ¨¡å‹ weights
- æ‰§è¡Œ PagedAttention kernels
- è¿”å› generated tokens

**æ­¥éª¤ 8: ç»“æœè¿”å›**
```
Worker â†’ ModelExecutor â†’ LLMEngine
  â†“
LLMEngine â†’ API Server â†’ OpenAI Server
  â†“
OpenAI Server â†’ ç”¨æˆ·
```

---

### 5.7.3 æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Layer 1: Interfaces               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  OpenAI Server  â†’  API Server  â†’  LLMEngine    â”‚
â”‚  (HTTP)            (REST)         (Core)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Layer 2: Model Authoring             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ModelExecutor  â†  Scheduler  â†  BlockManager   â”‚
â”‚  (Execution)      (Policy)       (Memory)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Layer 3: Runtime                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CacheEngine  â†’  Worker  â†’  GPU Kernels         â”‚
â”‚  (KV Cache)      (Compute)    (CUDA)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.7.4 ä¸åç»­ç« èŠ‚çš„å…³è”

**ç¬¬6ç«  KV Cache ä¼˜åŒ–**:
- BlockManager çš„è¯¦ç»†å®ç° (6.3.2)
- CacheEngine çš„å†…å­˜ç®¡ç† (6.3.3)
- PagedAttention çš„æ ¸å¿ƒåˆ›æ–° (6.3.2)

**ç¬¬7ç«  è¯·æ±‚è°ƒåº¦ç­–ç•¥**:
- Scheduler çš„è°ƒåº¦ç®—æ³• (7.4)
- Iteration-level scheduling (7.4.2)
- CPU overheads åˆ†æ (7.4.3)

**ç¬¬10ç«  ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²**:
- Interface å±‚éƒ¨ç½²æ¨¡å¼ (10.2-10.4)
- Model Authoring å®æˆ˜ (10.6)
- æ€§èƒ½åˆ†æä¸è°ƒä¼˜ (10.5)

---

### 5.7.5 å®æˆ˜: å¯åŠ¨ vLLM å¹¶è§‚å¯Ÿæ¶æ„

**å¯åŠ¨ vLLM server**:

```bash
vllm serve meta-llama/Llama-2-7b-hf \
  --port 8000 \
  --host 0.0.0.0
```

**æŸ¥çœ‹å¯åŠ¨è¿‡ç¨‹**:

```
INFO:     Started server process
INFO:     Waiting for vLLM engine to initialize
INFO:     Initializing an LLM engine with config
INFO:     Loading model weights
INFO:     GPU memory: 15.50 GB
INFO:     Model loaded
```

**å‘é€è¯·æ±‚**:

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-hf",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

---

### 5.7.6 æ¶æ„ç†è§£æ£€æŸ¥ç‚¹

å®Œæˆæœ¬ç« å,ä½ åº”è¯¥èƒ½å¤Ÿ:

- [ ] è§£é‡Š vLLM çš„ä¸‰å±‚æ¶æ„
- [ ] æè¿°ç”¨æˆ·è¯·æ±‚çš„å®Œæ•´æµç¨‹ (8æ­¥éª¤)
- [ ] ç†è§£ LLMEngineã€ModelExecutorã€Worker çš„èŒè´£
- [ ] çŸ¥é“ BlockManager å’Œ Scheduler çš„ä½œç”¨
- [ ] ç†è§£ PagedAttention åœ¨æ¶æ„ä¸­çš„ä½ç½®

---

## âœ… ç« èŠ‚æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬ç« å,ä½ åº”è¯¥èƒ½å¤Ÿ:

- [ ] è§£é‡Š LLM ç”Ÿæˆæ–‡æœ¬çš„ä¸¤ä¸ªé˜¶æ®µ (Prefill å’Œ Decode)
- [ ] ç†è§£ Attention æœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹å’Œå¤æ‚åº¦
- [ ] è¯´æ˜ KV Cache å¦‚ä½•å°†å¤æ‚åº¦ä» O(nÂ²) é™åˆ° O(n)
- [ ] è®¡ç®— KV Cache çš„æ˜¾å­˜å ç”¨
- [ ] è§£é‡Š Chunked Prefill çš„åŸç†å’Œåº”ç”¨åœºæ™¯
- [ ] å¯¹æ¯” Static Batchingã€Dynamic Batching å’Œ Continuous Batching
- [ ] æè¿° vLLM çš„ä¸‰å±‚æ¶æ„
- [ ] è¿½è¸ªç”¨æˆ·è¯·æ±‚åœ¨ vLLM ä¸­çš„å®Œæ•´æµç¨‹

---

## ğŸ“š åŠ¨æ‰‹ç»ƒä¹ 

**ç»ƒä¹  5.1**: è®¡ç®— KV Cache æ˜¾å­˜å ç”¨

Llama-2-7B çš„é…ç½®:
- å±‚æ•°: 32
- Attention heads: 32
- Head dimension: 128
- æ•°æ®ç±»å‹: FP16 (2 bytes)

é—®é¢˜:
1. å•ä¸ª token çš„ KV cache å¤§å°æ˜¯å¤šå°‘?
2. 1000 tokens çš„ KV cache éœ€è¦å¤šå°‘æ˜¾å­˜?
3. å¦‚æœæœ‰ 10 ä¸ªå¹¶å‘è¯·æ±‚,æ¯ä¸ªè¯·æ±‚å¹³å‡ 500 tokens,æ€»å…±éœ€è¦å¤šå°‘æ˜¾å­˜?

**ç»ƒä¹  5.2**: å¯¹æ¯”ä¸åŒ Batching æ–¹æ³•çš„æ•ˆç‡

å‡è®¾æœ‰ä»¥ä¸‹ 3 ä¸ªè¯·æ±‚:
- Request A: "Hi" (2 tokens)
- Request B: "Hello, how are you?" (5 tokens)
- Request C: "The quick brown fox jumps over the lazy dog" (10 tokens)

ä»»åŠ¡:
1. è®¡ç®— Static Batching çš„ padding æ•°é‡
2. ç”»å‡º Continuous Batching çš„ attention mask
3. è®¡ç®— GPU åˆ©ç”¨ç‡ (æœ‰æ•ˆ tokens / æ€» tokens)

**ç»ƒä¹  5.3**: è¿½è¸ª vLLM è¯·æ±‚æµç¨‹

å¯åŠ¨ vLLM server,å‘é€ä¸€ä¸ªè¯·æ±‚,å¹¶è§‚å¯Ÿæ—¥å¿—:
1. è¯†åˆ«æ¯ä¸ªå±‚çº§ (Interfaceã€Model Authoringã€Runtime) çš„æ—¥å¿—
2. è®°å½•è¯·æ±‚ä»è¿›å…¥åˆ°è¿”å›çš„æ—¶é—´
3. æ‰¾å‡º Schedulerã€ModelExecutorã€Worker çš„æ—¥å¿—

---

## ğŸ¯ æ€»ç»“

**å…³é”®è¦ç‚¹**:
- LLM æ¨ç†åˆ†ä¸º Prefill (è®¡ç®—å¯†é›†) å’Œ Decode (å¸¦å®½å¯†é›†) ä¸¤ä¸ªé˜¶æ®µ
- Attention æ˜¯å”¯ä¸€è®© token äº¤äº’çš„æ“ä½œ,å¤æ‚åº¦ä¸º O(nÂ²)
- KV Cache é€šè¿‡ç¼“å­˜å†å² token çš„ Kã€V,å°†å¤æ‚åº¦é™åˆ° O(n)
- Chunked Prefill å…è®¸å¤„ç†è¶…é•¿ prompt,é¿å…æ˜¾å­˜æº¢å‡º
- Continuous Batching é€šè¿‡å»é™¤ padding å’ŒåŠ¨æ€è°ƒåº¦,å¤§å¹…æå‡ GPU åˆ©ç”¨ç‡
- vLLM çš„ä¸‰å±‚æ¶æ„ (Interfaceã€Model Authoringã€Runtime) æä¾›äº†æ¸…æ™°çš„æŠ½è±¡

**ä¸‹ä¸€ç« **: ç¬¬6ç«  KV Cache ä¼˜åŒ–â€”â€”æ·±å…¥ç†è§£ PagedAttention å’Œå†…å­˜ç®¡ç†ã€‚

---

**æœ‰é—®é¢˜?åŠ å…¥ [ç¬¬5ç«  Discord é¢‘é“](https://discord.gg/TODO) è®¨è®º!**
