# ç¬¬11ç« : é«˜çº§è¯é¢˜

> "å”¯ä¸€ä¸å˜çš„æ˜¯å˜åŒ–æœ¬èº«,è€ŒæŠ€æœ¯æ ˆçš„æ·±åº¦è®©å˜åŒ–åŠ é€Ÿã€‚" - ä½šå

## ç®€ä»‹

å‰10ç« æˆ‘ä»¬è¦†ç›–äº†ä»GPUåŸºç¡€åˆ°ç”Ÿäº§éƒ¨ç½²çš„å®Œæ•´çŸ¥è¯†ä½“ç³»ã€‚æœ¬ç« å°†æ¢è®¨ä¸€äº›é«˜çº§è¯é¢˜å’Œæœªæ¥è¶‹åŠ¿,è¿™äº›å†…å®¹ä»£è¡¨äº†LLMæ¨ç†ä¼˜åŒ–çš„å‰æ²¿æ–¹å‘ã€‚

**ğŸ’° æˆæœ¬å½±å“**(åŸºäºè¡Œä¸šæ•°æ®)
- **MoEæ¨¡å‹**: ç¨€ç–æ¿€æ´»å¯é™ä½30-50%æ¨ç†æˆæœ¬
- **å¤šæ¨¡æ€**: å›¾åƒ+æ–‡æœ¬æ¨ç†,æ–°çš„æˆæœ¬ä¼˜åŒ–ç»´åº¦
- **è¾¹ç¼˜éƒ¨ç½²**: å°†æ¨ç†ç§»åˆ°è¾¹ç¼˜,é™ä½ä¸­å¿ƒæˆæœ¬å’Œå»¶è¿Ÿ
- **å¼‚æ„éƒ¨ç½²**: è®­ç»ƒç”¨H100,æ¨ç†ç”¨H200,å……åˆ†åˆ©ç”¨ç¡¬ä»¶

åœ¨æœ¬ç« ä¸­,ä½ å°†å­¦ä¹ :
- AgentåŸºç¡€è®¾æ–½çš„æŒ‘æˆ˜ä¸æœºé‡
- å¼‚æ„ç¡¬ä»¶éƒ¨ç½²çš„æœ€ä½³å®è·µ
- MoEæ¨¡å‹çš„æ¨ç†ä¼˜åŒ–
- å¤šæ¨¡æ€æ¨¡å‹æ¨ç†
- Flash Attentionç­‰åº•å±‚ä¼˜åŒ–æŠ€æœ¯
- æŠ€æœ¯å‘å±•çš„æœªæ¥è¶‹åŠ¿

---

## 11.1 AgentåŸºç¡€è®¾æ–½ âš ï¸ å¼€æºç”Ÿæ€ç¼ºå¤±

> **ğŸ’¡ 2025å¹´æŠ€æœ¯è¶‹åŠ¿**(æ¥æº:2025"é’ç¨"AIå˜‰å¹´å - å¼ æ˜æ˜Ÿ@æ¸…åã€æœ±ç«‹è€•@NVIDIA)
>
> **æ ¸å¿ƒæ´å¯Ÿ**: 2025å¹´ä¸‹åŠå¹´Agentå¿«é€Ÿå…´èµ·(Google NotebookLMã€Gemini Nano),ä½†å¼€æºAgent SystemåŸºæœ¬æ˜¯è´Ÿåˆ†ã€‚è¿™æ˜¯å½“å‰æœ€å¤§çš„æœºä¼šä¹‹ä¸€ã€‚

### 11.1.1 ä¸ºä»€ä¹ˆAgent Infraå¾ˆé‡è¦

**2025å¹´çš„çˆ†å‘**:

```
å•†ä¸šäº§å“:
  - Google: NotebookLMã€Gemini Flashã€Gemini Nano
  - å›½å†…: AutoJamã€å¤šå®ä¹¦è®°

å±•ç¤ºä»·å€¼:
  - Geminiå®Œå…¨å¯åšç§‘ç ”åŠ©æ‰‹
  - å¯ä»¥å°‘é›‡ä¸€äº›inference
```

**æ ¸å¿ƒä»·å€¼**(å¼ åšæ¶µ@æµ™å¤§):
- Geminiå®Œå…¨å¯åšç§‘ç ”åŠ©æ‰‹
- å¯ä»¥å°‘é›‡ä¸€äº›inference

**ç‹¬ç‰¹æŒ‘æˆ˜**:
- ä¸åƒä¼ ç»Ÿæ¨ç†åªæœ‰text input/output
- éœ€è¦å¤æ‚çš„ç¯å¢ƒäº¤äº’

### 11.1.2 Agent Systemçš„ç¼ºå¤±

**å½“å‰çŠ¶æ€**(æœ±ç«‹è€•@NVIDIA):
```
å¼€æºagent systemæ˜¯è´Ÿæ•°

ç°çŠ¶:
  - åœ¨å…¬å¸å†…éƒ¨æ­å»ºJupyter agentéƒ½å¾ˆéš¾
  - éœ€è¦manage K8Sã€è‡ªåŠ¨èµ·virtual environment
  - åªèƒ½ç”¨dirtyæ–¹æ³•(mock pythonè¿›ç¨‹)
  - æ— æ³•å¾ˆå¥½åœ°åšagent
  - å­¦æœ¯ç•Œå‡ ä¹æ²¡æœ‰ä½¿ç”¨ç»éªŒ
```

**éœ€æ±‚**:
- Scalable and easy to useçš„sandbox system
- åƒinference engineä¸€æ ·ç»™ä¸ªURL
- å‘HTTP requestå°±èƒ½å®Œæˆæ‰€æœ‰äº‹æƒ…

### 11.1.3 Agentç¯å¢ƒçš„å¤æ‚æ€§

**æ–‡ä»¶ç³»ç»Ÿ**:
```python
# Agentéœ€è¦æ“ä½œæ–‡ä»¶ç³»ç»Ÿ
agent.filesystem.write("/tmp/data.txt", content)
data = agent.filesystem.read("/tmp/data.txt")

# å¯èƒ½æŒ‚è½½å¤±è´¥éœ€è¦å¤„ç†
try:
    files = agent.list_files("/mnt/shared")
except MountError:
    # å¤„ç†æ–‡ä»¶ç³»ç»ŸæŒ‚è½½å¤±è´¥
    pass
```

**ç½‘ç»œ**:
```python
# HTTPè¯·æ±‚ã€APIè°ƒç”¨
response = agent.http_fetch("https://api.example.com/data")

# è¶…æ—¶ã€é‡è¯•ã€é”™è¯¯å¤„ç†
response = agent.http_fetch(
    url,
    timeout=10,
    retries=3,
    on_error="retry_with_backoff"
)
```

**è™šæ‹Ÿæœº**:
- å¯èƒ½éœ€è¦åµŒå¥—VM
- å¤æ‚çš„workflowæ„é€ 

**CPUçš„é‡è¦æ€§**(å¼ æ˜æ˜Ÿ@æ¸…å):
```
é—®é¢˜:
  - å¤§å®¶å¯¹CPUçš„å…³æ³¨ä¸å¤Ÿ
  - Agentç¯å¢ƒéœ€è¦å¤§é‡CPU
  - å¼€æºç”Ÿæ€CPUæ”¯æŒæ˜¯è´Ÿåˆ†

åŸå› :
  - Agentéœ€è¦è¿è¡Œå·¥å…·ã€è§£ææ–‡ä»¶
  - è¿™äº›éƒ½æ˜¯CPUå¯†é›†å‹ä»»åŠ¡
  - GPUæ¨ç†åªæ˜¯å…¶ä¸­ä¸€éƒ¨åˆ†
```

### 11.1.4 Agentç¯å¢ƒçš„ç±»å‹

**ç®€å•ç¯å¢ƒ**:
- Dockerå®¹å™¨
- åŸºæœ¬çš„æ–‡ä»¶ç³»ç»Ÿæ“ä½œ

**ä¸­ç­‰å¤æ‚**:
- K8Sä¸Šçš„è™šæ‹Ÿç¯å¢ƒ
- ç½‘ç»œè°ƒç”¨

**é«˜å¤æ‚**:
- åµŒå¥—VM
- å¤æ‚workflow
- å¤šä¸ªæœåŠ¡ååŒ

### 11.1.5 Agentéƒ¨ç½²æ¶æ„

**å•æœºéƒ¨ç½²**:
```python
# é€‚åˆå¼€å‘å’Œå®éªŒ
# å•æœºè¿è¡ŒAgent + Inference
docker-compose up agent inference
```

**K8Séƒ¨ç½²**:
```yaml
# éœ€è¦Operatorç®¡ç†
apiVersion: agent.example.com/v1
kind: AgentEnvironment
metadata:
  name: agent-env-1
spec:
  image: agent-runtime:latest
  resources:
    cpu: "4"
    memory: "16Gi"
    gpu: "1"
  autoScaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
```

**äº‘åŸç”Ÿéƒ¨ç½²**:
- ä½¿ç”¨AWS Lambdaã€GCP Cloud Functions
- Serverlessæ¶æ„

### 11.1.6 å®æˆ˜æ¡ˆä¾‹

**æ¡ˆä¾‹1: æ­å»ºç®€å•çš„Jupyter Agent**

```python
class JupyterAgent:
    """åœ¨Jupyterç¯å¢ƒä¸­è¿è¡Œçš„Agent"""

    def __init__(self):
        self.kernel = JupyterKernel()
        self.filesystem = LocalFileSystem()

    def execute(self, code: str) -> str:
        """æ‰§è¡ŒPythonä»£ç """
        try:
            result = self.kernel.execute(code)
            return result.stdout
        except Exception as e:
            return f"Error: {e}"

    def read_file(self, path: str) -> str:
        """è¯»å–æ–‡ä»¶"""
        return self.filesystem.read(path)

    def write_file(self, path: str, content: str):
        """å†™å…¥æ–‡ä»¶"""
        self.filesystem.write(path, content)

# ä½¿ç”¨
agent = JupyterAgent()
result = agent.execute("print('Hello, World!')")
```

**æ¡ˆä¾‹2: ä½¿ç”¨Dockeréƒ¨ç½²Agentç¯å¢ƒ**

```dockerfile
# Dockerfile
FROM python:3.11-slim

# å®‰è£…ä¾èµ–
RUN pip install jupyter \
               openai \
               langchain \
               requests

# åˆ›å»ºå·¥ä½œç›®å½•
WORKDIR /agent

# å¤åˆ¶Agentä»£ç 
COPY agent.py /agent/
COPY tools/ /agent/tools/

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/agent

# å¯åŠ¨AgentæœåŠ¡
CMD ["python", "-m", "agent.server"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  agent:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./workspace:/agent/workspace
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
```

**æ¡ˆä¾‹3: ç”Ÿäº§çº§Agent Systemçš„æŒ‘æˆ˜**

```
æŒ‘æˆ˜1: çŠ¶æ€ç®¡ç†
  - Agentæœ‰çŠ¶æ€(å¯¹è¯å†å²ã€æ–‡ä»¶ç³»ç»Ÿ)
  - è·¨å®ä¾‹åŒæ­¥å›°éš¾
  - è§£å†³: RedisçŠ¶æ€å­˜å‚¨ + Sessionè·¯ç”±

æŒ‘æˆ˜2: èµ„æºéš”ç¦»
  - å¤šä¸ªAgentå…±äº«èµ„æº
  - å¦‚ä½•é˜²æ­¢äº’ç›¸å¹²æ‰°?
  - è§£å†³: K8S ResourceQuota + LimitRange

æŒ‘æˆ˜3: é”™è¯¯æ¢å¤
  - Agentå´©æºƒå¦‚ä½•æ¢å¤?
  - ä¸­é—´çŠ¶æ€å¦‚ä½•ä¿å­˜?
  - è§£å†³: Checkpoint + äº‹ä»¶æº¯æº
```

### 11.1.7 Context Engineeringæœ€ä½³å®è·µ

> **æ¥æº**: [Manus - Context Engineering for AI Agents](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus)
>
> **æ ¸å¿ƒè§‚ç‚¹**: Context Engineeringæ˜¯Agentç³»ç»Ÿçš„"Stochastic Gradient Descent"â€”â€”é€šè¿‡å®éªŒå’Œè¿­ä»£æ‰¾åˆ°å±€éƒ¨æœ€ä¼˜è§£ã€‚Manuså›¢é˜Ÿé‡å»ºäº†4æ¬¡Agentæ¡†æ¶æ‰æ€»ç»“å‡ºè¿™äº›æ¨¡å¼ã€‚

#### å…­å¤§æ ¸å¿ƒåŸåˆ™

**åŸåˆ™1: Design Around the KV-Cache** â­â­â­

**æ ¸å¿ƒæ´å¯Ÿ**:
- KV-cache hit rateæ˜¯ç”Ÿäº§çº§agentæœ€é‡è¦çš„å•ä¸€æŒ‡æ ‡
- ç›´æ¥å½±å“latency(TTFT)å’Œcost
- Agentçš„è¾“å…¥è¾“å‡ºæ¯”ä¾‹100:1(vs chatbot 1:1)

**ä¸‰å¤§å®è·µ**:

1. **ç¨³å®šçš„Prompt Prefix**
   ```python
   # âŒ Bad: æ¯æ¬¡è¯·æ±‚éƒ½ä¸åŒ
   system_prompt = f"""
   You are an AI assistant.
   Current time: {datetime.now()}
   User ID: {user_id}
   Session ID: {session_id}
   """

   # âœ… Good: ç¨³å®šçš„å‰ç¼€
   system_prompt = """
   You are an AI assistant.
   Current time: {{current_time}}
   User ID: {{user_id}}
   """
   # åŠ¨æ€å†…å®¹é€šè¿‡æ¨¡æ¿å˜é‡æ³¨å…¥
   ```

2. **Append-only Context**
   ```python
   # âŒ Bad: ä¿®æ”¹å†å²
   context[5]["content"] = updated_content  # ç ´åcache!

   # âœ… Good: è¿½åŠ æ–°å†…å®¹
   context.append({
       "role": "system",
       "content": f"Correction: {updated_content}"
   })

   # âœ… Good: ç¡®å®šæ€§åºåˆ—åŒ–
   import json
   tools_str = json.dumps(tools, sort_keys=True)  # ä¿æŒé¡ºåº
   ```

3. **Cache Breakpointsç­–ç•¥**
   ```python
   # æ˜¾å¼æ ‡è®°å¯å¤ç”¨çš„æ–­ç‚¹
   cache_breakpoints = {
       "init": lambda: system_prompt + tools_str,
       "user_input": lambda ctx: ctx + user_input,
   }

   # vLLM prefix caching + session IDè·¯ç”±
   def route_request(session_id: str) -> str:
       worker_id = hash(session_id) % num_workers
       return f"worker-{worker_id}"
   ```

**åŸåˆ™2: Mask, Don't Remove** â­â­â­

**é—®é¢˜**: å·¥å…·æ•°é‡çˆ†ç‚¸
- MCPåè®®è®©ç”¨æˆ·plugæ•°ç™¾ä¸ªå·¥å…·
- å·¥å…·è¿‡å¤šå¯¼è‡´æ¨¡å‹é€‰æ‹©é”™è¯¯action
- åŠ¨æ€æ·»åŠ /åˆ é™¤å·¥å…·ç ´åKV-cache

**Solution**: Context-aware State Machine

```python
# ä¿æŒå·¥å…·å®šä¹‰ç¨³å®š(ä¿æŠ¤KV-cache)
ALL_TOOLS = [
    "browser_search",
    "browser_open",
    "shell_execute",
    "file_read",
    "file_write",
    # ... æ›´å¤šå·¥å…·
]

# é€šè¿‡response prefillæ§åˆ¶action space
def get_prefill(agent_state: str) -> str:
    if agent_state == "browsing":
        return "<|im_start|>assistant\n<|tool|>{\"name\": \"browser_"
        # åªèƒ½é€‰æ‹©browser_å¼€å¤´çš„å·¥å…·

    elif agent_state == "file_operations":
        return "<|im_start|>assistant\n<|tool|>{\"name\": \"file_"
        # åªèƒ½é€‰æ‹©file_å¼€å¤´çš„å·¥å…·

    return "<|im_start|>assistant\n"
    # å¯ä»¥é€‰æ‹©ä»»ä½•å·¥å…·
```

**ä¸‰ç§Function Callingæ¨¡å¼**:
```python
# Mode 1: Auto - æ¨¡å‹è‡ªä¸»é€‰æ‹©
prefix = "<|im_start|>assistant\n"

# Mode 2: Required - å¿…é¡»è°ƒç”¨å·¥å…·
prefix = "<|im_start|>assistant\n<|tool|>"

# Mode 3: Specified - å¿…é¡»è°ƒç”¨ç‰¹å®šå·¥å…·ç»„
prefix = "<|im_start|>assistant\n<|tool|>{\"name\": \"browser_"
# åªèƒ½é€‰æ‹©browser_å¼€å¤´çš„å·¥å…·
```

**åŸåˆ™3: File System as Ultimate Context** â­â­

**é•¿contextçš„ä¸‰å¤§ç—›ç‚¹**:
1. **Observationså·¨å¤§**: ç½‘é¡µã€PDFå¯èƒ½æ•°ä¸‡tokens
2. **æ€§èƒ½ä¸‹é™**: è¶…è¿‡ä¸€å®šé•¿åº¦åæ¨¡å‹æ€§èƒ½degrade
3. **æˆæœ¬é«˜æ˜‚**: å³ä½¿æœ‰cache,é•¿contextä»è´µ

**Solution**: æ–‡ä»¶ç³»ç»Ÿä½œä¸ºå¤–éƒ¨memory

```python
# ç½‘é¡µå†…å®¹ â†’ ä¿å­˜åˆ°æ–‡ä»¶
web_content = fetch_page(url)
file_path = agent.filesystem.write(web_content)

# Contextåªä¿ç•™å¼•ç”¨
context.append({
    "type": "web_page",
    "url": url,
    "file_path": file_path,  # éœ€è¦æ—¶å¯è¯»å–
    "summary": summarize(web_content)  # 100 tokens
})

# å‹ç¼©åŸåˆ™:
# - ç½‘é¡µ: ä¿ç•™URL
# - PDF: ä¿ç•™æ–‡ä»¶è·¯å¾„
# - æ•°æ®åº“: ä¿ç•™æŸ¥è¯¢è¯­å¥
# - å…³é”®: å¯æ¢å¤æ€§(information not lost, just externalized)
```

**åŸåˆ™4: Manipulate Attention Through Recitation** â­â­

**é—®é¢˜**:
- å…¸å‹Agentä»»åŠ¡: ~50æ­¥tool calls
- Contextå¿«é€Ÿå¢é•¿åˆ°æ•°ä¸‡tokens
- æ¨¡å‹å®¹æ˜“"lost-in-the-middle"æˆ–åç§»ç›®æ ‡

**Solution**: todo.mdæœºåˆ¶

```python
# Agentè‡ªåŠ¨åˆ›å»ºå’Œæ›´æ–°todo.md
todo_content = """
# Task: Research and book flight to Tokyo

- [ ] Search flights to Tokyo (Mar 1-7, 2025)
- [ ] Compare prices across airlines
- [ ] Check hotel availability
- [x] Get user preferences (budget, dates)
- [ ] Book best option
- [ ] Send confirmation

Current step: Comparing prices...
"""

# åŸç†:
# - å°†å…¨å±€planå¤è¿°åˆ°contextæœ«å°¾
# - æ¨å…¥æ¨¡å‹çš„recent attention span
# - é¿å…"lost-in-the-middle"
# - ç”¨è‡ªç„¶è¯­è¨€biasä»»åŠ¡ç›®æ ‡
```

**åŸåˆ™5: Keep the Wrong Stuff In** â­â­

**å¸¸è§é”™è¯¯**:
- Agentå‡ºé”™ â†’ æ¸…ç†trace â†’ é‡è¯•
- ä½¿ç”¨temperature"é‡å¯"
- éšè—é”™è¯¯è®©context"å¹²å‡€"

**ä¸ºä»€ä¹ˆé”™è¯¯**:
- ç§»é™¤å¤±è´¥ = ç§»é™¤è¯æ®
- æ¨¡å‹æ— æ³•ä»é”™è¯¯ä¸­å­¦ä¹ 
- æ— æ³•æ›´æ–°å†…éƒ¨beliefs
- å®¹æ˜“é‡å¤åŒæ ·é”™è¯¯

**æ­£ç¡®åšæ³•**:
```python
# ä¿ç•™å®Œæ•´trace(åŒ…æ‹¬é”™è¯¯)
context = [
    {"role": "user", "content": "Extract data from PDF"},
    {"role": "assistant", "tool_call": {
        "name": "pdf_parse",
        "args": {"file": "wrong.pdf"}  # é”™è¯¯!
    }},
    {"role": "tool", "content": "Error: File not found"},
    {"role": "assistant", "tool_call": {
        "name": "pdf_parse",
        "args": {"file": "correct.pdf"}  # ä¿®æ­£
    }},
    # æ¨¡å‹çœ‹åˆ°é”™è¯¯ â†’ å­¦ä¹ é¿å‘
]
```

**å…³é”®æ´å¯Ÿ**:
- **é”™è¯¯æ¢å¤æ˜¯true agentic behaviorçš„æ ‡å¿—**
- å­¦æœ¯ç•Œå¿½è§†çš„æŒ‡æ ‡
- äººç±»ä»é”™è¯¯ä¸­å­¦ä¹ ,Agentä¹Ÿåº”å¦‚æ­¤

**åŸåˆ™6: Don't Get Few-Shotted** â­

**é—®é¢˜**:
- LLMæ˜¯ä¼˜ç§€çš„mimic
- Few-shotåœ¨Agentä¸­å¯èƒ½é€‚å¾—å…¶å
- Contextå……æ»¡ç›¸ä¼¼action-observation pairs
- æ¨¡å‹é™·å…¥æ¨¡å¼,å¤±å»çµæ´»æ€§

**æ¡ˆä¾‹**:
- æ‰¹é‡å¤„ç†20ä»½ç®€å†
- Agenté™·å…¥èŠ‚å¥: é‡å¤ç›¸ä¼¼åŠ¨ä½œ
- ç»“æœ: driftã€overgeneralizationã€hallucination

**Solution**: å¢åŠ å¤šæ ·æ€§

```python
# å¼•å…¥å¾®å°å˜åŒ–
templates = [
    "Action: {tool}",
    "Execute: {tool}",
    "Calling {tool}...",
    "{tool}()",
]
# éšæœºä½¿ç”¨ä¸åŒæ¨¡æ¿

# å…³é”®:
# - é¿å…uniform context
# - å¢åŠ ç»“æ„åŒ–å¤šæ ·æ€§
# - è®©æ¨¡å‹ä¿æŒæ³¨æ„åŠ›
```

#### å¼€æºç”Ÿæ€çš„æœºä¼š

**å½“å‰ç¼ºå¤±**:
- âŒ æ²¡æœ‰æ ‡å‡†åŒ–çš„context management
- âŒ æ¯ä¸ªagentéƒ½è¦re-inventè¿™äº›æ¨¡å¼
- âŒ ç¼ºä¹best practicesæ–‡æ¡£
- âŒ æ²¡æœ‰agent-orientedçš„profilingå·¥å…·

**å¯ä»¥åšçš„äº‹æƒ…**:

1. **å¼€æºContext Management Library**
   ```python
   class AgentContext:
       def __init__(self):
           self.kv_cache_aware = True
           self.append_only = True
           self.deterministic_serialization = True

       def add_observation(self, obs, compressible=False):
           if compressible:
               return self.externalize(obs)  # æ–‡ä»¶ç³»ç»Ÿ
           return self.append(obs)  # Context

       def mask_tools(self, allowed_prefixes):
           return self.logit_mask(allowed_prefixes)
   ```

2. **æ ‡å‡†åŒ–Metrics**
   - KV-cache hit rate
   - Context length distribution
   - Tool call success rate
   - **Error recovery rate**(å­¦æœ¯ç•Œå¿½è§†!)
   - Session stickiness

3. **Agent-oriented Profiling**
   - Context growth rate
   - Token cost breakdown(by step)
   - Tool call latency
   - File system usage
   - Cache effectiveness

---

## 11.2 å¼‚æ„ç¡¬ä»¶éƒ¨ç½² â­

> **ğŸ’¡ 2025å¹´æŠ€æœ¯è¶‹åŠ¿**(æ¥æº:2025"é’ç¨"AIå˜‰å¹´å - æœ±ç«‹è€•@NVIDIA)
>
> **æ ¸å¿ƒæ´å¯Ÿ**: Trainingå’ŒRolloutçš„ç®—åŠ›éœ€æ±‚å·®å¼‚2-3ä¸ªæ•°é‡çº§(Training: 10^5 flops/byte, Rollout: ~80 flops/byte)ã€‚RLå¤©ç”Ÿé€‚åˆç”¨ä¸åŒç¡¬ä»¶ã€‚

### 11.2.1 è®­ç»ƒvsæ¨ç†çš„ç®—åŠ›å·®å¼‚

**è®­ç»ƒ**(æœ±ç«‹è€•@NVIDIA):
- Flops per byte â‰ˆ 10^5
- è®¡ç®—å¯†é›†

**æ¨ç†**:
- Flops per byte â‰ˆ 80
- å¸¦å®½å¯†é›†

**å·®è·**: 2-3ä¸ªæ•°é‡çº§

**å¯ç¤º**: åº”è¯¥ç”¨ä¸åŒçš„ç¡¬ä»¶

```
è®­ç»ƒ: éœ€è¦é«˜è®¡ç®—èƒ½åŠ› â†’ H100
æ¨ç†: éœ€è¦é«˜å¸¦å®½ â†’ H200ã€L40s

ä¸è¦ç”¨H100åšæ¨ç†! æµªè´¹!
```

### 11.2.2 å¼‚æ„éƒ¨ç½²çš„æœºä¼š

**ä¹‹å‰çš„é—®é¢˜**:
- å¤§å®¶éƒ½åœ¨SPMDæ—¶ä¸ä¼šè€ƒè™‘
- ç‰©ç†ä¸Šåœ¨åŒä¸€é›†ç¾¤ä½†æƒé™ä¸åŒ

**ç°åœ¨çš„æœºä¼š**(æœ±ç«‹è€•@NVIDIA):
- H100è®­ç»ƒ + H200æ¨ç†
- å›½äº§å¡æ¨ç† + NVè®­ç»ƒ
- å¯ä»¥æŠŠè¿™äº›å¡æ›´å¥½åˆ©ç”¨èµ·æ¥

**ä¸ºä»€ä¹ˆç°åœ¨å¯ä»¥**:
- RLæŠŠtrainingå’Œrolloutåˆ†å¼€äº†
- æ¨ç†ä¹‹é—´æ²¡æœ‰å¼‚æ„é€šä¿¡
- å¯ä»¥ç‹¬ç«‹æ“ä½œ

### 11.2.3 ä¸åŒGPUçš„åº”ç”¨åœºæ™¯

**H100**:
- è®­ç»ƒä¼˜åŒ–
- é«˜è®¡ç®—èƒ½åŠ›
- TFLOPS: ~4000 (FP16)

**H200/L40s**:
- æ¨ç†ä¼˜åŒ–
- é«˜å¸¦å®½
- Memory Bandwidth: ~4.8 TB/s (H200)

**å›½äº§å¡**(æœ±ç«‹è€•@NVIDIA):
- æ¨ç†åœºæ™¯å¯é€‰æ‹©ç¡¬ä»¶å¤š
- è®­ç»ƒä»æ˜¯NVçš„privilege

### 11.2.4 å®¹ç¾å’Œæ··éƒ¨çš„æœºä¼š

**ä¹‹å‰çš„é—®é¢˜**:
- NCCL/MPIä¸å¤ªèƒ½å®¹ç¾
- ä¸€ä¸ªèŠ‚ç‚¹æŒ‚äº†å°±æ•´ä½“å¤¯æ­»
- å¤§å®¶å…¨æ€æ‰é‡å¯

**ç°åœ¨çš„æœºä¼š**(æœ±å­æ—@è´¨æœ´):
- æ¨ç†engineå¯ä»¥ç‹¬ç«‹æ“ä½œ
- æ¨ç†ä¹‹é—´æ²¡æœ‰å¼‚æ„é€šä¿¡
- å¯ä»¥åšå®¹ç¾ã€æ··éƒ¨ã€æ‰©ç¼©å®¹

**åº”ç”¨åœºæ™¯**:
```python
# æ½®æ±é˜Ÿåˆ—: ç™½å¤©æ¨ç†,å¤œé—´RL
daytime:
  - ä¼˜å…ˆçº§: æ¨ç†
  - èµ„æºåˆ†é…: 80%æ¨ç†, 20%RL
  - ç”¨é€”: æœåŠ¡ç”¨æˆ·è¯·æ±‚

nighttime:
  - ä¼˜å…ˆçº§: RLè®­ç»ƒ
  - èµ„æºåˆ†é…: 20%æ¨ç†, 80%RL
  - ç”¨é€”: æ¨¡å‹è®­ç»ƒå’Œrollout

# SMPå’ŒRLçš„å¤§é›†ç¾¤æ··ç”¨
# æå‡æ•´ä½“ç¡¬ä»¶åˆ©ç”¨ç‡
```

### 11.2.5 å¼‚æ„éƒ¨ç½²çš„æŒ‘æˆ˜

**Checkpointç®¡ç†**:
- ä¸åŒç¡¬ä»¶é—´checkpointè½¬æ¢
- Tçº§åˆ«æ¨¡å‹checkpointå·¨å¤§(å¼ åšæ¶µ@æµ™å¤§)

**é€šä¿¡**:
- è·¨é›†ç¾¤çš„é€šä¿¡
- ç½‘ç»œå¸¦å®½ç“¶é¢ˆ

**ç›‘æ§**:
- ç»Ÿä¸€ç›‘æ§ä¸åŒç¡¬ä»¶
- èµ„æºè°ƒåº¦å¤æ‚

### 11.2.6 å®æˆ˜æ¡ˆä¾‹

**æ¡ˆä¾‹1: H100è®­ç»ƒ + H200æ¨ç†**

```yaml
# training-cluster.yaml
apiVersion: v1
kind: Node
metadata:
  name: h100-training-node
spec:
  hardwareType: H100
  purpose: training
  resources:
    nvidia.com/gpu: 8
    gpu.memory: "80Gi"
---
# inference-cluster.yaml
apiVersion: v1
kind: Node
metadata:
  name: h200-inference-node
spec:
  hardwareType: H200
  purpose: inference
  resources:
    nvidia.com/gpu: 8
    gpu.memory: "141Gi"
```

**æ¡ˆä¾‹2: è·¨é›†ç¾¤è®­ç»ƒå’Œæ¨ç†**

```python
class HeterogeneousCluster:
    """å¼‚æ„é›†ç¾¤ç®¡ç†"""

    def __init__(self):
        self.training_cluster = "h100-cluster"
        self.inference_cluster = "h200-cluster"

    def schedule_task(self, task_type: str):
        """è°ƒåº¦ä»»åŠ¡åˆ°åˆé€‚çš„é›†ç¾¤"""
        if task_type == "training":
            return self.training_cluster
        elif task_type == "inference":
            return self.inference_cluster
        else:
            raise ValueError(f"Unknown task type: {task_type}")

    def transfer_checkpoint(self, from_cluster: str, to_cluster: str):
        """è·¨é›†ç¾¤ä¼ è¾“checkpoint"""
        # ä½¿ç”¨é«˜é€Ÿç½‘ç»œ(å¦‚InfiniBand)
        # å¢é‡ä¼ è¾“
        # å‹ç¼©
        pass
```

---

## 11.3 MoEæ¨¡å‹æ¨ç†ä¼˜åŒ–

### 11.3.1 MoEæ¶æ„ç®€ä»‹

**ä»€ä¹ˆæ˜¯MoE**(Mixture of Experts):

```python
# ä¼ ç»ŸDenseæ¨¡å‹
output = DenseLayer(input)  # æ‰€æœ‰å‚æ•°éƒ½å‚ä¸è®¡ç®—

# MoEæ¨¡å‹
class MoELayer:
    def __init__(self, num_experts: int):
        self.gate = GateNetwork()  # è·¯ç”±ç½‘ç»œ
        self.experts = [Expert() for _ in range(num_experts)]

    def forward(self, x):
        # 1. Gateå†³å®šä½¿ç”¨å“ªäº›ä¸“å®¶
        expert_weights = self.gate(x)  # [batch, num_experts]

        # 2. ç¨€ç–æ¿€æ´»: åªä½¿ç”¨top-kä¸“å®¶
        top_k_experts = expert_weights.topk(k=2)

        # 3. è®¡ç®—ä¸“å®¶è¾“å‡º
        outputs = []
        for expert_id in top_k_experts:
            expert_output = self.experts[expert_id](x)
            outputs.append(expert_output)

        # 4. åŠ æƒç»„åˆ
        output = sum(outputs * expert_weights)
        return output
```

**MoEçš„ä¼˜åŠ¿**:
- **ç¨€ç–æ¿€æ´»**: æ¯ä¸ªtokenåªä½¿ç”¨éƒ¨åˆ†ä¸“å®¶
- **æ¨¡å‹å®¹é‡å¤§**: æ€»å‚æ•°é‡å¤š,ä½†è®¡ç®—é‡å°‘
- **æˆæœ¬ä¼˜åŒ–**: æ¨ç†æˆæœ¬é™ä½30-50%

### 11.3.2 MoEæ¨ç†çš„ç‰¹æ®ŠæŒ‘æˆ˜

**æŒ‘æˆ˜1: ä¸“å®¶è´Ÿè½½ä¸å‡è¡¡**

```python
# é—®é¢˜: æŸäº›ä¸“å®¶è¢«é¢‘ç¹è°ƒç”¨,æŸäº›ä¸“å®¶å¾ˆå°‘è¢«è°ƒç”¨
expert_call_counts = {
    "expert_0": 10000,  # çƒ­ç‚¹ä¸“å®¶
    "expert_1": 50,     # å†·é—¨ä¸“å®¶
    # ...
}

# å¯¼è‡´:
# - çƒ­ç‚¹ä¸“å®¶æˆä¸ºç“¶é¢ˆ
# - GPUåˆ©ç”¨ç‡ä¸å‡
# - æ•´ä½“ååé‡ä¸‹é™
```

**æŒ‘æˆ˜2: è·¨GPUé€šä¿¡**

```python
# å‡è®¾ä¸“å®¶åˆ†å¸ƒåœ¨å¤šä¸ªGPUä¸Š
# Tokenéœ€è¦è·¯ç”±åˆ°ä¸åŒçš„GPU
# All-to-Allé€šä¿¡å¼€é”€å¤§

communication_cost = O(num_tokens * num_gpus * num_experts)
```

**æŒ‘æˆ˜3: KV Cacheç®¡ç†**

```python
# ä¸åŒä¸“å®¶çš„KV Cacheä¸åŒ
# å¦‚ä½•å…±äº«å’Œå¤ç”¨?

# Token A: ä½¿ç”¨Expert 1, 3
# Token B: ä½¿ç”¨Expert 2, 4
# KV Cacheæ— æ³•ç›´æ¥å¤ç”¨!
```

### 11.3.3 ä¸“å®¶è·¯ç”±ä¼˜åŒ–

**è´Ÿè½½å‡è¡¡ç­–ç•¥**:

```python
class LoadBalancedGate:
    """è´Ÿè½½å‡è¡¡çš„è·¯ç”±ç½‘ç»œ"""

    def __init__(self, num_experts: int):
        self.num_experts = num_experts
        self.expert_loads = [0] * num_experts

    def route(self, x: Tensor, capacity_factor: float = 1.0):
        # 1. è®¡ç®—æ¯ä¸ªtokençš„ä¸“å®¶åå¥½
        logits = self.gate(x)  # [batch, num_experts]

        # 2. è€ƒè™‘ä¸“å®¶è´Ÿè½½
        for i in range(self.num_experts):
            logits[:, i] -= self.expert_loads[i] * 0.1

        # 3. Top-kè·¯ç”±
        top_k_experts = logits.topk(k=2)

        # 4. æ›´æ–°è´Ÿè½½è®¡æ•°
        for expert_id in top_k_experts:
            self.expert_loads[expert_id] += 1

        return top_k_experts
```

**ä¸“å®¶äº²å’Œæ€§**(Expert Affinity):

```python
# å°†ç›¸å…³çš„tokenè·¯ç”±åˆ°ç›¸åŒçš„ä¸“å®¶
# æå‡KV Cacheå¤ç”¨ç‡

def expert_affinity_routing(tokens: List[Token]):
    """åŸºäºtokenç›¸ä¼¼åº¦çš„è·¯ç”±"""

    # è®¡ç®—token embedding
    embeddings = [get_embedding(t) for t in tokens]

    # èšç±»ç›¸ä¼¼çš„token
    clusters = cluster_embeddings(embeddings)

    # åŒä¸€clusterçš„tokenä½¿ç”¨ç›¸åŒçš„ä¸“å®¶
    for cluster_id, token_ids in clusters.items():
        expert_id = assign_expert(cluster_id)
        for token_id in token_ids:
            route_token(token_id, expert_id)
```

### 11.3.4 Checkpointç®¡ç†

**Tçº§åˆ«æ¨¡å‹checkpointå·¨å¤§**(å¼ åšæ¶µ@æµ™å¤§):

```python
# DeepSeek-V3: 671Bå‚æ•°
# Checkpointå¤§å°: ~1.3TB (BF16)

# é—®é¢˜:
# 1. ä¿å­˜æ—¶é—´é•¿
# 2. åŠ è½½æ—¶é—´é•¿
# 3. å­˜å‚¨æˆæœ¬é«˜

# è§£å†³æ–¹æ¡ˆ: Partial Checkpoint
class PartialCheckpoint:
    """éƒ¨åˆ†checkpointä¿å­˜"""

    def save(self, experts: List[Expert], expert_ids: List[int]):
        """åªä¿å­˜æŒ‡å®šçš„ä¸“å®¶"""
        for expert_id in expert_ids:
            expert = experts[expert_id]
            self.save_expert(expert, expert_id)

    def load(self, expert_ids: List[int]) -> List[Expert]:
        """åªåŠ è½½éœ€è¦çš„ä¸“å®¶"""
        experts = []
        for expert_id in expert_ids:
            expert = self.load_expert(expert_id)
            experts.append(expert)
        return experts

# æ•…éšœæ¢å¤: å±è”½æŒ‚æ‰çš„ä¸“å®¶
def handle_expert_failure(failed_expert_id: int):
    """å¤„ç†ä¸“å®¶å¤±è´¥"""

    # æ–¹æ¡ˆ1: ä½¿ç”¨å¤‡ç”¨ä¸“å®¶
    backup_expert_id = get_backup_expert(failed_expert_id)
    remap_routing(failed_expert_id, backup_expert_id)

    # æ–¹æ¡ˆ2: é‡æ–°åˆå§‹åŒ–ä¸“å®¶
    new_expert = initialize_expert()
    replace_expert(failed_expert_id, new_expert)
```

### 11.3.5 å®æˆ˜: Mixtraléƒ¨ç½²

```bash
# ä½¿ç”¨vLLMéƒ¨ç½²Mixtral 8x7B
vllm serve mistralai/Mixtral-8x7B-Instruct-v0.1 \
  --tensor-parallel-size 4 \
  --max-model-len 8192 \
  --enable-prefix-caching

# æ€§èƒ½è°ƒä¼˜
# 1. è°ƒæ•´expertå¹¶è¡Œåº¦
export EXPERT_PARALLEL_SIZE=2

# 2. å¯ç”¨ä¸“å®¶è´Ÿè½½å‡è¡¡
export LOAD_BALANCE_STRATEGY=capacity_factor

# 3. ä¼˜åŒ–é€šä¿¡
export USE_NCCL=1
export NCCL_IB_DISABLE=0  # å¯ç”¨InfiniBand
```

---

## 11.4 å¤šæ¨¡æ€æ¨¡å‹æ¨ç†

### 11.4.1 å¤šæ¨¡æ€æ¨¡å‹æ¦‚è¿°

**å…¸å‹æ¶æ„**(LLaVA):

```python
class LLaVA:
    """Vision-Language Model"""

    def __init__(self):
        self.vision_encoder = CLIPVisionEncoder()  # è§†è§‰ç¼–ç å™¨
        self.projector = LinearProjection()  # è§†è§‰-è¯­è¨€æŠ•å½±
        self.llm = LLaMAModel()  # è¯­è¨€æ¨¡å‹

    def generate(self, image: Image, prompt: str):
        # 1. ç¼–ç å›¾åƒ
        image_features = self.vision_encoder(image)  # [num_patches, dim]

        # 2. æŠ•å½±åˆ°è¯­è¨€ç©ºé—´
        projected_features = self.projector(image_features)

        # 3. æ‹¼æ¥æ–‡æœ¬prompt
        text_tokens = tokenize(prompt)
        inputs = concatenate(projected_features, text_tokens)

        # 4. LLMç”Ÿæˆ
        outputs = self.llm.generate(inputs)
        return outputs
```

### 11.4.2 è§†è§‰ç¼–ç å™¨ä¼˜åŒ–

**æŒ‘æˆ˜**: å›¾åƒç¼–ç è®¡ç®—é‡å¤§

```
å›¾åƒ: 512x512 = 262K pixels
Patches: 16x16 = 1024 patches
Vision Encoder: ViT-L/14 (~300Må‚æ•°)

è®¡ç®—: 300M params Ã— 1024 patches â‰ˆ 300B FLOPs
```

**ä¼˜åŒ–ç­–ç•¥**:

1. **æå‰è®¡ç®—å›¾åƒç‰¹å¾**
   ```python
   # Cacheå›¾åƒfeatures
   image_features_cache = {}

   def get_image_features(image: Image):
       image_id = hash(image.tobytes())

       if image_id in image_features_cache:
           return image_features_cache[image_id]

       features = vision_encoder(image)
       image_features_cache[image_id] = features
       return features
   ```

2. **é‡åŒ–è§†è§‰ç¼–ç å™¨**
   ```python
   # INT8é‡åŒ–
   quantized_vision_encoder = quantize(vision_encoder, dtype=torch.int8)

   # æ€§èƒ½æå‡: 2x
   # ç²¾åº¦æŸå¤±: <1%
   ```

3. **æ‰¹å¤„ç†å¤šå¼ å›¾åƒ**
   ```python
   # Batch encode
   images = [image1, image2, image3, ...]
   batch_features = vision_encoder(images)  # [batch, num_patches, dim]

   # æ¯”å•å¼ ç¼–ç å¿«4-8x
   ```

### 11.4.3 å¤šæ¨¡æ€æ¨ç†æµæ°´çº¿

**å®Œæ•´çš„æµæ°´çº¿**:

```python
class MultiModalPipeline:
    """å¤šæ¨¡æ€æ¨ç†æµæ°´çº¿"""

    def __init__(self):
        self.vision_encoder = CLIPVisionEncoder()
        self.llm = vLLM(model="llava-v1.5-7b")

    def generate(self, image: Image, prompt: str):
        # Stage 1: è§†è§‰ç¼–ç (CPU/GPUå¹¶è¡Œ)
        with ThreadPoolExecutor() as executor:
            vision_future = executor.submit(self.vision_encoder, image)

            # Stage 2: æ–‡æœ¬tokenization(CPU)
            text_tokens = tokenize(prompt)

            # ç­‰å¾…visionå®Œæˆ
            image_features = vision_future.result()

        # Stage 3: ç‰¹å¾èåˆ
        inputs = prepare_inputs(image_features, text_tokens)

        # Stage 4: LLMç”Ÿæˆ
        outputs = self.llm.generate(inputs)

        return outputs
```

**æ€§èƒ½ä¼˜åŒ–**:

```python
# ä¼˜åŒ–1: Pipelineå¹¶è¡Œ
# - Vision Encoderå’ŒLLMå¯ä»¥å¹¶è¡Œæ‰§è¡Œ

# ä¼˜åŒ–2: å¼‚æ­¥é¢„å¤„ç†
async def async_generate(image: Image, prompt: str):
    # å¼‚æ­¥åŠ è½½å›¾åƒ
    image = await async_load_image(image)

    # å¼‚æ­¥ç¼–ç 
    image_features = await async_encode(image)

    # å¼‚æ­¥ç”Ÿæˆ
    outputs = await llm.async_generate(image_features, prompt)

    return outputs

# ä¼˜åŒ–3: KV Cache for Vision Features
# - ç›¸åŒå›¾åƒçš„å¤šæ¬¡å¯¹è¯å¯ä»¥å¤ç”¨vision features
# - ç±»ä¼¼Prefix Caching
```

### 11.4.4 Video Generationçš„æŒ‘æˆ˜

**Diffusion RLçš„å°´å°¬**(å¼ åšæ¶µ@æµ™å¤§):
```
åšç®—æ³•çš„:
  - infraå¤ªæ…¢
  - è®­ç»ƒæ—¶é—´å¤ªé•¿

åšç³»ç»Ÿçš„:
  - ç®—æ³•è¿˜æ²¡æˆç†Ÿ
  - ç­‰ç®—æ³•æˆç†Ÿå†è¯´

ä¸¤è¾¹å¤§çœ¼çªå°çœ¼
```

**æŠ€æœ¯ç–‘é—®**:
- Diffusionçš„è®­ç»ƒæ¨ç†åˆ†ç¦»æ˜¯å¦æˆç«‹?
  - è®­ç»ƒ: computation bound
  - æ¨ç†: I/O bound

**å¸‚åœºç©ºç™½**:
- Video generationæ²¡æœ‰å¥½çš„å¼€æºè®­ç»ƒæ¡†æ¶
- å¸‚é¢ä¸Šæ²¡æœ‰å¾ˆå¥½çš„Diffusion RLç³»ç»Ÿ

---

## 11.5 Torch Compileä¼˜åŒ–

### 11.5.1 torch.compileåŸç†

```python
import torch

# æœªä¼˜åŒ–
def model_forward(x):
    return model(x)

# ä½¿ç”¨torch.compile
compiled_model = torch.compile(model)

# torch.compileåšä»€ä¹ˆ?
# 1. Tracing: è®°å½•è®¡ç®—å›¾
# 2. Graph Analysis: åˆ†æä¼˜åŒ–æœºä¼š
# 3. Code Generation: ç”Ÿæˆä¼˜åŒ–åçš„ä»£ç 
# 4. Compilation: ç¼–è¯‘ä¸ºæœºå™¨ç 
```

**ä¼˜åŒ–æŠ€æœ¯**:
- **Dead Code Elimination**: ç§»é™¤æ— ç”¨ä»£ç 
- **Operator Fusion**: èåˆå¤šä¸ªæ“ä½œ
- **Memory Layout Optimization**: ä¼˜åŒ–å†…å­˜å¸ƒå±€
- **Loop Unrolling**: å±•å¼€å¾ªç¯

### 11.5.2 åœ¨æ¨ç†ä¸­çš„åº”ç”¨

```python
import torch
from vllm import LLM

# åŸå§‹æ¨¡å‹
llm = LLM(model="meta-llama/Llama-3.1-8B")

# åº”ç”¨torch.compile
# æ³¨æ„: vLLMå†…éƒ¨å·²ç»ä¼˜åŒ–,å¯èƒ½ä¸éœ€è¦é¢å¤–compile
import torch._dynamo
torch._dynamo.config.suppress_errors = True

compiled_model = torch.compile(
    llm.llm_engine.model_runner.model,
    mode="reduce-overhead"
)
```

### 11.5.3 ä¸vLLMç»“åˆ

```python
# vLLM 0.6.0+ æ”¯æŒtorch.compile
VLLM_USE_TORCH_COMPILE=1 vllm serve meta-llama/Llama-3.1-8B

# æ€§èƒ½æå‡:
# - P50 latency: -5%
# - P95 latency: -3%
# - Throughput: +2%

# æ³¨æ„: æå‡å¹…åº¦æœ‰é™,vLLMå·²ç»é«˜åº¦ä¼˜åŒ–
```

---

## 11.6 Flash Attention

### 11.6.1 Flash AttentionåŸç†

**æ ‡å‡†Attentionçš„é—®é¢˜**:

```python
# æ ‡å‡†Attention: O(NÂ²) å†…å­˜å¤æ‚åº¦
def standard_attention(Q, K, V):
    # Q, K, V: [batch, seq_len, dim]

    # 1. è®¡ç®—attention scores: [batch, seq_len, seq_len]
    scores = Q @ K.T / sqrt(d_k)  # O(NÂ²) å†…å­˜!

    # 2. Softmax
    attn = softmax(scores)

    # 3. åŠ æƒæ±‚å’Œ
    output = attn @ V

    return output
```

**Flash Attentionçš„ä¼˜åŒ–**:

```python
# Flash Attention: O(N) å†…å­˜å¤æ‚åº¦
# åˆ†å—è®¡ç®— + åœ¨çº¿Softmax

def flash_attention(Q, K, V, block_size=64):
    seq_len = Q.shape[1]
    outputs = []

    for i in range(0, seq_len, block_size):
        Q_block = Q[:, i:i+block_size, :]  # [batch, block, dim]

        # åœ¨çº¿æ›´æ–°attention statistics
        O_block = torch.zeros_like(Q_block)
        l = torch.zeros(Q_block.shape[0], Q_block.shape[1])  # logsumexp
        m = torch.full((Q_block.shape[0], Q_block.shape[1]), -float('inf'))  # max

        for j in range(0, seq_len, block_size):
            K_block = K[:, j:j+block_size, :]
            V_block = V[:, j:j+block_size, :]

            # è®¡ç®—block attention
            S_block = Q_block @ K_block.T / sqrt(d_k)
            m_new = torch.max(m, S_block.max(dim=-1).values)
            l_new = torch.exp(m - m_new) * l + torch.exp(S_block - m_new).sum(dim=-1)
            O_block = (l / l_new).unsqueeze(-1) * O_block + \
                      torch.exp(S_block - m_new.unsqueeze(-1)) @ V_block

            m = m_new
            l = l_new

        outputs.append(O_block)

    return torch.cat(outputs, dim=1)
```

### 11.6.2 Flash Attention 2

**Flash Attention 2æ”¹è¿›**:
- æ›´å¥½çš„work partition
- å‡å°‘éçŸ©é˜µè®¡ç®—
- æ›´å¥½çš„å¹¶è¡Œæ€§

```python
# ä½¿ç”¨Flash Attention 2
from flash_attn import flash_attn_qkvpacked_func

# QKV packed format: [batch, seq_len, 3, heads, dim]
qkv = torch.stack([Q, K, V], dim=2)

output = flash_attn_qkvpacked_func(qkv)
```

### 11.6.3 Sparse Attention vs Linear Attention

**è¶‹åŠ¿**(å¼ åšæ¶µ@æµ™å¤§):
```
å¤§å‚é€æ¸æ”¾å¼ƒlinear attention
æ”¶æ•›åˆ°sparse attention
```

**åŸå› **:
- Agentåœºæ™¯æ˜¯multi-turnçš„long context
- ç†æƒ³æƒ…å†µ: å…¨å­˜,sparse retrieval
- Make sense

**æŒ‘æˆ˜**:
- åœ¨long context reasoningåœºæ™¯ä¸‹
- æ€ä¹ˆæŠŠsparse attentionåšä¸æ‰ç‚¹?
- ä¾‹å¦‚: Needle In A Haystack(å¤§æµ·æå¤šé’ˆ)
  - Claude 3ç²¾åº¦åªæœ‰20-30%

### 11.6.4 æ€§èƒ½æå‡

```
æ ‡å‡†Attention:
  - FLOPs: 2NÂ²d
  - Memory: O(NÂ²)
  - Speed: Baseline

Flash Attention:
  - FLOPs: 2NÂ²d (ç›¸åŒ)
  - Memory: O(N)
  - Speed: 2-4x faster

Flash Attention 2:
  - FLOPs: 2NÂ²d (ç›¸åŒ)
  - Memory: O(N)
  - Speed: 2-3x faster than FA1
```

### 11.6.5 åœ¨vLLMä¸­çš„ä½¿ç”¨

```bash
# vLLMé»˜è®¤å¯ç”¨Flash Attention
vllm serve meta-llama/Llama-3.1-8B \
  --attention-backend flash \  # æ˜¾å¼æŒ‡å®š
  --max-model-len 32768

# æ€§èƒ½æå‡:
# - Long sequence: 2-3x faster
# - Memory: 50% reduction for 32K context
```

---

## 11.7 è‡ªå®šä¹‰ç®—å­å¼€å‘

### 11.7.1 ä½•æ—¶éœ€è¦è‡ªå®šä¹‰ç®—å­

**åœºæ™¯**:
1. **æ€§èƒ½ç“¶é¢ˆ**: ç°æœ‰ç®—å­æ€§èƒ½ä¸å¤Ÿ
2. **æ–°ç®—æ³•**: PyTorchæ²¡æœ‰å®ç°
3. **ç‰¹æ®Šä¼˜åŒ–**: é’ˆå¯¹ç‰¹å®šç¡¬ä»¶ä¼˜åŒ–

**ç¤ºä¾‹**:
- è‡ªå®šä¹‰Attentionå®ç°
- ç‰¹æ®Šé‡åŒ–ç®—å­
- MoEä¸“å®¶è·¯ç”±

### 11.7.2 CUDAç¼–ç¨‹åŸºç¡€

```cuda
// simple_add_kernel.cu
__global__ void simple_add_kernel(float* A, float* B, float* C, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < N) {
        C[idx] = A[idx] + B[idx];
    }
}

// Host code
extern "C" void launch_simple_add(float* A, float* B, float* C, int N) {
    int threads_per_block = 256;
    int blocks = (N + threads_per_block - 1) / threads_per_block;

    simple_add_kernel<<<blocks, threads_per_block>>>(A, B, C, N);
    cudaDeviceSynchronize();
}
```

### 11.7.3 Tritonè¯­è¨€ç®€ä»‹

```python
# Triton: Python-like GPUç¼–ç¨‹è¯­è¨€
import triton
import triton.language as tl

@triton.jit
def add_kernel(
    x_ptr, y_ptr, output_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    # ç¨‹åºID
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements

    # åŠ è½½æ•°æ®
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)

    # è®¡ç®—
    output = x + y

    # å†™å›
    tl.store(output_ptr + offsets, output, mask=mask)
```

### 11.7.4 å¼€å‘æµç¨‹

**Step 1: PyTorchå®ç°**

```python
def custom_op(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    # åŸå‹å®ç°
    return x + y
```

**Step 2: CUDA/Tritonä¼˜åŒ–**

```python
# ä½¿ç”¨Tritonä¼˜åŒ–
from triton import jit

@jit
def optimized_custom_op(x, y):
    return add_kernel(x, y)
```

**Step 3: é›†æˆåˆ°PyTorch**

```python
import torch
from torch.autograd import Function

class CustomOpFunction(Function):
    @staticmethod
    def forward(ctx, x, y):
        return optimized_custom_op(x, y)

    @staticmethod
    def backward(ctx, grad_output):
        # åå‘ä¼ æ’­
        return grad_output, grad_output

# ä½¿ç”¨
custom_op = CustomOpFunction.apply
```

**Step 4: æ€§èƒ½æµ‹è¯•**

```python
import time

def benchmark(func, *args, **kwargs):
    start = time.time()
    for _ in range(100):
        func(*args, **kwargs)
    torch.cuda.synchronize()
    end = time.time()

    return (end - start) / 100

# å¯¹æ¯”
torch_time = benchmark(torch.add, x, y)
custom_time = benchmark(custom_op, x, y)

print(f"PyTorch: {torch_time*1000:.2f}ms")
print(f"Custom: {custom_time*1000:.2f}ms")
print(f"Speedup: {torch_time/custom_time:.2f}x")
```

### 11.7.5 å‰ç«¯æ€§èƒ½ä¼˜åŒ–

**é—®é¢˜**(åˆ˜æµ·è¶…@vLLM):
- Pythonå†™web serviceæ€§èƒ½å·®
- éœ€è¦åŠ rest
- Inferenceçš„CPUä¼˜åŒ–
- æ˜¯å¦ç”¨C++(PyTorchä¹Ÿåœ¨è€ƒè™‘)

**è§£å†³æ–¹æ¡ˆ**:

```python
# ä½¿ç”¨FastAPI + uvicorn
from fastapi import FastAPI
import uvicorn

app = FastAPI()

@app.post("/generate")
async def generate(request: GenerateRequest):
    # å¼‚æ­¥å¤„ç†
    output = await llm.async_generate(request.prompt)
    return {"output": output}

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=4,  # å¤šworker
        loop="uvloop",  # é«˜æ€§èƒ½event loop
    )
```

---

## 11.8 æŠ€æœ¯å‘å±•ä¸å±•æœ›

> **ğŸ’¡ 2025å¹´æŠ€æœ¯è¶‹åŠ¿**: MoEæ¶æ„çš„å¤§è§„æ¨¡éƒ¨ç½²æˆä¸ºçƒ­ç‚¹,ä»å•ä¸€æ¨¡å‹åˆ°åˆ†å¸ƒå¼ä¸“å®¶ç³»ç»Ÿ,æ–°çš„æ¶æ„æ¨¡å¼æ­£åœ¨æ¶Œç°ã€‚

### 11.8.1 å¤§è§„æ¨¡MoEæœåŠ¡ (Large-scale Expert Parallelism)

> **æ¥æº**: [vLLM Blog - Large-scale Serving](https://blog.vllm.ai/2025/12/17/large-scale-serving.html)
>
> **æ ¸å¿ƒä»·å€¼**: è§£å†³ä¸‡äº¿å‚æ•°MoEæ¨¡å‹çš„éƒ¨ç½²éš¾é¢˜

**ä»€ä¹ˆæ˜¯Large EP**:
- ä¼ ç»Ÿçš„Tensor Parallelismåœ¨MoEä¸Šçš„å±€é™
- Expert Parallelism: å°†ä¸åŒä¸“å®¶åˆ†é…åˆ°ä¸åŒGPU
- è·¨èŠ‚ç‚¹çš„ä¸“å®¶è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
- All-to-Allé€šä¿¡ä¼˜åŒ–

**å…³é”®æŠ€æœ¯æŒ‘æˆ˜**:

1. **ä¸“å®¶è´Ÿè½½å‡è¡¡**
   - ä¸åŒä¸“å®¶çš„è®¿é—®é¢‘ç‡å·®å¼‚
   - åŠ¨æ€è·¯ç”±ç­–ç•¥
   - é¿å…çƒ­ç‚¹ä¸“å®¶è¿‡è½½

2. **é€šä¿¡ä¼˜åŒ–**
   - å‡å°‘è·¨èŠ‚ç‚¹All-to-Allé€šä¿¡
   - é€šä¿¡è®¡ç®—é‡å 
   - RDMAåŠ é€Ÿ

3. **å®¹é”™å’Œå¼¹æ€§**
   - ä¸“å®¶å¤±è´¥çš„å¤„ç†
   - åŠ¨æ€æ‰©ç¼©å®¹ä¸“å®¶æ•°é‡

**vLLMçš„å®ç°**:
- åˆ†å¸ƒå¼è°ƒåº¦å™¨è®¾è®¡
- ä¸“å®¶è·¯ç”±ç®—æ³•
- æ€§èƒ½åŸºå‡†æµ‹è¯•
- ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### 11.8.2 EPD: Expert-Parallel Data Parallelism

> **æ¥æº**: [vLLM Blog - EPD](https://blog.vllm.ai/2025/12/15/vllm-epd.html)
>
> **æ ¸å¿ƒä»·å€¼**: ç»“åˆä¸“å®¶å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œ,æå‡MoEæ¨ç†æ•ˆç‡

**EPDçš„æ ¸å¿ƒæ€æƒ³**:

ä¼ ç»ŸMoEéƒ¨ç½²çš„é—®é¢˜:
- å•çº¯Expert Parallelism: GPUåˆ©ç”¨ç‡ä¸å‡
- å•çº¯Data Parallelism: æ— æ³•å¤„ç†è¶…å¤§MoE

EPDçš„åˆ›æ–°:
- æ¯ä¸ªGPU: å¤šä¸ªä¸“å®¶çš„å‰¯æœ¬ + Dataå¹¶è¡Œ
- æ›´å¥½çš„è´Ÿè½½å‡è¡¡
- æå‡æ•´ä½“GPUåˆ©ç”¨ç‡

**æ€§èƒ½æå‡**:
- ååé‡æå‡: 2-3x
- å»¶è¿Ÿé™ä½: P95æ”¹å–„40%
- GPUåˆ©ç”¨ç‡: ä»60%æå‡åˆ°85%+

### 11.8.3 Elastic Expert Parallelism

> **æ¥æº**: [vLLM Issue #20323](https://github.com/vllm-project/vllm/issues/20323)

**æ ¸å¿ƒä»·å€¼**: åŠ¨æ€è°ƒæ•´ä¸“å®¶å¹¶è¡Œåº¦,é€‚åº”ä¸åŒè´Ÿè½½

**ä»€ä¹ˆæ˜¯Elastic EP**:
- é™æ€EPçš„é—®é¢˜: æ— æ³•é€‚åº”æµé‡æ³¢åŠ¨
- Elastic EP: æ ¹æ®è´Ÿè½½åŠ¨æ€è°ƒæ•´ä¸“å®¶å‰¯æœ¬æ•°
- å¼¹æ€§æ‰©ç¼©å®¹ä¸“å®¶

**åº”ç”¨åœºæ™¯**:
- æµé‡æ³¢åŠ¨å¤§çš„æœåŠ¡
- å¤šç§Ÿæˆ·ç¯å¢ƒ
- æˆæœ¬æ•æ„Ÿçš„éƒ¨ç½²

### 11.8.4 åˆ†ç¦»å¼æ¶æ„: MoonCakeèŒƒå¼

> **æ¥æº**: [MoonCake GitHub](https://github.com/kvcache-aif/MoonCake)
>
> **æ ¸å¿ƒä»·å€¼**: å½»åº•è§£è€¦Prefillå’ŒDecode,å®ç°ä¸“ç”¨çš„æ¨ç†é›†ç¾¤

**MoonCakeçš„æ ¸å¿ƒè®¾è®¡**:

```python
# disaggregated architecture

# Prefillé›†ç¾¤: è®¡ç®—ä¼˜åŒ–å‹GPU(H100)
prefill_cluster = Cluster(
    gpu_type="H100",
    purpose="compute",
    optimization="flops"
)

# Decodeé›†ç¾¤: å¸¦å®½ä¼˜åŒ–å‹GPU(H200ã€L40s)
decode_cluster = Cluster(
    gpu_type="H200",
    purpose="bandwidth",
    optimization="memory_bandwidth"
)

# KV Cacheé›†ç¾¤: é«˜å†…å­˜å¸¦å®½
kv_cache_cluster = Cluster(
    gpu_type="L40s",
    purpose="kv_cache",
    optimization="memory_capacity"
)
```

**ä¸ºä»€ä¹ˆåˆ†ç¦»**:
- Prefillå’ŒDecodeçš„è®¡ç®—æ¨¡å¼å®Œå…¨ä¸åŒ
- ç»Ÿä¸€éƒ¨ç½²å¯¼è‡´èµ„æºæµªè´¹
- åˆ†ç¦»åå¯åˆ†åˆ«ä¼˜åŒ–

**å…³é”®æŠ€æœ¯**:

1. **KV Cacheä¼ è¾“åè®®**
   - é«˜æ•ˆçš„åºåˆ—åŒ–å’Œååºåˆ—åŒ–
   - å¢é‡ä¼ è¾“
   - å‹ç¼©ç®—æ³•

2. **è¯·æ±‚è°ƒåº¦**
   - Prefillé˜Ÿåˆ—ç®¡ç†
   - Decodeé˜Ÿåˆ—ç®¡ç†
   - ä¸¤è€…ä¹‹é—´çš„é€Ÿç‡åŒ¹é…

3. **å®¹é”™æœºåˆ¶**
   - KV Cacheçš„æŒä¹…åŒ–
   - æ•…éšœæ¢å¤
   - é‡æ–°è®¡ç®—ç­–ç•¥

**æ€§èƒ½ä¼˜åŠ¿**:
- æˆæœ¬é™ä½: 40-60%
- ååæå‡: 2-3x
- èµ„æºåˆ©ç”¨ç‡: ä»50%æå‡åˆ°80%+
- å¼¹æ€§æ‰©å±•: Prefillå’ŒDecodeç‹¬ç«‹æ‰©ç¼©å®¹

### 11.8.5 æŠ€æœ¯æ ˆæ·±åŒ–: ä»æ¡†æ¶åˆ°ç½‘ç»œ

> **æ¥æº**: åˆ˜æµ·è¶…@vLLM (2025"é’ç¨"AIå˜‰å¹´å)
>
> **æ ¸å¿ƒæ´å¯Ÿ**: 2025å¹´çš„ä¼˜åŒ–å·²ç»è¶…å‡ºäº†æ¨ç†æ¡†æ¶æœ¬èº«

**2024 vs 2025å¯¹æ¯”**:
- **2024å¹´**: æ¡†æ¶å±‚é¢ä¼˜åŒ–(vLLMã€TGI)
- **2025å¹´**: éœ€è¦æ·±å…¥åˆ°æ›´ä½å±‚æ¬¡
  - RDMAä¼˜åŒ–
  - Networkingå±‚ä¼˜åŒ–
  - Kernelå±‚ä¼˜åŒ–

**ä¸ºä»€ä¹ˆéœ€è¦æ›´æ·±å±‚**:
- æ¡†æ¶å±‚çš„ä¼˜åŒ–å·²ç»æ¥è¿‘æé™
- ç“¶é¢ˆè½¬ç§»åˆ°ç½‘ç»œå’Œé€šä¿¡
- éœ€è¦å…¨æ ˆååŒä¼˜åŒ–

**æŠ€æœ¯è¦æ±‚**:
- éœ€è¦æ‡‚: ç®—æ³• + ç¡¬ä»¶ + ç³»ç»Ÿ + ç½‘ç»œ
- è·¨é¢†åŸŸåä½œæˆä¸ºå¸¸æ€
- äººæ‰ç¨€ç¼ºæ€§å¢åŠ 

### 11.8.6 ä»SPMDåˆ°Event Driven

> **æ¥æº**: å¼ æ˜æ˜Ÿ@æ¸…å (2025"é’ç¨"AIå˜‰å¹´å)
>
> **æ ¸å¿ƒæ´å¯Ÿ**: ä¼ ç»ŸSPMDæ¨¡å¼ä¸é€‚åˆæ‰€æœ‰åœºæ™¯

**SPMD (Single Program Multiple Data)**:
- ä¼ ç»Ÿçš„æ•°æ®å¹¶è¡Œæ¨¡å¼
- Workflowäº‹å…ˆprogramå¥½
- é€‚åˆå¤§è§„æ¨¡æ‰¹é‡å¤„ç†

**Event Drivenæ¨¡å¼**:
- åŠ¨æ€è°ƒåº¦å’Œæ‰§è¡Œ
- é€‚åˆbatch sizeè¾¾ä¸åˆ°çš„åœºæ™¯
- æ›´çµæ´»ä½†ç¼–ç¨‹å¤æ‚åº¦é«˜

**é€‚ç”¨åœºæ™¯å¯¹æ¯”**:

**SPMDé€‚åˆ**:
- é«˜ååé‡åœºæ™¯
- è¯·æ±‚æ¨¡å¼ç¨³å®š
- æ‰¹å¤„ç†ä»»åŠ¡

**Event Drivené€‚åˆ**:
- ä½å»¶è¿Ÿè¦æ±‚
- è¯·æ±‚æ¨¡å¼å¤šå˜
- äº¤äº’å¼åº”ç”¨

### 11.8.7 ç®—æ³•å’Œç³»ç»Ÿçš„Co-Design

> **æ¥æº**: å¼ åšæ¶µ@æµ™å¤§ (2025"é’ç¨"AIå˜‰å¹´å)
>
> **æ ¸å¿ƒæ´å¯Ÿ**: ç®—æ³•å’Œç³»ç»Ÿéœ€è¦åŒæ­¥èºæ—‹å¼ä¸Šå‡

**ä¼ ç»Ÿæ¨¡å¼çš„é—®é¢˜**:
- ç³»ç»Ÿå›¢é˜Ÿ: ç­‰ç®—æ³•æˆç†Ÿå†åšä¼˜åŒ–
- ç®—æ³•å›¢é˜Ÿ: ç­‰ç³»ç»Ÿä¼˜åŒ–å¥½å†å®éªŒ
- ç»“æœ: ä¸¤è¾¹éƒ½åœ¨ç­‰,è¿›åº¦ç¼“æ…¢

**Co-Designæ–¹æ³•**:

**åŒæ­¥èºæ—‹å¼ä¸Šå‡**:
- ç®—æ³•å’Œç³»ç»ŸåŒæ­¥æ¼”è¿›
- æ¯ä¸ªç‰ˆæœ¬éƒ½äº’ç›¸åé¦ˆ
- å¿«é€Ÿè¿­ä»£éªŒè¯

**æ¡ˆä¾‹**:
- INT4 QAT: ç®—æ³•åˆ›æ–° + ç³»ç»Ÿä¼˜åŒ–
- PDåˆ†ç¦»: æ¶æ„åˆ›æ–° + å·¥ç¨‹å®ç°

**å®è·µå»ºè®®**:
- å»ºç«‹è”åˆå¼€å‘å›¢é˜Ÿ
- å…±äº«æ€§èƒ½åŸºå‡†
- å®šæœŸæŠ€æœ¯åŒæ­¥

---

## ğŸš« å¸¸è§è¯¯åŒº

### âŒ "MoEæ€»æ˜¯æ›´ä¾¿å®œ"

**å®é™…æƒ…å†µ**: å–å†³äºéƒ¨ç½²ç­–ç•¥ã€‚

```python
# Denseæ¨¡å‹
# - å‚æ•°å°‘,ä½†æ‰€æœ‰å‚æ•°éƒ½å‚ä¸è®¡ç®—
# - é€‚åˆ: å°æ¨¡å‹ã€ä½å¹¶å‘

# MoEæ¨¡å‹
# - å‚æ•°å¤š,ä½†ç¨€ç–æ¿€æ´»
# - é€‚åˆ: å¤§æ¨¡å‹ã€é«˜å¹¶å‘

# æˆæœ¬å¯¹æ¯”:
# 70B Dense vs 8x7B MoE
# - Dense: å›ºå®šæˆæœ¬
# - MoE: åŸºç¡€æˆæœ¬ + è·¯ç”±æˆæœ¬ + é€šä¿¡æˆæœ¬
# - ç»“è®º: åªæœ‰åœ¨é«˜å¹¶å‘æ—¶MoEæ‰æ›´ä¾¿å®œ
```

### âŒ "æ›´å¤šGPUæ€»æ˜¯æ›´å¿«"

**å®é™…æƒ…å†µ**: é€šä¿¡å¼€é”€å¯èƒ½æŠµæ¶ˆæ”¶ç›Šã€‚

```python
# å•GPU: 100 tokens/s
# 2 GPU (TP): 180 tokens/s (80%æ•ˆç‡)
# 4 GPU (TP): 300 tokens/s (75%æ•ˆç‡)
# 8 GPU (TP): 400 tokens/s (50%æ•ˆç‡)

# ä¸ºä»€ä¹ˆ?
# - è·¨GPUé€šä¿¡å¼€é”€
# - è´Ÿè½½ä¸å‡è¡¡
# - å¸¦å®½ç“¶é¢ˆ
```

### âŒ "Agentç³»ç»Ÿå°±æ˜¯LLM + Tools"

**å®é™…æƒ…å†µ**: Agent Infraæ˜¯å¤æ‚çš„ç³»ç»Ÿå·¥ç¨‹ã€‚

```
éœ€è¦è€ƒè™‘:
  âœ“ æ–‡ä»¶ç³»ç»Ÿç®¡ç†
  âœ“ è™šæ‹Ÿç¯å¢ƒéš”ç¦»
  âœ“ çŠ¶æ€åŒæ­¥
  âœ“ é”™è¯¯æ¢å¤
  âœ“ èµ„æºè°ƒåº¦
  âœ“ ç›‘æ§å‘Šè­¦

å¼€æºç”Ÿæ€ç¼ºå¤±æ˜¯æœ€å¤§çš„æœºä¼š!
```

### âŒ "Linear Attentionæ˜¯æœªæ¥"

**å®é™…æƒ…å†µ**: Sparse Attentionæ›´å®ç”¨ã€‚

```python
# Linear Attention
# - ç†è®º: O(N)å¤æ‚åº¦
# - å®é™…: ç²¾åº¦æŸå¤±å¤§

# Sparse Attention
# - ç†è®º: O(N log N)å¤æ‚åº¦
# - å®é™…: ç²¾åº¦æŸå¤±å°,å·¥ç¨‹å¯è¡Œ

# è¶‹åŠ¿: å¤§å‚æ”¶æ•›åˆ°Sparse Attention
```

---

## âœ… ç« èŠ‚æ£€æŸ¥æ¸…å•

é˜…è¯»æœ¬ç« å,ä½ åº”è¯¥èƒ½å¤Ÿ:

- [ ] ç†è§£Agent Infraçš„æŒ‘æˆ˜å’Œæœºé‡
- [ ] æŒæ¡Context Engineeringçš„å…­å¤§åŸåˆ™
- [ ] è®¾è®¡å¼‚æ„ç¡¬ä»¶éƒ¨ç½²æ–¹æ¡ˆ
- [ ] ä¼˜åŒ–MoEæ¨¡å‹æ¨ç†
- [ ] å¤„ç†å¤šæ¨¡æ€æ¨¡å‹æ¨ç†
- [ ] ä½¿ç”¨Flash AttentionåŠ é€Ÿ
- [ ] å¼€å‘è‡ªå®šä¹‰CUDAç®—å­
- [ ] ç†è§£å¤§è§„æ¨¡MoEæœåŠ¡çš„å‰æ²¿æŠ€æœ¯

---

## ğŸ“š åŠ¨æ‰‹ç»ƒä¹ 

**ç»ƒä¹ 11.1**: æ­å»ºç®€å•çš„Jupyter Agent

ç›®æ ‡: å®ç°ä¸€ä¸ªåœ¨Jupyterç¯å¢ƒä¸­è¿è¡Œçš„Agent

ä»»åŠ¡:
1. å®ç°ä»£ç æ‰§è¡ŒåŠŸèƒ½
2. å®ç°æ–‡ä»¶è¯»å†™åŠŸèƒ½
3. å®ç°å·¥å…·è°ƒç”¨åŠŸèƒ½
4. é›†æˆLLM

éªŒæ”¶:
```python
agent = JupyterAgent()
result = agent.execute("print(sum([1,2,3]))")
assert result == "6"
```

---

**ç»ƒä¹ 11.2**: å¼‚æ„ç¡¬ä»¶éƒ¨ç½²å®éªŒ

ç›®æ ‡: ä½“éªŒå¼‚æ„éƒ¨ç½²çš„ä¼˜åŠ¿

ä»»åŠ¡:
1. åœ¨H100ä¸Šè®­ç»ƒå°æ¨¡å‹
2. åœ¨H200ä¸Šéƒ¨ç½²æ¨ç†
3. å¯¹æ¯”æ€§èƒ½å·®å¼‚

éªŒæ”¶:
- è®°å½•è®­ç»ƒå’Œæ¨ç†çš„æ€§èƒ½
- åˆ†ææˆæœ¬å·®å¼‚
- æ€»ç»“é€‚ç”¨åœºæ™¯

---

**ç»ƒä¹ 11.3**: Context Engineeringå®è·µ

ç›®æ ‡: åº”ç”¨Manusçš„å…­å¤§åŸåˆ™

ä»»åŠ¡:
1. å®ç°KV-cache awareçš„contextç®¡ç†
2. å®ç°File System fallback
3. å®ç°Todo recitation
4. å¯¹æ¯”ä¼˜åŒ–å‰åæˆæœ¬

éªŒæ”¶:
- KV-cache hit rate > 80%
- å¹³å‡contexté•¿åº¦å‡å°‘50%
- æˆæœ¬é™ä½40%

---

## âœ… ç»ƒä¹ å‚è€ƒç­”æ¡ˆ

**ç»ƒä¹ 11.1: æ­å»ºç®€å•çš„Jupyter Agent**

```python
from jupyter_client import KernelManager
import json

class SimpleAgent:
    def __init__(self):
        # å¯åŠ¨Jupyter kernel
        self.km = KernelManager()
        self.km.start_kernel()
        self.kc = self.km.client()
        self.kc.start_channels()

    def execute_code(self, code: str) -> str:
        """æ‰§è¡ŒPythonä»£ç """
        self.kc.execute(code)
        msg = self.kc.get_shell_msg(timeout=10)

        if msg['content']['status'] == 'ok':
            # è·å–è¾“å‡º
            msg = self.kc.get_iopub_msg(timeout=10)
            if msg['content']['ename']:
                return f"Error: {msg['content']['evalue']}"
            return str(msg['content'].get('text', ''))
        return "Execution failed"

    def read_file(self, path: str) -> str:
        """è¯»å–æ–‡ä»¶"""
        code = f'with open("{path}", "r") as f: print(f.read())'
        return self.execute_code(code)

    def write_file(self, path: str, content: str):
        """å†™å…¥æ–‡ä»¶"""
        escaped_content = json.dumps(content)
        code = f'with open("{path}", "w") as f: f.write({escaped_content})'
        return self.execute_code(code)

    def __del__(self):
        self.km.shutdown_kernel()

# ä½¿ç”¨
agent = SimpleAgent()
result = agent.execute_code("print(sum([1,2,3]))")
print(result)  # 6
```

---

## ğŸ¯ æ€»ç»“

å…³é”®è¦ç‚¹:
- **Agent Infraæ˜¯æœ€å¤§çš„æœºä¼š**: å¼€æºç”Ÿæ€æ˜¯è´Ÿåˆ†,ç­‰å¾…åˆ›æ–°
- **Context Engineeringæ˜¯Agentçš„"SGD"**: å›´ç»•KV-cacheè®¾è®¡,é€šè¿‡å®éªŒå’Œè¿­ä»£æ‰¾åˆ°å±€éƒ¨æœ€ä¼˜
- **å¼‚æ„éƒ¨ç½²æ˜¯è¶‹åŠ¿**: Trainingç”¨H100,Rolloutç”¨H200,å……åˆ†åˆ©ç”¨ç¡¬ä»¶
- **MoEéœ€è¦æ–°çš„åˆ†å¸ƒå¼æŠ€æœ¯**: Large EPã€EPDã€Elastic EP
- **æŠ€æœ¯æ ˆè¶Šæ¥è¶Šæ·±**: ä»æ¡†æ¶åˆ°ç½‘ç»œåˆ°kernel,éœ€è¦å…¨æ ˆä¼˜åŒ–
- **ç®—æ³•å’Œç³»ç»Ÿéœ€è¦Co-Design**: åŒæ­¥èºæ—‹å¼ä¸Šå‡,å¿«é€Ÿè¿­ä»£éªŒè¯

**ä¸‹ä¸€æ­¥**: æ¢ç´¢é™„å½•ä¸­çš„å·¥å…·å’Œèµ„æº

---

**æœ‰é—®é¢˜?åŠ å…¥ [ç¬¬11ç«  Discordé¢‘é“](https://discord.gg/TODO) è®¨è®º!**
