# 第3章: GPU基础

> "在能够优化之前,你需要了解自己在优化什么。" - 佚名

## 简介

为什么我们需要使用 GPU 来进行 LLM 推理?难道不能直接使用 CPU 吗?简短的回答是:**你可以使用 CPU,但这会慢得让人痛苦**。本章将解释原因,并为你提供思考 GPU 性能所需的思维模型。

在本章中,你将学习:
- GPU 与 CPU 的区别(以及为什么这对推理很重要)
- GPU 架构基础(SM、内存、带宽)
- 如何计算任何模型的内存需求
- 如何监控和诊断 GPU 性能
- 关于 GPU 性能的常见误解

本章结束后,你将能够:
- ✅ 计算模型需要多少 GPU 内存
- ✅ 使用 `nvidia-smi` 诊断性能瓶颈
- ✅ 解释为什么 GPU 推理比 CPU 更快
- ✅ 为你的用例选择合适的 GPU

---

## 3.1 CPU vs GPU: 类比

### 数学教授 vs 学生

想象你需要计算 1000 个简单的数学问题(比如"2 + 3"、"7 × 4")。

**CPU = 一位数学教授**
- 极其聪明和快速
- 可以处理复杂的逻辑
- 但只有一个人在工作
- 擅长:顺序任务、复杂逻辑
- 不擅长:做 1000 次相同的简单事情

**GPU = 1000 个小学生**
- 每个学生都比教授慢
- 只能做简单的算术
- 但 1000 个人同时工作
- 擅长:并行、重复性任务
- 不擅长:复杂的决策

对于 LLM 推理,我们要进行数十亿次简单的矩阵乘法。我们不需要天才教授——我们需要一支学生大军!

---

## 3.2 GPU 架构: 思维模型

### 核心组件

```
┌─────────────────────────────────────┐
│         GPU (e.g., RTX 4090)        │
│                                     │
│  ┌─────────┐  ┌─────────┐          │
│  │   SM    │  │   SM    │  ← 128+  │
│  │ (Unit)  │  │ (Unit)  │     SMs  │
│  └────┬────┘  └────┬────┘          │
│       │            │               │
│       └─────┬──────┘               │
│             ▼                      │
│       ┌──────────┐                 │
│       │ VRAM     │  24GB           │
│       │ (Memory) │  ~1 TB/s        │
│       └──────────┘                 │
└─────────────────────────────────────┘
         ▲             │
         │             │
    PCIe 4.0       Host RAM
    ~32 GB/s       (CPU)
```

**流式多处理器(Streaming Multiprocessors, SMs)** = "学生班级"
- 一个 SM 内含多个计算核心、调度器、寄存器和共享内存
- 每个 SM 以 warp 为单位并行执行大量线程
- RTX 4090 有 128 个 SM → 并行规模非常大
- SM 越多 = 并行潜力越高(但仍受内存带宽等限制)

**VRAM(显存)** = 工作空间
- 存储模型权重和激活值
- VRAM 越多 = 模型越大或批次大小越大
- 但容量不是一切——带宽更重要!

**内存带宽** = GPU 访问显存的速度
- 数据中心卡(HBM): 数 TB/s 级别
- 消费级卡(GDDR6X): 约 1 TB/s 级别
- PCIe 4.0 x16 单向约 32 GB/s(远低于显存带宽);NVLink 更高
- 对推理来说,带宽常是瓶颈,但要结合具体模型与实现判断

---

## 3.3 计算模型内存需求

### 公式

```
总内存 = 模型权重 + KV 缓存 + 激活值 + 开销
```

这是推理时的常见拆分;训练还需要梯度与优化器状态,内存需求会显著更高。

让我们逐一分解:

### 1. 模型权重 (FP16)

对于像 Llama-3-70B 这样的模型:
- 参数量: 700 亿
- 每个参数 (FP16): 2 字节
- 总权重: 70B × 2 = 140 GB

等等,这无法放入单个 GPU!这就是为什么我们需要:
- 量化(INT8 = 1 字节, INT4 = 0.5 字节)
- 模型并行(跨 GPU 分割)
- 或者直接使用更小的模型!

### 2. KV 缓存 (每个请求)

```
KV 缓存大小(更通用) = 2 × 层数 × 序列长度 × 批次大小 × KV 头数 × 头维度 × 每参数字节数
```

对于 Llama-3-8B(粗略估算,假设 KV 头数 = 注意力头数):
- 层数: 32
- 隐藏维度: 4096
- 注意力头数: 32
- 头维度: 128 (4096/32)
- KV 头数: 32 (若不是 GQA/MQA)
- 序列长度: 2048
- 批次大小: 1
- 每参数字节数: 2 (FP16)

```
KV 缓存 = 2 × 32 × 2048 × 1 × 32 × 128 × 2
        = 1,073,741,824 字节
        ≈ 1 GB/请求
```

**重要说明**: 许多现代模型使用 GQA/MQA(即 KV 头数 < 注意力头数),KV 缓存会显著变小。上面的公式是"最保守"估算,实际可能更低。

**这会随着序列长度和批次大小增长!**

### 3. 激活值 (临时内存)

推理时通常小于模型权重的一小部分(与实现、批次大小、序列长度有关);长上下文时 KV 往往占主导。对于 Llama-3-8B (16 GB 权重),激活值约 2-3 GB。

### 4. 开销

CUDA 上下文、驱动开销: 约 0.5-2 GB(依系统与驱动而变);多 GPU 还会有通信与缓冲开销

### 综合计算: Llama-3-8B 在 24GB GPU 上

```
模型权重 (FP16): 16 GB
激活值: 2 GB
开销: 1 GB
─────────────────────
小计: 19 GB

KV 缓存剩余空间: 24 - 19 = 5 GB
可支持: 5 个并发请求(每个 1 GB)
```

---

## 3.4 GPU 监控工具

### nvidia-smi

你将使用的最基本的工具:

```bash
nvidia-smi
```

**需要理解的关键指标**:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05   Driver Version: 535.104.05   CUDA Version: 12.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:00:04.0 Off |                    0 |
| N/A   32C    P0    54W / 400W |  18939MiB / 81920MiB |     28%      Default |
+-------------------------------+----------------------+----------------------+
```

**需要关注的指标**:
- **Memory-Usage**: 接近容量? → 考虑减少批次大小
- **GPU-Util**: 内存高但利用率低(<80%)? → 可能是带宽瓶颈或 CPU/IO 受限
- **Memory-Usage** 只表示容量占用,不代表带宽利用率
- **Power Usage**: 功耗很低 + 内存占用高 → 可能是带宽/IO 瓶颈,不一定代表"没用 GPU"

### 持续监控

```bash
watch -n 1 nvidia-smi
```

### Python: pynvml

用于程序化监控:

```python
import pynvml

pynvml.nvmlInit()
handle = pynvml.nvmlDeviceGetHandleByIndex(0)

# 内存使用
info = pynvml.nvmlDeviceGetMemoryInfo(handle)
print(f"已使用: {info.used / 1024**3:.2f} GB")
print(f"总计: {info.total / 1024**3:.2f} GB")

# GPU 利用率
util = pynvml.nvmlDeviceGetUtilizationRates(handle)
print(f"GPU 利用率: {util.gpu}%")
print(f"内存利用率: {util.memory}%")
```

---

## 3.5 常见 GPU 规格

| GPU | VRAM | 带宽(大致) | 相对价格 | 最适合 |
|-----|------|-----------|----------------|----------|
| RTX 4090 | 24 GB | ~1 TB/s | 高 | 开发、小模型 |
| RTX 3090 | 24 GB | ~0.94 TB/s | 中 | 经济型开发 |
| A100 (40GB) | 40 GB | ~1.6-2.0 TB/s | 很高 | 生产环境、较大模型 |
| A100 (80GB) | 80 GB | ~2.0 TB/s | 极高 | 超大模型 |
| H100 | 80 GB | ~3.0-3.35 TB/s | 顶级 | 高端生产环境 |

**关键洞察**: 尽管 VRAM 相似,但数据中心卡的带宽通常显著高于消费级 GPU,对推理很重要!

**备注**: 价格随市场波动,且二手/租赁价格差异很大。本表仅用于相对量级比较。

---

## 3.6 性能瓶颈诊断

### 三种瓶颈

1. **内存受限**(推理中很常见)
   - GPU 利用率 < 80%
   - 内存使用率高
   - 解决方案:减少批次大小、使用量化

2. **计算受限**
   - GPU 利用率 ~100%
   - 内存使用率适中
   - 解决方案:更好的 GPU、更多 SM

3. **主机 CPU/IO 受限**
   - GPU 利用率低
   - CPU 使用率高
   - 解决方案:更好的 CPU、更快的数据加载

**注意**: 以上诊断是经验法则,更可靠的方式是使用 profiler(如 Nsight Systems/Compute)查看 SM/内存吞吐与 kernel 时间占比。

### 诊断流程图

```
GPU 利用率 > 80% 吗?
  ├─ 是 → 计算受限(需要更好的 GPU 或优化)
  └─ 否 → 内存使用率 > 80% 吗?
      ├─ 是 → 内存受限(减少批次大小、量化)
      └─ 否 → CPU/IO 受限(检查 CPU、磁盘、网络)
```

---

## 🚫 常见误解

### ❌ "VRAM 越多总是越好"

**实际情况**: 带宽对推理更重要。
- RTX 4090 (24 GB, ~1 TB/s) vs A100 (40 GB, ~1.6-2.0 TB/s)
- A100 往往更快,但提升幅度取决于模型、批次与量化
- 对于推理,你需要快速的内存访问,而不仅仅是大内存

### ❌ "更大的批次大小总是意味着更好的吞吐量"

**实际情况**: 取决于你的请求分布。
- 如果请求长度都相同:是的,更大的批次有帮助
- 如果请求差异很大:批处理可能会损害性能(填充开销)
- 对于聊天应用(长度不一),小批次通常更好

### ❌ "消费级 GPU (RTX) 只是数据中心 GPU 的慢版本"

**实际情况**: 不同的权衡。
- RTX:FP16/BF16 性能好,FP64 差
- A100:FP16/BF16 和 FP64 都优秀,可靠性更好
- 对于推理(主要是 FP16),RTX 可以有竞争力!

### ❌ "GPU 温度表示负载"

**实际情况**: 不一定。
- 高温 + 低利用率 = 可能是散热不良,不是高负载
- 使用 `nvidia-smi` 查看实际利用率指标

---

## ✅ 章节检查清单

阅读本章后,你应该能够:

- [ ] 使用教授/学生的类比解释 CPU 和 GPU 的区别
- [ ] 计算任何 LLM 的内存需求(模型 + KV 缓存)
- [ ] 使用 `nvidia-smi` 监控 GPU 性能
- [ ] 诊断你的系统是内存受限、计算受限还是 CPU 受限
- [ ] 为你的用例选择合适的 GPU
- [ ] 避免常见的 GPU 误解

---

## 📚 动手练习

**练习 3.1**: 计算内存需求

你想运行 Llama-3-70B,配置如下:
- 4 位量化(每参数 0.5 字节)
- 序列长度: 4096
- 批次大小: 4
- 模型有 80 层,隐藏维度 8192

**问题**:
1. 模型权重需要多少 VRAM?
2. 每个请求的 KV 缓存需要多少 VRAM?
3. 这能放入 A100 (80GB) 吗?如果可以,最大批次大小是多少?

**练习 3.2**: 监控实际推理

运行第 3 章代码示例:
```bash
cd code/chapter03
docker-compose up
```

使用 `nvidia-smi` 观察 GPU 并回答:
1. 是内存受限还是计算受限?
2. 峰值内存使用是多少?
3. 内存使用如何随批次大小扩展?

---

## ✅ 练习参考答案(带假设)

以下答案使用一致假设: KV 头数 = 注意力头数,KV 使用 FP16,忽略激活值与运行时开销(实际应预留)。

**练习 3.1**

1) 模型权重  
70B 参数 × 0.5 字节 = 35e9 字节 ≈ 32.6 GiB

2) 每个请求 KV 缓存  
头数 = 8192/128 = 64  
KV = 2 × 80 × 4096 × 1 × 64 × 128 × 2  
= 10,737,418,240 字节 ≈ 10 GiB / 请求

3) 是否能放入 A100 (80GB) + 最大批次  
批次 4 的 KV ≈ 40 GiB  
总计 ≈ 32.6 + 40 = 72.6 GiB → 理论可放入 80GB  
忽略开销的最大批次 ≈ floor((80 − 32.6) / 10) = 4  
考虑激活值与运行时开销,更稳妥的批次是 3

**GQA/MQA 变体(仅给缩放公式)**  
若 KV 头数 = `kv_heads`, 注意力头数 = `attn_heads`,则  
KV 缓存按比例缩放: `KV_GQA = KV_full × (kv_heads / attn_heads)`  
例如: 若 `kv_heads = 8`, `attn_heads = 64`,则  
每请求 KV ≈ 10 GiB × (8/64) = 1.25 GiB  
最大批次也按同样比例放大(忽略其它开销)

**Qwen3-8B 示例(基于官方配置,序列长度 32K,批次 1)**  
层数 = 36, 注意力头数 = 32, KV 头数 = 8, 头维度 = 128  
KV (FP16) = 2 × 36 × 32768 × 1 × 8 × 128 × 2  
= 4,831,838,208 字节 ≈ 4.5 GiB / 请求  
若批次为 4, KV ≈ 18 GiB  

**KV 精度影响(同一配置)**  
- FP16/BF16: 4.5 GiB / 请求  
- FP8/INT8: 约 2.25 GiB / 请求  
- INT4: 约 1.125 GiB / 请求

**组合示例(权重 INT4 + KV FP8)**  
权重: 70B × 0.5 字节 ≈ 32.6 GiB  
KV: 约 2.25 GiB / 请求 (32K,批次 1)  
合计: 约 34.9 GiB + 其它开销

## 🎯 总结

关键要点:
- GPU 擅长并行任务(如 LLM 的矩阵乘法)
- 内存带宽经常是瓶颈,但要结合模型与实现判断
- KV 缓存随序列长度和批次大小增长
- 使用 `nvidia-smi` 诊断瓶颈
- VRAM 越多 ≠ 总是更好;带宽很重要

**下一章**: 使用 Docker、CUDA 和 vLLM 设置环境。

---

**有问题?加入 [第 3 章 Discord 频道](https://discord.gg/TODO) 讨论!**
