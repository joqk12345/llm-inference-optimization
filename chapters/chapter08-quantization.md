# ç¬¬8ç« : é‡åŒ–æŠ€æœ¯

> **ğŸ’° æˆæœ¬å½±å“** (åŸºäºè¡Œä¸šæ•°æ®)
> - **æ˜¾å­˜èŠ‚çœ**: INT8 é‡åŒ–èŠ‚çœ 50% æ˜¾å­˜,INT4 èŠ‚çœ 75%
> - **æˆæœ¬é™ä½**: åŒæ ·æ¨¡å‹å¯åœ¨æ›´å°/æ›´ä¾¿å®œçš„ GPU ä¸Šè¿è¡Œ
> - **ç²¾åº¦æŸå¤±**: ç°ä»£é‡åŒ–æŠ€æœ¯ç²¾åº¦æŸå¤± <1%
> - **ç¡¬ä»¶æ•ˆç‡**: INT8 æ¨ç†é€Ÿåº¦æ¯” FP16 å¿« 2-3 å€
> - **æç«¯å‹ç¼©**: INT4 QAT å¯å°† ~1TB æ¨¡å‹å‹ç¼©åˆ°å• H200 (7å€å‹ç¼©) â­

## ç®€ä»‹

åœ¨å‰é¢çš„ç« èŠ‚ä¸­,æˆ‘ä»¬å­¦ä¹ äº† GPU åŸºç¡€ã€KV Cache ä¼˜åŒ–ã€è°ƒåº¦ç­–ç•¥ç­‰æŠ€æœ¯ã€‚ä½†è¿™äº›æŠ€æœ¯éƒ½æœ‰ä¸€ä¸ªå…±åŒçš„é™åˆ¶:**æ¨¡å‹å¤ªå¤§,æ˜¾å­˜ä¸å¤Ÿ**ã€‚Llama-2-70B éœ€è¦ 140GB æ˜¾å­˜ (FP16),è¿œè¶…å•å¼  A100 çš„å®¹é‡ (40GB æˆ– 80GB)ã€‚

**é‡åŒ– (Quantization)** æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„æ ¸å¿ƒæ­¦å™¨ã€‚é€šè¿‡é™ä½æ¨¡å‹æƒé‡çš„æ•°å€¼ç²¾åº¦,æˆ‘ä»¬å¯ä»¥:
- å°† 70B æ¨¡å‹ä» 140GB å‹ç¼©åˆ° 35GB (INT4)
- åœ¨å•å¼  GPU ä¸Šè¿è¡Œæ›´å¤§çš„æ¨¡å‹
- æå‡æ¨ç†é€Ÿåº¦ (INT8 æ¯” FP16 å¿« 2-3 å€)
- é™ä½ç¡¬ä»¶æˆæœ¬ (æ›´å°çš„ GPU æˆ–æ›´å°‘çš„ GPU)

ä½†é‡åŒ–ä¹Ÿå¸¦æ¥æŒ‘æˆ˜: ç²¾åº¦æŸå¤±ã€è®­ç»ƒæ¨ç†ä¸ä¸€è‡´ã€å®ç°å¤æ‚åº¦ç­‰ã€‚æœ¬ç« å°†æ·±å…¥è®²è§£é‡åŒ–çš„åŸç†ã€æ–¹æ³•å’Œå®æˆ˜æŠ€å·§,å¸®åŠ©ä½ å®‰å…¨åœ°åº”ç”¨é‡åŒ–æŠ€æœ¯ã€‚

**å­¦å®Œæœ¬ç« ,ä½ å°†èƒ½å¤Ÿé‡åŒ–ä»»æ„æ¨¡å‹,åœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹æœ€å¤§åŒ–æ€§èƒ½ã€‚**

---

## 8.1 é‡åŒ–åŸºç¡€

### 8.1.1 ä»€ä¹ˆæ˜¯é‡åŒ–

**å®šä¹‰**: å°†é«˜ç²¾åº¦çš„æ•°å€¼è¡¨ç¤ºè½¬æ¢ä¸ºä½ç²¾åº¦è¡¨ç¤º

```
FP32 (32ä½æµ®ç‚¹):
  ç¬¦å· (1 bit) + æŒ‡æ•° (8 bits) + å°¾æ•° (23 bits)
  è¡¨ç¤ºèŒƒå›´: Â±3.4Ã—10Â³â¸
  ç²¾åº¦: çº¦ 7 ä½åè¿›åˆ¶æ•°å­—

INT8 (8ä½æ•´æ•°):
  ç¬¦å· (1 bit) + æ•°å€¼ (7 bits)
  è¡¨ç¤ºèŒƒå›´: -128 åˆ° 127
  ç²¾åº¦: æ•´æ•°
```

**ç›´è§‚ç†è§£**:
```
FP32 æƒé‡: [0.23456789, -1.2345678, 0.00001234]
                â†“ é‡åŒ–
INT8 æƒé‡:  [0, -1, 0]  (æŸå¤±äº†ç²¾åº¦!)
```

**æ ¸å¿ƒé—®é¢˜**: å¦‚ä½•åœ¨é™ä½ç²¾åº¦çš„åŒæ—¶,ä¿æŒæ¨¡å‹æ€§èƒ½?

---

### 8.1.2 ä¸ºä»€ä¹ˆé‡åŒ–èƒ½èŠ‚çœæ˜¾å­˜

**è®¡ç®—æ¨¡å‹æ˜¾å­˜å ç”¨**:

```
æ¨¡å‹æ€»æ˜¾å­˜ = æ¨¡å‹æƒé‡ + KV Cache + æ¿€æ´»å€¼ + å¼€é”€

æ¨¡å‹æƒé‡ = å‚æ•°é‡ Ã— æ¯å‚æ•°å­—èŠ‚æ•°

Llama-2-7B (70äº¿å‚æ•°):
  FP32: 70B Ã— 4 bytes = 280 GB
  FP16: 70B Ã— 2 bytes = 140 GB
  INT8: 70B Ã— 1 byte  = 70 GB
  INT4: 70B Ã— 0.5 byte= 35 GB
```

**é‡åŒ–æ•ˆæœ**:
```
FP16 â†’ INT8:
  æ˜¾å­˜: 140GB â†’ 70GB (èŠ‚çœ 50%)
  é€Ÿåº¦: 1.0x â†’ 2.5x (INT8 è®¡ç®—æ›´å¿«)
  ç²¾åº¦: æŸå¤± <0.5%

FP16 â†’ INT4:
  æ˜¾å­˜: 140GB â†’ 35GB (èŠ‚çœ 75%)
  é€Ÿåº¦: 1.0x â†’ 3.0x
  ç²¾åº¦: æŸå¤± 1-2%
```

**å…³é”®ä¼˜åŠ¿**:
- âœ… æ˜¾å­˜å ç”¨å‡åŠæˆ–æ›´å¤š
- âœ… æ¨ç†é€Ÿåº¦æå‡
- âœ… å¯ä»¥åœ¨æ›´å°çš„ GPU ä¸Šè¿è¡Œ
- âœ… é™ä½ç¡¬ä»¶æˆæœ¬

---

### 8.1.3 ç²¾åº¦ vs æ€§èƒ½çš„æƒè¡¡

**æƒè¡¡æ›²çº¿**:
```
ç²¾åº¦
  â†‘
  â”‚     â•±
  â”‚    â•±  â† é‡åŒ–å¯¼è‡´ç²¾åº¦ä¸‹é™
  â”‚   â•±
  â”‚  â•±
  â”‚ â•±     â† æœ€ä½³å¹³è¡¡ç‚¹
  â”‚â•±_______
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ€§èƒ½ (æ˜¾å­˜ã€é€Ÿåº¦)
```

**ä¸åŒé‡åŒ–çš„æ•ˆæœ**:

| æ ¼å¼ | æ˜¾å­˜å ç”¨ | é€Ÿåº¦ | ç²¾åº¦æŸå¤± | é€‚ç”¨åœºæ™¯ |
|------|---------|------|---------|---------|
| **FP32** | 100% | 1x | 0% | è®­ç»ƒ |
| **FP16** | 50% | 1.5x | <0.1% | æ¨ç†æ ‡å‡† |
| **BF16** | 50% | 1.5x | <0.1% | æ¨ç†æ ‡å‡† |
| **INT8** | 25% | 2.5x | 0.5-1% | ç”Ÿäº§æ¨ç† |
| **INT4** | 12.5% | 3x | 1-3% | æé™å‹ç¼© |
| **FP8** | 25% | 2x | <0.5% | æœªæ¥æ–¹å‘ |
| **FP4** | 12.5% | 4x | 1-5% | ç ”ç©¶ä¸­ |

**é€‰æ‹©åŸåˆ™**:
```
è¿½æ±‚ç²¾åº¦: FP16/BF16
è¿½æ±‚æ€§ä»·æ¯”: INT8
è¿½æ±‚æé™å‹ç¼©: INT4 (QAT)
æœªæ¥æ–¹å‘: FP8/FP4
```

---

### 8.1.4 ä¸ºä»€ä¹ˆé‡åŒ–æœ‰æ•ˆ: æ¨¡å‹çš„å†—ä½™æ€§

**æ ¸å¿ƒæ´å¯Ÿ**: æ·±åº¦å­¦ä¹ æ¨¡å‹æœ‰å¤§é‡å†—ä½™,é‡åŒ–ä¸ä¼šç ´åå…³é”®ä¿¡æ¯

**ä¸ºä»€ä¹ˆå†—ä½™?**
1. **è¿‡å‚æ•°åŒ–**: æ¨¡å‹å‚æ•°è¿œè¶…éœ€è¦
2. **åˆ†å¸ƒå¼è¡¨ç¤º**: ä¿¡æ¯åˆ†æ•£åœ¨å¤šä¸ªå‚æ•°ä¸­
3. **é²æ£’æ€§**: å°çš„æ‰°åŠ¨ä¸å½±å“æ•´ä½“æ€§èƒ½

**å®éªŒè¯æ®**:
```
Berkeley ç ”ç©¶å›¢é˜Ÿ (2024):
  - éšæœºä¸¢å¼ƒ 50% çš„æƒé‡ â†’ ç²¾åº¦æŸå¤± <2%
  - é‡åŒ–åˆ° INT8 â†’ ç²¾åº¦æŸå¤± <1%
  - ç»“è®º: æ¨¡å‹æœ‰å¤§é‡å†—ä½™

NVIDIA (2023):
  - LLM å¯¹é‡åŒ–å®¹å¿åº¦é«˜
  - INT8 é‡åŒ–å‡ ä¹æ— æŸ
  - INT4 é‡åŒ–å¯æ§æŸå¤±
```

**ç›´è§‚ç†è§£**:
```
ç¥ç»ç½‘ç»œ: ä¸æ˜¯æ‰€æœ‰å‚æ•°éƒ½é‡è¦
  é‡è¦å‚æ•°: å†³å®šæ¨¡å‹æ ¸å¿ƒèƒ½åŠ› (ä¸èƒ½é‡åŒ–)
  å†—ä½™å‚æ•°: åªè´¡çŒ®å¾®å°å½±å“ (å¯ä»¥é‡åŒ–)

é‡åŒ–: ä¿ç•™é‡è¦ä¿¡æ¯,ä¸¢å¼ƒå†—ä½™ç»†èŠ‚
  ç±»æ¯”: å›¾åƒå‹ç¼© (JPEG ä¿ç•™è§†è§‰å…³é”®ä¿¡æ¯)
```

---

## 8.2 é‡åŒ–æ–¹æ³•åˆ†ç±»

### 8.2.1 PTQ (Post-Training Quantization)

**å®šä¹‰**: è®­ç»ƒåé‡åŒ–,æ— éœ€é‡æ–°è®­ç»ƒ

**æµç¨‹**:
```
1. è®­ç»ƒ FP32/FP16 æ¨¡å‹
2. æ”¶é›†æ ¡å‡†æ•°æ® (Calibration Dataset)
3. è®¡ç®—é‡åŒ–å‚æ•° (Scaleã€Zero Point)
4. é‡åŒ–æƒé‡
5. (å¯é€‰) å¾®è°ƒæ¢å¤ç²¾åº¦
```

**å¸¸è§æ–¹æ³•**:
- **GPTQ**: Gradient-based Post-Training Quantization
- **AWQ**: Activation-aware Quantization
- **bitsandbytes**: ç®€å•æ˜“ç”¨çš„ INT8 é‡åŒ–
- **SpQR**: æ··åˆç²¾åº¦é‡åŒ–

**ä¼˜ç‚¹**:
- âœ… å¿«é€Ÿ (å‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶)
- âœ… æ— éœ€å®Œæ•´è®­ç»ƒå‘¨æœŸ
- âœ… é€‚åˆå¿«é€Ÿéƒ¨ç½²

**ç¼ºç‚¹**:
- âŒ å¯èƒ½æœ‰ä¸€å®šç²¾åº¦æŸå¤±
- âŒ å¯¹æç«¯å€¼æ•æ„Ÿ
- âŒ éœ€è¦æ ¡å‡†æ•°æ®é›†

**é€‚ç”¨åœºæ™¯**:
```
âœ… å¿«é€ŸåŸå‹éªŒè¯
âœ… ä¸å…·å¤‡è®­ç»ƒèµ„æº
âœ… æ¨¡å‹å·²è®­ç»ƒå¥½,åªéœ€è¦éƒ¨ç½²
âŒ ç²¾åº¦è¦æ±‚æé«˜ (è€ƒè™‘ QAT)
```

---

### 8.2.2 QAT (Quantization-Aware Training) â­

**å®šä¹‰**: é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ,åœ¨è®­ç»ƒæ—¶æ¨¡æ‹Ÿé‡åŒ–

**æ ¸å¿ƒæ€æƒ³**:
```
è®­ç»ƒæ—¶:
  å‰å‘ä¼ æ’­: Fake Quantization (æ¨¡æ‹Ÿé‡åŒ–)
    â†’ FP32 æƒé‡ â†’ Fake Quant â†’ INT8 æ¨¡æ‹Ÿå€¼ â†’ è®¡ç®—
  åå‘ä¼ æ’­: STE (Straight-Through Estimator)
    â†’ æ¢¯åº¦è·³è¿‡é‡åŒ–æ­¥éª¤,ç›´æ¥ä¼ ç»™ FP32 æƒé‡

æ¨ç†æ—¶:
  å¯¼å‡º INT8 æƒé‡ â†’ ç›´æ¥ INT8 æ¨ç†
```

**Fake Quantization åŸç†**:
```python
def fake_quantize(x, scale, zero_point):
    """
    æ¨¡æ‹Ÿé‡åŒ–,ä½†ä¿æŒå¯å¾®åˆ†æ€§
    """
    # å‰å‘: æ¨¡æ‹Ÿé‡åŒ–
    x_quant = torch.round(x / scale) + zero_point
    x_quant = torch.clamp(x_quant, 0, 255)

    # åå‘: STE (ç›´é€šä¼°è®¡å™¨)
    # æ¢¯åº¦ç›´æ¥è·³è¿‡ round æ“ä½œ
    x_dequant = (x_quant - zero_point) * scale

    return x_dequant

# STE å®ç°
class STE(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        return x.round()  # å‰å‘: æ­£å¸¸é‡åŒ–

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output  # åå‘: ç›´æ¥ä¼ é€’æ¢¯åº¦
```

**ä¼˜ç‚¹**:
- âœ… ç²¾åº¦æŸå¤±æœ€å°
- âœ… Train-Infer ä¸€è‡´æ€§å¥½
- âœ… é€‚åˆ RL è®­ç»ƒå’Œé«˜ç²¾åº¦åœºæ™¯

**ç¼ºç‚¹**:
- âŒ éœ€è¦å®Œæ•´è®­ç»ƒå‘¨æœŸ
- âŒ è®¡ç®—æˆæœ¬é«˜
- âŒ å®ç°å¤æ‚åº¦é«˜

**é€‚ç”¨åœºæ™¯**:
```
âœ… éœ€è¦æœ€ä½³ç²¾åº¦
âœ… RL è®­ç»ƒ (éœ€è¦ train-infer ä¸€è‡´)
âœ… PTQ ç²¾åº¦æŸå¤±ä¸å¯æ¥å—
âœ… å¤§è§„æ¨¡æ¨¡å‹ (100B+ å‚æ•°)
âŒ åªéœ€è¦æ¨ç† (ç”¨ PTQ æ›´å¿«)
```

---

### 8.2.3 QLoRA vs Native Quantized Training vs QAT

| æ–¹æ³• | ç›®çš„ | é€‚ç”¨åœºæ™¯ | ä¼˜ç¼ºç‚¹ |
|------|------|---------|--------|
| **QLoRA** | é™ä½ LoRA å¾®è°ƒçš„è®­ç»ƒå†…å­˜ | å‚æ•°é«˜æ•ˆå¾®è°ƒ | âœ… èŠ‚çœè®­ç»ƒå†…å­˜<br>âŒ åªç”¨äºå¾®è°ƒ,ä¸ç”¨äºæ¨ç† |
| **Native Quantized Training** | ç«¯åˆ°ç«¯ä½ç²¾åº¦è®­ç»ƒ | ç ”ç©¶å’Œæ–°ç®—æ³• | âœ… æè‡´æ˜¾å­˜èŠ‚çœ<br>âŒ å®ç°æå¤æ‚<br>âŒ ç¨³å®šæ€§å·® |
| **QAT** | æ”¹å–„é‡åŒ–æ¨ç†ç²¾åº¦ | ç”Ÿäº§çº§é‡åŒ–éƒ¨ç½² | âœ… æœ€ä½³ç²¾åº¦<br>âœ… Train-Infer ä¸€è‡´<br>âŒ éœ€è¦å®Œæ•´è®­ç»ƒå‘¨æœŸ |

**å…³ç³»å›¾**:
```
è®­ç»ƒé˜¶æ®µ:
  Full Precision Training
    â†“
  LoRA Fine-tuning (QLoRA: é‡åŒ–ç‰ˆ)
    â†“
  QAT (è®­ç»ƒæ—¶æ¨¡æ‹Ÿé‡åŒ–)

æ¨ç†é˜¶æ®µ:
  PTQ (è®­ç»ƒåç›´æ¥é‡åŒ–)
  QAT (è®­ç»ƒå¯¼å‡ºçš„é‡åŒ–æ¨¡å‹)
```

---

### 8.2.4 é‡åŒ–æ–¹æ³•é€‰æ‹©å†³ç­–æ ‘

**å†³ç­–æµç¨‹**:

```
é—®é¢˜ 1: ä½ éœ€è¦è®­ç»ƒè¿˜æ˜¯åªéœ€æ¨ç†?

åªéœ€è¦æ¨ç†:
  â†’ é—®é¢˜ 2: å¯¹ç²¾åº¦è¦æ±‚å¤šé«˜?

  ç²¾åº¦è¦æ±‚ä¸é«˜ (å¯æ¥å— 1-2% æŸå¤±):
    â†’ PTQ (INT8/INT4)
    â†’ å¿«é€Ÿã€ç®€å•

  ç²¾åº¦è¦æ±‚é«˜ (æŸå¤± <0.5%):
    â†’ QAT (INT8)
    â†’ æˆ–ä¿æŒ FP16

éœ€è¦è®­ç»ƒ (RLã€å¾®è°ƒ):
  â†’ é—®é¢˜ 3: è®­ç»ƒèµ„æºå¦‚ä½•?

  èµ„æºæœ‰é™:
    â†’ QLoRA (é‡åŒ–ç‰ˆ LoRA)
    â†’ èŠ‚çœè®­ç»ƒå†…å­˜

  èµ„æºå……è¶³:
    â†’ QAT (INT8/INT4)
    â†’ æœ€ä½³ç²¾åº¦å’Œä¸€è‡´æ€§

  æè‡´å‹ç¼©:
    â†’ Native Quantized Training
    â†’ å®éªŒæ€§,é£é™©é«˜
```

**åœºæ™¯æ¨è**:
```
åœºæ™¯ 1: å¿«é€Ÿéƒ¨ç½² â†’ PTQ
åœºæ™¯ 2: ç²¾åº¦è¦æ±‚é«˜ â†’ QAT
åœºæ™¯ 3: éœ€è¦å¾®è°ƒ â†’ QLoRA æˆ– QAT
åœºæ™¯ 4: RL è®­ç»ƒ â†’ QAT (å¿…é¡»ä¿è¯ä¸€è‡´æ€§)
```

---

## 8.3 å¸¸ç”¨é‡åŒ–æ ¼å¼

### 8.3.1 FP32 (32ä½æµ®ç‚¹) - è®­ç»ƒæ ‡å‡†

**è¡¨ç¤º**:
```
1 bit  ç¬¦å·
8 bits æŒ‡æ•°
23 bits å°¾æ•°

èŒƒå›´: Â±3.4Ã—10Â³â¸
ç²¾åº¦: ~7 ä½åè¿›åˆ¶æ•°å­—
```

**ç‰¹ç‚¹**:
- âœ… ç²¾åº¦æœ€é«˜
- âœ… è®­ç»ƒç¨³å®š
- âŒ æ˜¾å­˜å ç”¨å¤§ (280GB for 70B)
- âŒ æ¨ç†é€Ÿåº¦æ…¢

**ç”¨é€”**: æ¨¡å‹è®­ç»ƒ

---

### 8.3.2 FP16/BF16 (16ä½æµ®ç‚¹) - æ¨ç†å¸¸ç”¨

**FP16 (åŠç²¾åº¦æµ®ç‚¹)**:
```
1 bit  ç¬¦å·
5 bits æŒ‡æ•°
10 bits å°¾æ•°

èŒƒå›´: Â±65504
ç²¾åº¦: ~3 ä½åè¿›åˆ¶æ•°å­—
```

**BF16 (Brain Float 16)**:
```
1 bit  ç¬¦å·
8 bits æŒ‡æ•° (ä¸ FP32 ç›¸åŒ)
7 bits å°¾æ•°

èŒƒå›´: Â±3.4Ã—10Â³â¸ (ä¸ FP32 ç›¸åŒ)
ç²¾åº¦: ~2 ä½åè¿›åˆ¶æ•°å­—
```

**å¯¹æ¯”**:
| æ ¼å¼ | èŒƒå›´ | ç²¾åº¦ | ç¨³å®šæ€§ | æ¨èåº¦ |
|------|------|------|--------|--------|
| **FP16** | å° | é«˜ | ä¸€èˆ¬ (å¯èƒ½ä¸‹æº¢) | â­â­â­ |
| **BF16** | å¤§ | ä¸­ | å¥½ (ä¸æ˜“ä¸‹æº¢) | â­â­â­â­â­ |

**æ¨è**: BF16 (èŒƒå›´ä¸ FP32 ç›¸åŒ,æ›´ç¨³å®š)

---

### 8.3.3 INT8 (8ä½æ•´æ•°) - ç»å…¸é‡åŒ–

**è¡¨ç¤º**:
```
æœ‰ç¬¦å· INT8:
  èŒƒå›´: -128 åˆ° 127
  ç²¾åº¦: æ•´æ•°

æ— ç¬¦å· UINT8:
  èŒƒå›´: 0 åˆ° 255
  ç²¾åº¦: æ•´æ•°
```

**é‡åŒ–å…¬å¼**:
```python
# Affine é‡åŒ–
Q = round(R / S) + Z
R = (Q - Z) * S

å…¶ä¸­:
  R: åŸå§‹å®æ•°
  Q: é‡åŒ–åçš„æ•´æ•°
  S: Scale (ç¼©æ”¾å› å­)
  Z: Zero Point (é›¶ç‚¹åç§»)
```

**ä¼˜ç‚¹**:
- âœ… æ˜¾å­˜å‡åŠ
- âœ… æ¨ç†é€Ÿåº¦æå‡ 2-3x
- âœ… ç²¾åº¦æŸå¤±å° (<1%)
- âœ… ç¡¬ä»¶æ”¯æŒå¥½ (Tensor Core)

**ç¼ºç‚¹**:
- âŒ éœ€è¦æ ¡å‡†æ•°æ®é›†
- âŒ æç«¯å€¼å¤„ç†

**æ¨èåº¦**: â­â­â­â­â­ (ç”Ÿäº§ç¯å¢ƒæ ‡å‡†)

---

### 8.3.4 INT4 (W4A16) â­

**è¡¨ç¤º**:
```
INT4 æƒé‡:
  èŒƒå›´: -8 åˆ° 7 (æœ‰ç¬¦å·)
  æˆ–: 0 åˆ° 15 (æ— ç¬¦å·)

FP16 æ¿€æ´»:
  ä¿æŒ 16 ä½æµ®ç‚¹
```

**ä¸ºä»€ä¹ˆ W4A16?**
```
æƒé‡é‡åŒ–åˆ° INT4:
  èŠ‚çœ 75% æ˜¾å­˜

æ¿€æ´»ä¿æŒ FP16:
  é¿å…ç²¾åº¦ç´¯ç§¯è¯¯å·®
  ç¡¬ä»¶å®ç°ç®€å•
```

**ä¼˜ç‚¹**:
- âœ… æ˜¾å­˜èŠ‚çœ 75%
- âœ… é€Ÿåº¦æå‡ 3x
- âœ… ç²¾åº¦æŸå¤±å¯æ§ (1-3%)
- âœ… å¹¿æ³›ç¡¬ä»¶æ”¯æŒ

**ç¼ºç‚¹**:
- âŒ éœ€è¦ QAT ä¿è¯ç²¾åº¦
- âŒ å®ç°å¤æ‚åº¦é«˜

**æ¨èåº¦**: â­â­â­â­ (æé™å‹ç¼©é¦–é€‰)

---

### 8.3.5 FP4 vs INT4

**FP4 (4ä½æµ®ç‚¹)**:
```
2 bit æŒ‡æ•°
2 bit å°¾æ•°

èŒƒå›´: Â±6
ç²¾åº¦: æä½
```

**å¯¹æ¯”**:

| ç»´åº¦ | INT4 | FP4 |
|------|------|-----|
| **è¡¨ç¤ºèŒƒå›´** | çª„ (-8 åˆ° 7) | æ›´å®½ (Â±6 æµ®ç‚¹) |
| **ç²¾åº¦** | æ•´æ•° | æµ®ç‚¹ |
| **ç¨³å®šæ€§** | é«˜ | ä½ |
| **æ€§èƒ½** | å¿« | ç†è®ºæ›´å¿« |
| **ç¡¬ä»¶æ”¯æŒ** | å¹¿æ³› | éœ€è¦ Blackwell |
| **ç”Ÿæ€** | æˆç†Ÿ | å‘å±•ä¸­ |
| **æ¨èåº¦** | â­â­â­â­ | â­â­â­ (æœªæ¥) |

**é€‰æ‹©å»ºè®®**:
- **å½“å‰**: INT4 (ç”Ÿæ€æˆç†Ÿ,ç¨³å®šå¯é )
- **æœªæ¥**: FP4 (ç†è®ºæ€§èƒ½æ›´é«˜,Blackwell åŸç”Ÿæ”¯æŒ)

---

### 8.3.6 FP8 / NVFP4: æœªæ¥æ–¹å‘

**FP8 (8ä½æµ®ç‚¹)**:
```
E4M3 (4 bit æŒ‡æ•°, 3 bit å°¾æ•°):
  ç”¨äºè®­ç»ƒ
  èŒƒå›´: Â±448
  ç²¾åº¦: ç±»ä¼¼ FP16

E5M2 (5 bit æŒ‡æ•°, 2 bit å°¾æ•°):
  ç”¨äºæ¨ç†
  èŒƒå›´: Â±57344
  ç²¾åº¦: ä½äº E4M3
```

**NVFP4 (NVIDIA FP4)**:
```
Blackwell åŸç”Ÿæ”¯æŒ
æ›´ä¼˜çš„ç¡¬ä»¶åŠ é€Ÿ
ä¸ Tensor Core æ·±åº¦é›†æˆ
```

**ç¡¬ä»¶æ”¯æŒ**:
- **H100/H200**: FP8 åŸç”Ÿæ”¯æŒ
- **Blackwell**: FP4/FP8 åŸç”Ÿæ”¯æŒ
- **Ampere**: ä¸æ”¯æŒ (éœ€è¦è½¯ä»¶æ¨¡æ‹Ÿ)

**æ€§èƒ½æ½œåŠ›**:
```
FP8 vs FP16:
  æ˜¾å­˜: 50%
  é€Ÿåº¦: 2x
  ç²¾åº¦: <0.5% æŸå¤±

FP4 vs INT4:
  ç†è®ºé€Ÿåº¦: 1.5x
  ç²¾åº¦: å¯èƒ½æ›´å¥½
  ä½†: ç”Ÿæ€ä¸æˆç†Ÿ
```

---

### 8.3.7 AWQ / GPTQ: æµè¡Œçš„ INT4 æ ¼å¼

**AWQ (Activation-aware Quantization)**:
```
åŸç†:
  åŸºäºæ¿€æ´»å€¼çš„é‡è¦æ€§æ¥é‡åŒ–æƒé‡
  é‡è¦çš„æƒé‡ä¿æŒé«˜ç²¾åº¦

æ­¥éª¤:
  1. æ”¶é›†æ¿€æ´»å€¼ç»Ÿè®¡
  2. è®¡ç®—æƒé‡é‡è¦æ€§
  3. éå‡åŒ€é‡åŒ– (é‡è¦æƒé‡ç²¾åº¦é«˜)

ä¼˜ç‚¹:
  âœ… ç²¾åº¦ä¼˜äº GPTQ
  âœ… é€Ÿåº¦å¿«

ç¼ºç‚¹:
  âŒ éœ€è¦æ ¡å‡†æ•°æ®
  âŒ å®ç°å¤æ‚
```

**GPTQ (Gradient-based Post-Training Quantization)**:
```
åŸç†:
  åŸºäºæ¢¯åº¦çš„äºŒé˜¶ä¿¡æ¯
  è¿­ä»£é‡åŒ–æƒé‡

æ­¥éª¤:
  1. è®¡ç®—æµ·æ£®çŸ©é˜µè¿‘ä¼¼
  2. è¿­ä»£é‡åŒ–æƒé‡
  3. æœ€å°åŒ–é‡åŒ–è¯¯å·®

ä¼˜ç‚¹:
  âœ… ä¸éœ€è¦æ ¡å‡†æ•°æ®
  âœ… ç²¾åº¦å¥½
  âœ… å¼€æºå·¥å…·æˆç†Ÿ

ç¼ºç‚¹:
  âŒ é‡åŒ–é€Ÿåº¦æ…¢
  âŒ å†…å­˜å ç”¨é«˜
```

**å¯¹æ¯”**:
| ç‰¹æ€§ | AWQ | GPTQ |
|------|-----|------|
| **ç²¾åº¦** | æ›´å¥½ | å¥½ |
| **é€Ÿåº¦** | å¿« | æ…¢ |
| **æ ¡å‡†æ•°æ®** | éœ€è¦ | ä¸éœ€è¦ |
| **å·¥å…·æ”¯æŒ** | vLLM, AutoGPTQ | AutoGPTQ, llama.cpp |
| **æ¨èåº¦** | â­â­â­â­â­ | â­â­â­â­ |

**æ¨è**:
- ç”Ÿäº§ç¯å¢ƒ: AWQ (æ›´å¿«ã€ç²¾åº¦æ›´å¥½)
- ç ”ç©¶/ç¦»çº¿: GPTQ (ä¸éœ€è¦æ ¡å‡†æ•°æ®)

---

## 8.4 æµè¡Œçš„é‡åŒ–æ¡†æ¶

### 8.4.1 vLLM é‡åŒ–æ”¯æŒ

**æ”¯æŒçš„æ ¼å¼**:
- âœ… AWQ (æ¨è)
- âœ… GPTQ
- âœ… bitsandbytes (INT8)
- âœ… FP8 (å®éªŒæ€§)

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from vllm import LLM, SamplingParams

# AWQ é‡åŒ–æ¨¡å‹
llm = LLM(
    model="TheBloke/Llama-2-7b-AWQ",
    quantization="awq",
    max_model_len=4096,
)

# GPTQ é‡åŒ–æ¨¡å‹
llm = LLM(
    model="TheBloke/Llama-2-7b-GPTQ",
    quantization="gptq",
    max_model_len=4096,
)

# æ¨ç†
prompts = ["Hello, my name is", "The future of AI is"]
sampling_params = SamplingParams(temperature=0.8)
outputs = llm.generate(prompts, sampling_params)
```

**KV Cache é‡åŒ–**:
```python
llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    quantization="awq",
    kv_cache_dtype="int8",  # KV Cache é‡åŒ–åˆ° INT8
)
```

**PagedAttention + é‡åŒ–**:
```
ä¼˜åŠ¿:
  âœ… å†…å­˜åˆ©ç”¨ç‡é«˜ (PagedAttention)
  âœ… æ˜¾å­˜å ç”¨ä½ (é‡åŒ–)
  âœ… ä¸¤è€…ååŒ,æ•ˆæœå åŠ 
```

---

### 8.4.2 SGLang INT4 æ¨ç† â­

**Marlin å†…æ ¸æ”¯æŒ**:
```
Marlin: ä¸“ä¸º INT4 è®¾è®¡çš„é«˜æ•ˆæ¨ç†å†…æ ¸
  - Bit packing: 8 ä¸ª INT4 å€¼æ‰“åŒ…åˆ° 1 ä¸ª INT32
  - é«˜æ•ˆè§£åŒ…: ä½è¿ç®— (>> 4 å’Œ & 0xF)
  - è®¡ç®—å’Œ IO é‡å : è§£åŒ…è¿‘é›¶å¼€é”€
  - MoE ç®—å­æ·±åº¦èåˆ
```

**W4A16 é«˜æ•ˆæ¨ç†**:
```python
# å¯åŠ¨ SGLang INT4 æ¨ç†
python -m sglang.launch_server \
  --model-path /path/to/llama-2-7b-gptq \
  --quantization marlin \  # ä½¿ç”¨ Marlin å†…æ ¸
  --context-length 4096 \
  --tp 1 \  # Tensor parallelism
  --host 0.0.0.0 \
  --port 8000
```

**Bit Packing åŸç†**:
```python
def pack_int4(values):
    """
    å°† 8 ä¸ª INT4 å€¼æ‰“åŒ…åˆ° 1 ä¸ª INT32

    è¾“å…¥: [v0, v1, v2, v3, v4, v5, v6, v7]
           æ¯ä¸ª vi âˆˆ [-8, 7]

    è¾“å‡º: 1 ä¸ª INT32
    """
    packed = 0
    for i, v in enumerate(values):
        packed |= (v & 0xF) << (4 * i)
    return packed

def unpack_int4(packed):
    """
    ä» 1 ä¸ª INT32 è§£åŒ… 8 ä¸ª INT4 å€¼
    """
    values = []
    for i in range(8):
        v = (packed >> (4 * i)) & 0xF
        # è½¬æ¢ä¸ºæœ‰ç¬¦å· INT4
        if v >= 8:
            v -= 16
        values.append(v)
    return values
```

**MoE ç®—å­æ·±åº¦èåˆ**:
```python
# åŠ¨æ€è°ƒæ•´ MoE block size
def dynamic_moe_align_block_size(block_size):
    """
    æ ¹æ® GPU æ¶æ„åŠ¨æ€è°ƒæ•´ MoE block size
    - ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
    - å‡å°‘ kernel å¯åŠ¨æ¬¡æ•°
    - Gating éƒ¨åˆ†èåˆä¸ºå•ä¸€å†…æ ¸
    """
    if gpu_arch == "H100":
        return 64
    elif gpu_arch == "A100":
        return 128
    else:
        return 32
```

**æ€§èƒ½ Benchmark**:
```
Llama-2-7B INT4 vs FP16:
  æ˜¾å­˜: 35GB vs 140GB (4x èŠ‚çœ)
  é€Ÿåº¦: 120 tokens/s vs 40 tokens/s (3x æå‡)
  ç²¾åº¦: æŸå¤± 1.2%
```

---

### 8.4.3 NVIDIA Model Optimizer â­

**QAT è®­ç»ƒæ”¯æŒ**:
```python
import torch
import torch.nn as nn
from modelopt.torch import quantization as mtq

# å®šä¹‰æ¨¡å‹
model = MyModel()

# é…ç½®é‡åŒ–
config = mtq.GPTQConfig(
    weights="int4",
    activations="int8",
)

# é‡åŒ–æ¨¡å‹
mtq.replace_quantizers(model, config)

# è®­ç»ƒ (æ¨¡æ‹Ÿé‡åŒ–)
optimizer = torch.optim.AdamW(model.parameters())
for batch in dataloader:
    loss = model(batch)
    loss.backward()
    optimizer.step()
```

**Megatron-LM é›†æˆ**:
```
å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒ:
  - Tensor Parallelism
  - Pipeline Parallelism
  - é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
  - æ··åˆç²¾åº¦ (FP8 + INT4)
```

**MXFP4 / NVFP4 æ ¼å¼æ”¯æŒ**:
```python
# NVIDIA åŸç”Ÿ FP4 é‡åŒ–
from modelopt.torch.quantization import NVFP4Quantizer

quantizer = NVFP4Quantizer()
model = quantizer.quantize(model)
```

**Fake Quantization å®ç°**:
```python
class FakeQuantize(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, scale, zero_point, qmin, qmax):
        # å‰å‘: é‡åŒ– + åé‡åŒ–
        x_quant = x / scale + zero_point
        x_quant = torch.clamp(x_quant, qmin, qmax)
        x_quant = torch.round(x_quant)
        x_dequant = (x_quant - zero_point) * scale
        return x_dequant

    @staticmethod
    def backward(ctx, grad_output):
        # STE: æ¢¯åº¦ç›´æ¥ä¼ é€’
        return grad_output, None, None, None
```

---

### 8.4.4 AutoGPTQ / llama.cpp

**AutoGPTQ**:
```python
from auto_gptq import AutoGPTQForCausalLM
from transformers import AutoTokenizer

# åŠ è½½é‡åŒ–æ¨¡å‹
model = AutoGPTQForCausalLM.from_quantized(
    "TheBloke/Llama-2-7b-GPTQ",
    use_safetensors=True,
)
tokenizer = AutoTokenizer.from_pretrained("TheBloke/Llama-2-7b-GPTQ")

# æ¨ç†
input_ids = tokenizer("Hello, world!", return_tensors="pt").input_ids
output = model.generate(input_ids, max_new_tokens=100)
print(tokenizer.decode(output[0]))
```

**llama.cpp (CPU æ¨ç†)**:
```bash
# é‡åŒ–æ¨¡å‹
./llama-cli \
  --model /path/to/llama-2-7b.gguf \
  --prompt "Hello, world!" \
  --n-predict 100

# GGUF æ ¼å¼æ”¯æŒ:
# - Q2_K: 2-bit é‡åŒ–
# - Q3_K: 3-bit é‡åŒ–
# - Q4_K: 4-bit é‡åŒ– (æœ€å¸¸ç”¨)
# - Q5_K: 5-bit é‡åŒ–
# - Q8_0: 8-bit é‡åŒ–
```

**å¯¹æ¯”**:
| å·¥å…· | GPU æ¨ç† | CPU æ¨ç† | é‡åŒ–æ ¼å¼ | æ˜“ç”¨æ€§ |
|------|---------|---------|---------|--------|
| **vLLM** | âœ… | âŒ | AWQ, GPTQ | â­â­â­â­â­ |
| **SGLang** | âœ… | âŒ | GPTQ (Marlin) | â­â­â­â­ |
| **AutoGPTQ** | âœ… | âŒ | GPTQ | â­â­â­ |
| **llama.cpp** | âŒ | âœ… | GGUF | â­â­â­â­ |

---

## 8.5 KV Cache é‡åŒ–

### 8.5.1 ä¸ºä»€ä¹ˆé‡åŒ– KV Cache

**KV Cache å æ˜¾å­˜çš„ 50%+**:

```
Llama-2-7B (åºåˆ—é•¿åº¦ 4096):
  æ¨¡å‹æƒé‡: 13 GB (FP16)
  KV Cache: 4 GB (FP16)
  æ¿€æ´»å€¼: 2 GB
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  æ€»è®¡: 19 GB

KV Cache å æ¯”: 4/19 = 21%

Llama-2-7B (åºåˆ—é•¿åº¦ 32768):
  æ¨¡å‹æƒé‡: 13 GB (FP16)
  KV Cache: 32 GB (FP16)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  æ€»è®¡: 45 GB (è¶…è¿‡ A100 40GB!)

KV Cache å æ¯”: 32/45 = 71%
```

**é•¿ä¸Šä¸‹æ–‡åœºæ™¯å°¤å…¶é‡è¦**:
```
åºåˆ—é•¿åº¦è¶Šé•¿,KV Cache è¶Šå¤§:
  4K tokens:   4 GB
  8K tokens:   8 GB
  16K tokens:  16 GB
  32K tokens:  32 GB
  64K tokens:  64 GB
```

---

### 8.5.2 KV Cache é‡åŒ–æ–¹æ³•

**INT8 KV Cache**:
```python
from vllm import LLM

llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    kv_cache_dtype="int8",  # KV Cache é‡åŒ–åˆ° INT8
)

# æ˜¾å­˜èŠ‚çœ:
# FP16: 4 GB
# INT8: 2 GB (èŠ‚çœ 50%)
```

**åŠ¨æ€é‡åŒ– vs é™æ€é‡åŒ–**:
```python
# é™æ€é‡åŒ– (æ¨è):
kv_cache_dtype="int8"
# é¢„å…ˆè®¡ç®—å¥½ scale å’Œ zero point
# é€Ÿåº¦å¿«,ç²¾åº¦å¥½

# åŠ¨æ€é‡åŒ–:
kv_cache_dtype="dynamic_int8"
# æ¯æ¬¡è®¡ç®—æ—¶åŠ¨æ€é‡åŒ–
# æ›´çµæ´»,ä½†ç•¥æ…¢
```

**Per-token é‡åŒ–**:
```python
# æ¯ä¸ª token ç‹¬ç«‹é‡åŒ–
# ç²¾åº¦æ›´é«˜,ä½†å¼€é”€æ›´å¤§

llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    kv_cache_dtype="int8_per_token",
)
```

---

### 8.5.3 ç²¾åº¦ä¸é€Ÿåº¦å¹³è¡¡

**ç²¾åº¦æŸå¤±è¯„ä¼°**:
```python
import torch
from vllm import LLM

# FP16 åŸºçº¿
llm_fp16 = LLM(model="meta-llama/Llama-2-7b-hf")
output_fp16 = llm_fp16.generate(prompts)

# INT8 KV Cache
llm_int8 = LLM(
    model="meta-llama/Llama-2-7b-hf",
    kv_cache_dtype="int8"
)
output_int8 = llm_int8.generate(prompts)

# è®¡ç®—ç›¸ä¼¼åº¦
from sklearn.metrics import accuracy_score
similarity = accuracy_score(
    tokenize(output_fp16),
    tokenize(output_int8)
)
print(f"Similarity: {similarity:.4f}")  # > 0.98
```

**æ€§èƒ½æå‡**:
```
æ˜¾å­˜:
  FP16: 19 GB
  INT8: 13 GB (èŠ‚çœ 6GB)

ååé‡:
  FP16: 40 requests/s
  INT8: 45 requests/s (ç•¥å¿«)
```

**ç”Ÿäº§ç¯å¢ƒæ³¨æ„äº‹é¡¹**:
```
âœ… æ¨è:
  - é•¿åºåˆ— (>8K tokens)
  - é«˜å¹¶å‘åœºæ™¯
  - æ˜¾å­˜ç´§å¼ 

âš ï¸ è°¨æ…:
  - çŸ­åºåˆ— (<2K tokens)
  - ç²¾åº¦æ•æ„Ÿä»»åŠ¡
  - éœ€è¦æè‡´æ€§èƒ½

âŒ ä¸æ¨è:
  - åºåˆ—é•¿åº¦ <1K (èŠ‚çœæœ‰é™)
  - ç²¾åº¦è¦æ±‚æé«˜
```

---

## 8.6 å®æˆ˜: é‡åŒ–éƒ¨ç½²

### 8.6.1 ä½¿ç”¨ vLLM åŠ è½½é‡åŒ–æ¨¡å‹

**AWQ/GPTQ æ¨¡å‹åŠ è½½**:
```python
from vllm import LLM, SamplingParams

# AWQ é‡åŒ–
llm_awq = LLM(
    model="TheBloke/Llama-2-7b-AWQ",
    quantization="awq",
    max_model_len=4096,
    gpu_memory_utilization=0.9,
)

# GPTQ é‡åŒ–
llm_gptq = LLM(
    model="TheBloke/Llama-2-7b-GPTQ",
    quantization="gptq",
    max_model_len=4096,
)

# ç”Ÿæˆ
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=100,
)

prompts = ["Write a story about AI", "Explain quantum computing"]
outputs = llm_awk.generate(prompts, sampling_params)
```

**æ€§èƒ½å¯¹æ¯”æµ‹è¯•**:
```python
import time

def benchmark(llm, prompts, num_iterations=100):
    latencies = []
    throughputs = []

    for i in range(num_iterations):
        start = time.time()
        outputs = llm.generate(prompts)
        end = time.time()

        latencies.append(end - start)
        throughputs.append(len(prompts) / (end - start))

    avg_latency = sum(latencies) / len(latencies)
    avg_throughput = sum(throughputs) / len(throughputs)

    return {
        "avg_latency": avg_latency,
        "p95_latency": sorted(latencies)[int(len(latencies) * 0.95)],
        "avg_throughput": avg_throughput,
    }

# å¯¹æ¯”
fp16_stats = benchmark(llm_fp16, prompts)
awq_stats = benchmark(llm_awq, prompts)

print(f"FP16:  {fp16_stats}")
print(f"AWQ:   {awq_stats}")
print(f"Speedup: {awq_stats['avg_throughput'] / fp16_stats['avg_throughput']:.2f}x")
```

**ç²¾åº¦æŸå¤±è¯„ä¼°**:
```python
from datasets import load_dataset
from evaluate import load

# åŠ è½½è¯„ä¼°æ•°æ®é›†
dataset = load_dataset("truthfulqa", "validation")
metric = load("truthfulness")

# FP16 åŸºçº¿
outputs_fp16 = llm_fp16.generate(dataset["question"][:100])
score_fp16 = metric.compute(
    references=dataset["correct_answer"][:100],
    predictions=outputs_fp16
)

# AWQ é‡åŒ–
outputs_awq = llm_awq.generate(dataset["question"][:100])
score_awq = metric.compute(
    references=dataset["correct_answer"][:100],
    predictions=outputs_awq
)

print(f"FP16 Accuracy: {score_fp16['accuracy']:.4f}")
print(f"AWQ  Accuracy:  {score_awq['accuracy']:.4f}")
print(f"Accuracy Drop: {score_fp16['accuracy'] - score_awq['accuracy']:.4f}")
```

---

### 8.6.2 ä½¿ç”¨ SGLang éƒ¨ç½² INT4 æ¨¡å‹ â­

**W4A16 æ¨ç†é…ç½®**:
```bash
# å®‰è£… SGLang
pip install "sglang[all]"

# å¯åŠ¨ INT4 æ¨ç†æœåŠ¡
python -m sglang.launch_server \
  --model-path TheBloke/Llama-2-7b-GPTQ \
  --quantization marlin \        # ä½¿ç”¨ Marlin å†…æ ¸
  --context-length 4096 \
  --tp 1 \                       # Tensor parallelism
  --host 0.0.0.0 \
  --port 8000

# æµ‹è¯•
curl http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hello, world!",
    "sampling_params": {
      "temperature": 0.8,
      "max_new_tokens": 100
    }
  }'
```

**Marlin å†…æ ¸å¯ç”¨**:
```python
# SGLang è‡ªåŠ¨æ£€æµ‹å¹¶å¯ç”¨ Marlin å†…æ ¸
# å¦‚æœæ£€æµ‹åˆ° GPTQ æ ¼å¼çš„ INT4 æƒé‡,è‡ªåŠ¨ä½¿ç”¨ Marlin

# éªŒè¯æ˜¯å¦ä½¿ç”¨ Marlin
import sglang as sgl

# æŸ¥çœ‹å†…æ ¸ä¿¡æ¯
print(sgl.kernels.get_active_kernel())
# è¾“å‡º: "marlin_int4" âœ…
```

**æ€§èƒ½ Benchmark**:
```python
import time
import requests

def benchmark_sglang(num_requests=1000):
    latencies = []

    for i in range(num_requests):
        start = time.time()
        response = requests.post(
            "http://localhost:8000/generate",
            json={
                "text": "Write a short story about AI.",
                "sampling_params": {
                    "temperature": 0.8,
                    "max_new_tokens": 100,
                }
            }
        )
        end = time.time()

        latencies.append(end - start)

    avg_latency = sum(latencies) / len(latencies)
    p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]
    throughput = num_requests / sum(latencies)

    return {
        "avg_latency": avg_latency,
        "p95_latency": p95_latency,
        "throughput": throughput,
    }

stats = benchmark_sglang()
print(f"Average Latency: {stats['avg_latency']:.3f}s")
print(f"P95 Latency: {stats['p95_latency']:.3f}s")
print(f"Throughput: {stats['throughput']:.2f} req/s")
```

---

### 8.6.3 ç”Ÿäº§ç¯å¢ƒæ³¨æ„äº‹é¡¹

**æ¨¡å‹æ ¼å¼é€‰æ‹©**:
```
ç”Ÿäº§ç¯å¢ƒ (ç¨³å®šæ€§ä¼˜å…ˆ):
  â†’ AWQ (ç²¾åº¦æ›´å¥½,é€Ÿåº¦å¿«)

å®éªŒ/ç ”ç©¶:
  â†’ GPTQ (å¼€æºå·¥å…·æˆç†Ÿ)

æè‡´å‹ç¼©:
  â†’ INT4 QAT (æœ€ä½³ç²¾åº¦)
```

**ç¡¬ä»¶è¦æ±‚**:
```
INT8:
  - A100/A100+/H100/H200
  - RTX 3090/4090
  - æ”¯æŒ INT8 Tensor Core

INT4:
  - éœ€è¦ä¸“ç”¨å†…æ ¸ (Marlin)
  - A100/A100+ (æ¨è)
  - RTX 4090 (å®éªŒæ€§)

FP8:
  - H100/H200 (åŸç”Ÿæ”¯æŒ)
  - A100 (è½¯ä»¶æ¨¡æ‹Ÿ,è¾ƒæ…¢)
```

**ç›‘æ§æŒ‡æ ‡**:
```python
# å…³é”®æŒ‡æ ‡
metrics = {
    # æ˜¾å­˜
    "memory_used": get_gpu_memory_used(),
    "memory_fragmentation": get_fragmentation(),

    # æ€§èƒ½
    "throughput": get_throughput(),
    "p50_latency": get_p50_latency(),
    "p95_latency": get_p95_latency(),
    "p99_latency": get_p99_latency(),

    # ç²¾åº¦
    "perplexity": get_perplexity(),
    "accuracy": get_accuracy(),

    # é‡åŒ–ç‰¹æœ‰
    "quantization_error": get_quant_error(),
}

# å‘Šè­¦é˜ˆå€¼
if metrics["memory_used"] > 0.95 * total_memory:
    print("âš ï¸  æ˜¾å­˜æ¥è¿‘ä¸Šé™,è€ƒè™‘é™ä½ batch size")

if metrics["quantization_error"] > 0.05:
    print("âš ï¸  é‡åŒ–è¯¯å·®è¿‡å¤§,è€ƒè™‘ QAT")

if metrics["p95_latency"] > sla_target:
    print("âš ï¸  P95 å»¶è¿Ÿè¶…æ ‡,è€ƒè™‘ä¼˜åŒ–")
```

---

## 8.7 é‡åŒ–è¿›é˜¶: INT4 QAT å®æˆ˜ âš ï¸ SGLang å›¢é˜ŸéªŒè¯

> **ğŸ’¡ æ¡ˆä¾‹æ¥æº**: SGLang RL Team, InfiXAI Team, Ant Group (2026-01-26)
>
> **æ ¸å¿ƒæˆæœ**: å°† ~1TB è§„æ¨¡çš„æ¨¡å‹å‹ç¼©åˆ°å•å¼  H200 (141GB),æ¶ˆé™¤è·¨èŠ‚ç‚¹é€šä¿¡ç“¶é¢ˆ,æ˜¾è‘—æå‡ rollout æ•ˆç‡

### 8.7.1 ä»€ä¹ˆæ˜¯ QAT

**Fake Quantization åŸç†**:
```python
class FakeInt4QuantizationSTE(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, scale):
        # å‰å‘: æ¨¡æ‹Ÿ INT4 é‡åŒ–
        # 1. å½’ä¸€åŒ–
        x_norm = x / scale

        # 2. é‡åŒ–åˆ° [-7, 7] (INT4 èŒƒå›´)
        x_quant = torch.clamp(torch.round(x_norm), -7, 7)

        # 3. åé‡åŒ–
        x_dequant = x_quant * scale

        return x_dequant

    @staticmethod
    def backward(ctx, grad_output):
        # STE: æ¢¯åº¦ç›´æ¥ä¼ é€’ (è·³è¿‡ round æ“ä½œ)
        return grad_output, None
```

**STE (Straight-Through Estimator) åŸç†**:
```
é—®é¢˜: round æ“ä½œä¸å¯å¯¼
  y = round(x)  # dy/dx = 0 (é™¤äº† 0 ç‚¹)

STE è§£å†³æ–¹æ¡ˆ:
  å‰å‘: y = round(x)
  åå‘: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y (ç›´æ¥ä¼ é€’æ¢¯åº¦)

ç›´è§‰:
  æ¢¯åº¦çš„æœŸæœ›æ˜¯æ­£ç¡®çš„
  è™½ç„¶å•ä¸ªæ ·æœ¬çš„æ¢¯åº¦æœ‰è¯¯å·®
  ä½†å¤§é‡æ ·æœ¬çš„å¹³å‡æ¢¯åº¦å‡†ç¡®
```

**Train-Infer ä¸€è‡´æ€§çš„é‡è¦æ€§**:
```
è®­ç»ƒæ—¶: Fake Quantization
  â†’ æ¨¡å‹"çœ‹åˆ°"é‡åŒ–çš„å™ªå£°
  â†’ å­¦ä¹ é€‚åº”è¿™ç§å™ªå£°

æ¨ç†æ—¶: True Quantization
  â†’ å®é™… INT4 æƒé‡
  â†’ ä¸è®­ç»ƒæ—¶ä¸€è‡´

å¦‚æœä¸ä¸€è‡´:
  è®­ç»ƒæ—¶ FP16,æ¨ç†æ—¶ INT4
  â†’ æ¨¡å‹æ²¡è§è¿‡é‡åŒ–å™ªå£°
  â†’ æ€§èƒ½å´©æºƒ
```

**æ¶ˆèå®éªŒ: QAT vs PTQ çš„ç²¾åº¦å·®å¼‚**:
```
PTQ (è®­ç»ƒåé‡åŒ–):
  - Llama-2-70B INT4
  - ç²¾åº¦æŸå¤±: 3-5%
  - PPL ä¸Šå‡: 15-20%

QAT (é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ):
  - Llama-2-70B INT4
  - ç²¾åº¦æŸå¤±: <1%
  - PPL ä¸Šå‡: <5%

ç»“è®º: QAT æ˜¾è‘—ä¼˜äº PTQ
```

---

### 8.7.2 INT4 QAT å®Œæ•´ Pipeline

**Stage 1: QAT è®­ç»ƒ (æ¨¡æ‹Ÿé‡åŒ–)**:
```python
class QATModel(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.quantizer = FakeInt4QuantizationSTE()
        self.scales = {}  # æ¯å±‚çš„ scale

    def forward(self, x):
        # 1. æ™®é€š FP16 å‰å‘ä¼ æ’­
        x = self.model.embed_tokens(x)

        # 2. æ¯å±‚é‡åŒ–
        for layer_name, layer in self.model.layers.items():
            # è®¡ç®—é‡åŒ– scale (per-group max absolute value)
            weight = layer.weight
            scale = weight.abs().max() / 7
            self.scales[layer_name] = scale

            # Fake quantization
            weight_quant = self.quantizer(weight, scale)

            # å‰å‘ä¼ æ’­ (ä½¿ç”¨é‡åŒ–åçš„æƒé‡)
            x = layer(x, weight=weight_quant)

        return x

    def backward(self, loss):
        # åå‘ä¼ æ’­: STE è‡ªåŠ¨ä¼ é€’æ¢¯åº¦
        loss.backward()
```

**Stage 2: æƒé‡è½¬æ¢ (çœŸé‡åŒ–)**:
```python
def convert_to_int4(model_fp16, scales):
    """
    å°† FP16 æƒé‡è½¬æ¢ä¸º INT4

    Args:
        model_fp16: BF16/FP16 æƒé‡ (è®­ç»ƒå)
        scales: æ¯å±‚çš„é‡åŒ– scale

    Returns:
        model_int4: INT4 æƒé‡
    """
    model_int4 = {}

    for layer_name, layer in model_fp16.layers.items():
        weight = layer.weight
        scale = scales[layer_name]

        # çœŸé‡åŒ–
        weight_quant = torch.clamp(torch.round(weight / scale), -7, 7)
        weight_quant = weight_quant.char()  # è½¬æ¢ä¸º INT8 (INT4 æ‰“åŒ…)

        # è½¬æ¢ä¸º Marlin æ ¼å¼
        weight_marlin = marlin_pack(weight_quant)

        model_int4[layer_name] = {
            'weight': weight_marlin,
            'scale': scale,
        }

    return model_int4
```

**Stage 3: W4A16 æ¨ç†**:
```python
# SGLang åŠ è½½ INT4 æƒé‡
python -m sglang.launch_server \
  --model-path /path/to/int4_model \
  --quantization marlin \
  --context-length 4096

# é«˜æ•ˆæ¨ç† (INT4 æƒé‡ Ã— BF16 æ¿€æ´»)
# 1. Bit packing: 8 ä¸ª INT4 â†’ 1 ä¸ª INT32
# 2. è§£åŒ…: ä½è¿ç®— (>> 4 å’Œ & 0xF)
# 3. è®¡ç®—: INT4 æƒé‡ Ã— BF16 æ¿€æ´»
# 4. ç”Ÿæˆ: BF16 è¾“å‡º
```

---

### 8.7.3 è®­ç»ƒç«¯å®ç°

**Fake Quantization å’Œ STE å®ç°**:
```python
class _FakeInt4QuantizationSTE(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        # åŠ¨æ€é‡åŒ–: per-group max absolute value
        group_size = 128
        x = x.view(-1, group_size)

        # è®¡ç®— scale
        scale = x.abs().max(dim=1, keepdim=True) / 7

        # æ¨¡æ‹Ÿ INT4 çš„ [-7, 7] èŒƒå›´
        x_quant = torch.clamp(torch.round(x / scale), -7, 7)

        # è®°å½• scale (ç”¨äºåç»­çœŸé‡åŒ–)
        ctx.save_for_backward(scale)

        # è¿”å›åé‡åŒ–ç»“æœ (ä¿æŒå¯å¾®åˆ†)
        return (x_quant * scale).view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        # STE: æ¢¯åº¦ç›´æ¥ä¼ é€’
        scale, = ctx.saved_tensors
        return grad_output

def apply_fake_quantization(model):
    """å¯¹æ¨¡å‹åº”ç”¨ fake quantization"""
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # æ›¿æ¢ä¸ºé‡åŒ–ç‰ˆæœ¬
            module.weight = _FakeInt4QuantizationSTE.apply(module.weight)
```

**æƒé‡æ›´æ–°å’Œæ ¼å¼é€‚é…**:
```python
def restore_weights_before_loading(model):
    """
    åœ¨åŠ è½½æƒé‡å‰æ¢å¤åŸå§‹æƒé‡

    é—®é¢˜: PyTorch åŠ è½½æƒé‡çš„æœºåˆ¶
    â†’ åŠ è½½åä¿®æ”¹æƒé‡å¯èƒ½å¤±æ•ˆ

    è§£å†³: ä½¿ç”¨ register_buffer
    """
    for name, module in model.named_modules():
        if hasattr(module, 'weight_fp16'):
            # æ³¨å†Œä¸º buffer (ä¸ä¼šè¢« optimizer æ›´æ–°)
            module.register_buffer('weight_fp16', module.weight_fp16)

def process_weights_after_loading(model):
    """
    åŠ è½½æƒé‡åå¤„ç†

    1. æ¢å¤åŸå§‹ FP16 æƒé‡
    2. åº”ç”¨ fake quantization
    3. è½¬æ¢ä¸º Marlin æ ¼å¼
    """
    for name, module in model.named_modules():
        if hasattr(module, 'weight_fp16'):
            # æ¢å¤ FP16 æƒé‡
            weight = module.weight_fp16

            # åº”ç”¨ fake quantization
            weight_quant = fake_quantize(weight)

            # è½¬æ¢ä¸º Marlin æ ¼å¼
            weight_marlin = marlin_pack(weight_quant)

            # ä¿å­˜
            module.register_buffer('weight_marlin', weight_marlin)
            del module.weight_fp16  # é‡Šæ”¾æ˜¾å­˜
```

**æ¶ˆèå®éªŒ: QAT çš„å¿…è¦æ€§**:
```
å®éªŒ 1: QAT INT4 è®­ç»ƒ + BF16 rollout
  - è®­ç»ƒ: Fake Quantization (INT4)
  - Rollout: BF16 æƒé‡
  - ç»“æœ: è¯¯å·®ä»é«˜ (æ¨¡å‹ä¸é€‚åº”é‡åŒ–)

å®éªŒ 2: ä¸å¯ç”¨ QAT + ç›´æ¥ INT4 rollout
  - è®­ç»ƒ: BF16 æƒé‡
  - Rollout: ç›´æ¥ INT4 æƒé‡
  - ç»“æœ: è¯¯å·®éœ‡è¡ä¸Šå‡ (å´©æºƒ)

å®éªŒ 3: QAT INT4 è®­ç»ƒ + INT4 rollout (æ­£ç¡®)
  - è®­ç»ƒ: Fake Quantization (INT4)
  - Rollout: INT4 æƒé‡
  - ç»“æœ: è¯¯å·®æ”¶æ•›,ä¸ BF16 baseline æ¥è¿‘

**ç»“è®º**: è®­ç»ƒå’Œæ¨ç†å¿…é¡»åŒæ—¶å¯ç”¨é‡åŒ–!
```

---

### 8.7.4 æ¨ç†ç«¯å®ç°

**SGLang W4A16 æ¨ç†**:
```python
# Bit packing: 8 ä¸ª INT4 å€¼æ‰“åŒ…åˆ° 1 ä¸ª INT32
def marlin_pack(int4_weights):
    """
    å°† INT4 æƒé‡æ‰“åŒ…ä¸º Marlin æ ¼å¼

    Args:
        int4_weights: [N, M] INT4 å¼ é‡

    Returns:
        packed: [N//2, M] INT32 å¼ é‡ (8 ä¸ª INT4 â†’ 1 ä¸ª INT32)
    """
    # é‡æ’å’Œæ‰“åŒ…
    int4_weights = int4_weights.reshape(-1, 16)  # æ¯ 16 ä¸ªä¸€ç»„
    packed = torch.zeros(int4_weights.shape[0], int4_weights.shape[1] // 4,
                        dtype=torch.int32, device=int4_weights.device)

    for i in range(16):
        packed[:, i//4] |= (int4_weights[:, i] & 0xF) << (4 * (i % 4))

    return packed

# é«˜æ•ˆè§£åŒ…: ä½è¿ç®—
def marlin_unpack(packed):
    """
    ä» Marlin æ ¼å¼è§£åŒ… INT4 æƒé‡

    Args:
        packed: [N//2, M] INT32 å¼ é‡

    Returns:
        int4_weights: [N, M] INT4 å¼ é‡
    """
    int4_weights = torch.zeros(packed.shape[0] * 2, packed.shape[1] * 4,
                                dtype=torch.int8, device=packed.device)

    for i in range(8):
        int4_weights[i::8] = ((packed[i//4, ::8] >> (4 * (i % 4))) & 0xF).to(torch.int8)
        if int4_weights[i::8].min() >= 8:
            int4_weights[i::8] -= 16  # è½¬æ¢ä¸ºæœ‰ç¬¦å·

    return int4_weights
```

**è®¡ç®—å’Œ IO é‡å ,è§£åŒ…è¿‘é›¶å¼€é”€**:
```
ä¼˜åŒ–ç­–ç•¥:
  1. é¢„å–ä¸‹ä¸€æ‰¹æƒé‡ (Prefetch)
  2. å½“å‰æ‰¹è®¡ç®—æ—¶,å¼‚æ­¥è§£åŒ…ä¸‹ä¸€æ‰¹
  3. ä½¿ç”¨ CUDA stream å¹¶è¡ŒåŒ–

æ•ˆæœ:
  - è§£åŒ…å¼€é”€: <5% æ€»æ—¶é—´
  - å‡ ä¹"è¿‘é›¶å¼€é”€"
```

**MoE ç®—å­æ·±åº¦èåˆ**:
```python
def dynamic_moe_align_block_size(num_experts):
    """
    åŠ¨æ€è°ƒæ•´ MoE block size

    ç›®æ ‡:
    - ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
    - å‡å°‘ kernel å¯åŠ¨æ¬¡æ•°
    - Gating éƒ¨åˆ†èåˆä¸ºå•ä¸€å†…æ ¸
    """
    if num_experts <= 8:
        return 64
    elif num_experts <= 32:
        return 128
    else:
        return 256

# èåˆ Gating å’Œ Expert kernel
def fused_moe_kernel(gate, experts, block_size):
    """
    èåˆ MoE çš„ Gating å’Œ Expert è®¡ç®—

    é¿å…å¤šæ¬¡ kernel å¯åŠ¨:
    æ—§: 1 (gate) + N (experts) = N+1 æ¬¡
    æ–°: 1 (èåˆ) = 1 æ¬¡
    """
    # å•ä¸€ kernel å®Œæˆ gating å’Œ routing
    return torch.ops.fused_moe(gate, experts, block_size)
```

---

### 8.7.5 å®æˆ˜æ¡ˆä¾‹: 1TB æ¨¡å‹å‹ç¼©åˆ°å• H200

**æ¡ˆä¾‹ 1: Qwen3-235B-A22B**:
```
é…ç½®:
  - å‚æ•°é‡: 235B (MoE æ¶æ„)
  - åŸå§‹å¤§å°: ~1TB (BF16)
  - é‡åŒ–: INT4 QAT
  - ç›®æ ‡ç¡¬ä»¶: å•å¼  H200 (141GB)

ç»“æœ:
  - é‡åŒ–åå¤§å°: ~250GB (INT4)
  - é…åˆæ¨¡å‹å¹¶è¡Œ: 2 å¼  H200
  - æˆ–æ¿€è¿›å‹ç¼© + KV Cache é‡åŒ– â†’ å• H200

ç²¾åº¦:
  - Raw-Reward: ç¨³å®šå¢é•¿,ä¸ BF16/FP8 è¶‹åŠ¿ä¸€è‡´
  - AIME è¯„ä¼°: æ–œç‡å’Œå³°å€¼ä¸ BF16 é«˜åº¦å¯¹é½
  - Train-Infer Gap: å‡ ä¹é‡å  BF16 baseline
```

**æ¡ˆä¾‹ 2: Kimi-K2-Thinking**:
```
é…ç½®:
  - å‚æ•°é‡: 100B+
  - åŒèŠ‚ç‚¹éƒ¨ç½² (åŸå§‹)

åŒèŠ‚ç‚¹ (BF16):
  - å—é™äºè·¨èŠ‚ç‚¹å¸¦å®½
  - é€šä¿¡æˆä¸ºç“¶é¢ˆ
  - Rollout æ•ˆç‡ä½

å•èŠ‚ç‚¹ (INT4 QAT):
  - æ¶ˆé™¤è·¨èŠ‚ç‚¹é€šä¿¡
  - æ˜¾å­˜å ç”¨é™ä½åˆ°å• H200 èŒƒå›´
  - Rollout æ•ˆç‡å¤§å¹…æå‡

æ€§èƒ½å¯¹æ¯”:
  - ç²¾åº¦: INT4 QAT â‰ˆ BF16 > FP8
  - é€Ÿåº¦: INT4 â‰ˆ FP8 > BF16 (H ç³»åˆ— GPU)
  - æ˜¾å­˜: INT4 èŠ‚çœ 75% (å…³é”®ä¼˜åŠ¿)
```

---

### 8.7.6 QAT çš„é€‚ç”¨åœºæ™¯

**âœ… æ¨è**:
- å¤§è§„æ¨¡ RL è®­ç»ƒ (100B+ å‚æ•°)
- éœ€è¦å•èŠ‚ç‚¹éƒ¨ç½²è¶…å¤§æ¨¡å‹
- éœ€è¦ train-infer ä¸€è‡´æ€§
- PTQ ç²¾åº¦æŸå¤±ä¸å¯æ¥å—

**âš ï¸ æ³¨æ„**:
- è®­ç»ƒæˆæœ¬è¾ƒé«˜ (éœ€è¦å®Œæ•´å¾®è°ƒå‘¨æœŸ)
- å®ç°å¤æ‚åº¦è¾ƒé«˜ (éœ€è¦ç†è§£ QATã€STEã€æ ¼å¼è½¬æ¢)

**âŒ ä¸æ¨è**:
- å°è§„æ¨¡æ¨¡å‹ (æˆæœ¬ä¸å€¼å¾—)
- åªéœ€è¦æ¨ç†ä¸éœ€è¦å¾®è°ƒ (ç”¨ PTQ æ›´å¿«)

---

## 8.8 ç²¾åº¦å¯¹é½: Train vs Inference âš ï¸ å·¥ä¸šç•Œå®è·µ

> **ğŸ’¡ å·¥ä¸šç•Œå®è·µ** (æ¥æº: 2025"é’ç¨"AIå˜‰å¹´å - æœ±ç«‹è€•@NVIDIA)
>
> **æ ¸å¿ƒæ´å¯Ÿ**: ä½ç²¾åº¦è®­ç»ƒä¸ç¨³å®šçš„æ ¹æœ¬åŸå› å¾€å¾€ä¸æ˜¯ä½ç²¾åº¦æœ¬èº«,è€Œæ˜¯è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨çš„ç®—å­ç²¾åº¦ä¸å¯¹é½ã€‚
>
> **å¤§å›¢é˜Ÿçš„åšæ³•**: Train å’Œ Inference çš„ç®—å­åœ¨åŒä¸€ä¸ªå¤§çš„ wrapper é‡Œç»´æŠ¤,ç²¾åº¦é—®é¢˜å°±ä¸æ˜¯é—®é¢˜ã€‚
>
> **å¼€æºç¤¾åŒºçš„é—®é¢˜**: Train å’Œ Inference æ˜¯ä¸¤å¸®äººåš,ç®—å­æ²¡å¯¹é½å¯¼è‡´ accuracy ä¸ç¨³å®šã€‚

### 8.8.1 ç²¾åº¦ä¸å¯¹é½çš„é—®é¢˜

**å…¸å‹åœºæ™¯**:
```
è®­ç»ƒæ—¶:
  - è‡ªå®šä¹‰ kernel (å¦‚è‡ªå·±å†™çš„ Flash Attention)
  - FP32/FP16 æ•°å€¼å¤„ç†
  - ç‰¹å®šçš„ç®—æ³•å®ç°

æ¨ç†æ—¶:
  - ç¤¾åŒºä¼˜åŒ–çš„ kernel (å¦‚ SGLang çš„ Flash Attention)
  - INT8/INT4 é‡åŒ–
  - ä¸åŒçš„æ•°å€¼å¤„ç†

ç»“æœ: Numerical gap å¯¼è‡´ accuracy ä¸ç¨³å®š
  - Training loss spike
  - æœ€ç»ˆ accuracy æ‰ç‚¹
```

**è¡¨ç°**:
```
ç—‡çŠ¶:
  - è®­ç»ƒæ—¶ loss æ­£å¸¸ä¸‹é™
  - éƒ¨ç½²åˆ°æ¨ç†æ¡†æ¶åæ€§èƒ½å´©æºƒ
  - PPL é£™å‡ 2-3x
  - ç”Ÿæˆè´¨é‡æ˜æ˜¾ä¸‹é™

åŸå› :
  - è®­ç»ƒå’Œæ¨ç†ç®—å­ä¸å¯¹é½
  - æ•°å€¼ç²¾åº¦ä¸åŒ
  - ç®—æ³•å®ç°å·®å¼‚
```

---

### 8.8.2 ä¸ºä»€ä¹ˆç²¾åº¦ä¸å¯¹é½?

**å¼€å‘å›¢é˜Ÿåˆ†ç¦»**:
```
Training Team:
  - å…³æ³¨æ”¶æ•›é€Ÿåº¦
  - è‡ªå®šä¹‰ä¼˜åŒ–
  - å¿«é€Ÿè¿­ä»£

Inference Team:
  - å…³æ³¨æ¨ç†é€Ÿåº¦
  - ç¤¾åŒºä¼˜åŒ–
  - å…¼å®¹æ€§

é—®é¢˜: ä¸¤ä¸ªå›¢é˜Ÿæ²¡æœ‰ååŒ
```

**ä¼˜åŒ–ç›®æ ‡ä¸åŒ**:
```
Training:
  - æœ€å¤§åŒ–è®­ç»ƒåå
  - æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
  - å®¹é”™å’Œæ£€æŸ¥ç‚¹

Inference:
  - æœ€å¤§åŒ–æ¨ç†åå
  - æœ€å°åŒ–å»¶è¿Ÿ
  - é‡åŒ–å‹ç¼©

å†²çª: ä¼˜åŒ–æ–¹å‘ä¸åŒ,å®ç°æœ‰å·®å¼‚
```

**å®ç°ç»†èŠ‚å·®å¼‚**:
```
Flash Attention:
  - è®­ç»ƒç‰ˆæœ¬: æŸç§æ•°å€¼ç®€åŒ–
  - æ¨ç†ç‰ˆæœ¬: å¦ä¸€ç§ä¼˜åŒ–
  - ç»“æœ: è¾“å‡ºæœ‰å¾®å°å·®å¼‚

Attention Mask:
  - è®­ç»ƒ: å¸ƒå°” mask
  - æ¨ç†: æµ®ç‚¹ mask (ä¸ºäº†å…¼å®¹æ€§)
  - ç»“æœ: ç²¾åº¦ç´¯ç§¯è¯¯å·®
```

**æµ‹è¯•åœºæ™¯ä¸åŒ**:
```
Training:
  - åˆæˆæ•°æ® (éšæœºè¾“å…¥)
  - å¿«é€ŸéªŒè¯
  - ä¸è¦†ç›– edge case

Inference:
  - çœŸå®æ•°æ®
  - å„ç§ edge case
  - é•¿åºåˆ—ã€ç‰¹æ®Šå­—ç¬¦

é—®é¢˜: è®­ç»ƒæ²¡è¦†ç›–çš„åœºæ™¯,æ¨ç†æš´éœ²é—®é¢˜
```

---

### 8.8.3 å¦‚ä½•ç¡®ä¿ç²¾åº¦å¯¹é½

**æ–¹æ³• 1: ç»Ÿä¸€ç®—å­åº“** (æ¨è)
```python
# ç»Ÿä¸€çš„ Attention wrapper
class UnifiedAttention:
    def __init__(self, use_quantization=False):
        self.use_quantization = use_quantization
        self.kernel = get_attention_kernel(use_quantization)

    def forward(self, q, k, v):
        # è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨åŒä¸€å¥—ç®—å­
        return self.kernel(q, k, v)

# è®­ç»ƒæ—¶
attn = UnifiedAttention(use_quantization=True)  # Fake quant
output = model(input)

# æ¨ç†æ—¶
attn = UnifiedAttention(use_quantization=True)  # True quant
output = model(input)
```

**æ–¹æ³• 2: æ•°å€¼å¯¹é½æµ‹è¯•**
```python
def test_numerical_alignment():
    """æµ‹è¯•è®­ç»ƒå’Œæ¨ç†ç®—å­çš„æ•°å€¼å¯¹é½"""
    # ç”Ÿæˆç›¸åŒè¾“å…¥
    x = torch.randn(1, 512, 4096)

    # è®­ç»ƒç®—å­
    train_output = training_attention(x)

    # æ¨ç†ç®—å­
    infer_output = inference_attention(x)

    # æ¯”è¾ƒè¾“å‡ºå·®å¼‚
    abs_error = (train_output - infer_output).abs().max()

    assert abs_error < 1e-5, f"ä¸å¯¹é½! æœ€å¤§è¯¯å·®: {abs_error}"

# CI/CD Pipeline
def ci_pipeline():
    """è‡ªåŠ¨æ£€æµ‹ç²¾åº¦ regression"""
    for commit in recent_commits:
        if not test_numerical_alignment():
            print(f"âŒ Commit {commit} å¯¼è‡´ç²¾åº¦ä¸å¯¹é½")
            return False

    print("âœ… æ‰€æœ‰ commit ç²¾åº¦å¯¹é½")
    return True
```

**æ–¹æ³• 3: ç«¯åˆ°ç«¯éªŒè¯**
```python
def end_to_end_validation():
    """ç«¯åˆ°ç«¯éªŒè¯: è®­ç»ƒåç›´æ¥åœ¨æ¨ç†æ¡†æ¶ä¸­æµ‹è¯•"""
    # 1. è®­ç»ƒæ¨¡å‹
    model = train_model()

    # 2. å¯¼å‡ºæƒé‡
    weights = model.state_dict()

    # 3. åŠ è½½åˆ°æ¨ç†æ¡†æ¶
    inference_model = load_for_inference(weights)

    # 4. æ¯”è¾ƒè¾“å‡º
    train_output = model.generate(test_input)
    infer_output = inference_model.generate(test_input)

    # 5. æ£€æŸ¥å·®å¼‚
    diff = (train_output - infer_output).abs().max()
    if diff > threshold:
        print(f"âš ï¸  å‘ç°ç²¾åº¦ regression: {diff}")
        return False

    return True
```

---

### 8.8.4 ä¸åŒä»»åŠ¡å¯¹ç²¾åº¦çš„æ•æ„Ÿåº¦

**LLM**: ç¦»æ•£é‡‡æ ·,å¯¹ä½ç²¾åº¦å®¹å¿åº¦é«˜
```
ä¸ºä»€ä¹ˆ LLM å¯¹é‡åŒ–å‹å¥½?
  - è¾“å‡ºæ˜¯ç¦»æ•£çš„ (token IDs)
  - Temperature å¼•å…¥éšæœºæ€§
  - å°çš„é‡åŒ–è¯¯å·®è¢«é‡‡æ ·æ©ç›–

è¯æ®:
  - INT8 é‡åŒ–: æŸå¤± <0.5%
  - INT4 é‡åŒ– (QAT): æŸå¤± 1-2%
  - ç»“è®º: LLM å¯¹é‡åŒ–å®¹å¿åº¦é«˜
```

**Diffusion**: è¿ç»­ç©ºé—´é‡‡æ ·,è¯¯å·®ç´¯ç§¯ä¸¥é‡
```
ä¸ºä»€ä¹ˆ Diffusion å¯¹é‡åŒ–æ•æ„Ÿ?
  - è¾“å‡ºæ˜¯è¿ç»­çš„ (åƒç´ å€¼)
  - å¤šæ­¥é‡‡æ · (50-1000 steps)
  - æ¯æ­¥è¯¯å·®ç´¯ç§¯

è¯æ® (å¼ åšæ¶µ@æµ™å¤§):
  - FP4 é‡åŒ–: æ‰ 10-20 ä¸ªç‚¹
  - éœ€è¦ special clipping å’Œ correction
  - æ¨èè‡³å°‘ä½¿ç”¨ FP8

ç»“è®º: Diffusion æ¨¡å‹è‡³å°‘ä½¿ç”¨ FP8
```

**å¯¹æ¯”**:
| ä»»åŠ¡ç±»å‹ | æ¨èæ ¼å¼ | ç²¾åº¦æŸå¤± | è¯´æ˜ |
|---------|---------|---------|------|
| **LLM** | INT4/INT8 | 1-2% | ç¦»æ•£é‡‡æ ·,å®¹å¿åº¦é«˜ |
| **Diffusion** | FP8/FP16 | <1% | è¿ç»­é‡‡æ ·,è¯¯å·®ç´¯ç§¯ |
| **Recommendation** | INT8 | <1% | ç±»ä¼¼ LLM |
| **RL** | INT4 (QAT) | 1-3% | éœ€è¦ train-infer ä¸€è‡´ |

---

### 8.8.5 ä½ç²¾åº¦çš„è½¯ä»¶æŠ½è±¡å¤æ‚åº¦

**BF16/FP16**: ä¸€ä¸ª tensor å°±æ˜¯ä¸€ä¸ªæ•°æ®
```python
weight = torch.randn(4096, 4096, dtype=torch.float16)
# ç®€å•ã€ç›´è§‚
```

**FP8**: ä¸€ä¸ª weight å˜æˆ 3 ä¸ª tensor
```python
weight_fp8 = torch.randn(4096, 4096, dtype=torch.float8_e4m3)
scale = weight_fp8.abs().max() / 127  # ç¼©æ”¾å› å­
weight_meta = {"dtype": "fp8_e4m3", "scale": scale}  # å…ƒæ•°æ®

# è½¯ä»¶å¤æ‚åº¦å¤§å¹…å¢åŠ 
# éœ€è¦åŒæ—¶ç®¡ç† 3 ä¸ªå¯¹è±¡
```

**FP4**: éœ€è¦ paddingã€pack ç­‰æ“ä½œ
```python
# FP4 æ‰“åŒ…: 2 ä¸ª FP4 â†’ 1 byte
weight_fp4_packed = pack_fp4(weight_fp4)  # è‡ªå®šä¹‰æ ¼å¼

# PyTorch æœ€å°‘ 1 byte
# éœ€è¦ç‰¹æ®Šå¤„ç†

# è½¯ä»¶ç”Ÿæ€éœ€è¦å¤§è§„æ¨¡æ¼”è¿›
```

**æŒ‘æˆ˜**: ç”¨æˆ·å¿ƒæ™ºè´Ÿæ‹…å¤§
```
é—®é¢˜:
  - å¦‚ä½•å¹³è¡¡æ”¶ç›Šå’Œå¤æ‚åº¦?
  - æŠ½è±¡åº”è¯¥åœ¨å“ªé‡Œ?
  - ç”¨æˆ·éœ€è¦ç†è§£åº•å±‚ç»†èŠ‚å—?

æ–¹å‘:
  - æ¡†æ¶è‡ªåŠ¨å¤„ç† (vLLMã€SGLang)
  - ç”¨æˆ·å‹å¥½ API
  - æ¸è¿›å¼ä¼˜åŒ–
```

---

### 8.8.6 ä½ç²¾åº¦è®­ç»ƒçš„ç¨³å®šæ€§é—®é¢˜

**å¸¸è§ç—‡çŠ¶**:
```
ç—‡çŠ¶ 1: è®­ç»ƒåˆ°ä¸€åŠ loss ç‚¸äº†
  - å‰ 1000 steps: loss æ­£å¸¸ä¸‹é™
  - Step 1001: loss çªç„¶æš´æ¶¨
  - Step 1002: NaN

ç—‡çŠ¶ 2: åŒæ · task é«˜ç²¾åº¦æ²¡é—®é¢˜,ä½ç²¾åº¦ç›´æ¥èµ·é£
  - FP32: æ”¶æ•›æ­£å¸¸
  - FP8: loss ä¸ä¸‹é™

ç—‡çŠ¶ 3: é«˜ç²¾åº¦ accuracy æŒºå¥½,ä½ç²¾åº¦ç¬é—´æ‰ 3-4 ä¸ªç‚¹
  - FP32: 85% accuracy
  - FP8: 81% accuracy (æ‰ 4 ä¸ªç‚¹)
```

**æ ¹æœ¬åŸå› **: (å¼ æ˜æ˜Ÿ@æ¸…å)
```
ä¸å…¨æ˜¯ç²¾åº¦é—®é¢˜,è€Œæ˜¯ç®—æ³•æ²¡è°ƒå¥½

å¸¸è§é—®é¢˜:
  - Loss control æ²¡åšå¥½
  - Data mixing ä¸åˆç†
  - Curriculum learning ç¼ºå¤±
  - LR schedule ä¸é€‚åˆä½ç²¾åº¦
```

**è§£å†³æ–¹å‘**:
```
1. æŠŠå„ç§"å†…ç§‘" (å¼ æ˜æ˜Ÿè¯­) æ£€æŸ¥å¾—æ›´ç»†
   - Gradient clipping
   - Weight decay
   - Learning rate warmup
   - Batch size è°ƒæ•´

2. ä¸è¦ä¸Šæ¥å°±æå¾ˆéš¾çš„é¢˜ç›®,ä»ç®€å•å¼€å§‹
   - Curriculum learning
   - ä»ç®€å• task å¼€å§‹
   - é€æ­¥å¢åŠ éš¾åº¦

3. ä½ç²¾åº¦å¯èƒ½å¼•å…¥å™ªå£°,åè€Œæœ‰åŠ©äºæ”¶æ•›
   - Kimi K2 çš„ INT4 ç»éªŒ
   - å™ªå£°æœ‰åŠ©äºæ³›åŒ–
   - ä½†éœ€è¦æ§åˆ¶å™ªå£°æ°´å¹³
```

---

### 8.8.7 ä»å†å²çœ‹ç²¾åº¦æ¼”è¿› (æœ±ç«‹è€•@NVIDIA)

**FP32 â†’ FP16**: è§è¿‡ç±»ä¼¼é—®é¢˜,æœ€ç»ˆè§£å†³
```
2016-2018 å¹´:
  é—®é¢˜: FP16 è®­ç»ƒä¸ç¨³å®š
  è§£å†³: æ··åˆç²¾åº¦è®­ç»ƒã€Loss scaling
  ç°çŠ¶: å®Œå…¨æˆç†Ÿ,å·¥ä¸šæ ‡å‡†
```

**FP16 â†’ BF16**: è§è¿‡ç±»ä¼¼é—®é¢˜,æœ€ç»ˆè§£å†³
```
2020-2022 å¹´:
  é—®é¢˜: FP16 èŒƒå›´å°,å®¹æ˜“ä¸‹æº¢
  è§£å†³: BF16 (ä¸ FP32 ç›¸åŒèŒƒå›´)
  ç°çŠ¶: å®Œå…¨æˆç†Ÿ,å¹¿æ³›ä½¿ç”¨
```

**BF16 â†’ FP8**: ç°åœ¨æ˜¯è¿‡æ¸¡æœŸé˜µç—›
```
2023-2025 å¹´:
  é—®é¢˜: FP8 è®­ç»ƒç¨³å®šæ€§
  è§£å†³: æ­£åœ¨è§£å†³ä¸­...
  é¢„æœŸ: 1-2 å¹´å†…æˆç†Ÿ
```

**ç»“è®º**:
```
éšç€ç®—æ³• stabilize å’Œ config æ‘¸æ¸…,é—®é¢˜å¯ä»¥è§£å†³
ä½ç²¾åº¦æ”¶ç›Šè¿˜æ˜¯å¾ˆå¤§çš„,å€¼å¾—æŠ•å…¥
```

---

## 8.9 é‡åŒ–æŠ€æœ¯æ€»ç»“ä¸å±•æœ›

### 8.9.1 é‡åŒ–æŠ€æœ¯æ¼”è¿›è·¯çº¿

```
2020-2021:
  FP32/FP16 æ ‡å‡†
  INT8 é‡åŒ–å¼€å§‹æµè¡Œ

2022-2023:
  BF16 å¹¿æ³›é‡‡ç”¨
  INT4 é‡åŒ–æˆç†Ÿ (GPTQã€AWQ)
  PTQ ä¸ºä¸»æµ

2024-2025:
  FP8 å‡ºç° (H100/H200 æ”¯æŒ)
  QAT å—é‡è§†
  PTQ + QAT æ··åˆæ–¹æ¡ˆ

2025-2026 (é¢„æœŸ):
  FP4/NVFP4 æˆç†Ÿ (Blackwell)
  ç»Ÿä¸€é‡åŒ–æ¡†æ¶
  ç«¯åˆ°ç«¯ä¼˜åŒ–
```

---

### 8.9.2 ä¸åŒåœºæ™¯çš„æœ€ä½³å®è·µ

**åœºæ™¯ 1: å¿«é€Ÿéƒ¨ç½²**
```
æ¨è: PTQ (INT8/INT4)
å·¥å…·: vLLM + AWQ
æµç¨‹:
  1. é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹
  2. AWQ/GPTQ é‡åŒ–
  3. éƒ¨ç½²åˆ° vLLM
æ—¶é—´: 1-2 å°æ—¶
```

**åœºæ™¯ 2: ç”Ÿäº§ç¯å¢ƒ (ç²¾åº¦è¦æ±‚é«˜)**
```
æ¨è: QAT (INT8)
å·¥å…·: NVIDIA Model Optimizer
æµç¨‹:
  1. å‡†å¤‡è®­ç»ƒæ•°æ®
  2. QAT è®­ç»ƒ (å‡ ä¸ª epoch)
  3. å¯¼å‡º INT8 æƒé‡
  4. éƒ¨ç½²
æ—¶é—´: 1-3 å¤©
```

**åœºæ™¯ 3: æé™å‹ç¼© (100B+ å‚æ•°)**
```
æ¨è: QAT (INT4)
å·¥å…·: SGLang + Marlin
æµç¨‹:
  1. QAT è®­ç»ƒ (å®Œæ•´å¾®è°ƒ)
  2. è½¬æ¢ä¸º Marlin æ ¼å¼
  3. SGLang éƒ¨ç½²
  4. ç²¾åº¦éªŒè¯
æ—¶é—´: 1-2 å‘¨
```

---

### 8.9.3 æœªæ¥å‘å±•æ–¹å‘: FP4ã€NVFP4ã€Blackwell

**Blackwell çš„åŸç”Ÿ FP4/FP8 æ”¯æŒ**:
```
ç¡¬ä»¶ç‰¹æ€§:
  - ç¬¬äºŒä»£ Transformer Engine
  - FP4/FP8 Tensor Core
  - æ›´é«˜çš„å†…å­˜å¸¦å®½

æ€§èƒ½æ½œåŠ›:
  - FP4: 4x FP16 ç†è®ºé€Ÿåº¦
  - FP8: 2x FP16 ç†è®ºé€Ÿåº¦
  - æ˜¾å­˜èŠ‚çœ: 75%-87.5%

æŒ‘æˆ˜:
  - è½¯ä»¶ç”Ÿæ€ä¸æˆç†Ÿ
  - éœ€è¦æ–°çš„é‡åŒ–ç®—æ³•
  - ç²¾åº¦å¯¹é½é—®é¢˜
```

**æ—¶é—´è¡¨**:
```
2025: Blackwell ä¸Šå¸‚,FP8 æˆç†Ÿ
2026: FP4/NVFP4 å®éªŒæ€§æ”¯æŒ
2027: FP4/NVFP4 ç”Ÿäº§å°±ç»ª
2028+: æ–°çš„é‡åŒ–æ ¼å¼ (FP2?)
```

---

### 8.9.4 ç®—æ³•å’Œç³»ç»Ÿçš„ co-design (å¼ åšæ¶µ@æµ™å¤§)

**æ ¸å¿ƒè§‚ç‚¹**:
```
ä¸æ˜¯ç³»ç»Ÿç­‰ç®—æ³•æˆç†Ÿ
ä¸æ˜¯ç®—æ³•ç­‰ç³»ç»Ÿä¼˜åŒ–
éœ€è¦åŒæ­¥èºæ—‹å¼ä¸Šå‡
```

**ä¾‹å­**:
```
ç®—æ³•è¿›æ­¥:
  - æ–°çš„é‡åŒ–æ–¹æ³•
  - æ›´å¥½çš„ fake quantization
  - æ›´ç¨³å®šçš„è®­ç»ƒæŠ€å·§

ç³»ç»Ÿè¿›æ­¥:
  - æ›´å¿«çš„é‡åŒ– kernel
  - æ›´å¥½çš„ç¡¬ä»¶æ”¯æŒ
  - æ›´æˆç†Ÿçš„å·¥å…·é“¾

ä¸¤è€…ååŒ:
  - ç®—æ³•æŒ‡å¯¼ç³»ç»Ÿè®¾è®¡
  - ç³»ç»Ÿçº¦æŸæ¨åŠ¨ç®—æ³•åˆ›æ–°
  - å…±åŒæ¼”è¿›
```

**å¯ç¤º**:
```
ä¸è¦ç­‰å¾…"å®Œç¾"çš„ç®—æ³•
ç®—æ³•å’Œç³»ç»Ÿè¦ä¸€èµ·è¿­ä»£
å°æ­¥å¿«è·‘,å¿«é€ŸéªŒè¯
```

---

## âœ… ç« èŠ‚æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬ç« å,ä½ åº”è¯¥èƒ½å¤Ÿ:

- [ ] è§£é‡Šé‡åŒ–çš„åŸºæœ¬åŸç†å’Œå…¬å¼
- [ ] è®¡ç®—é‡åŒ–åçš„æ˜¾å­˜å ç”¨
- [ ] å¯¹æ¯” PTQ å’Œ QAT çš„ä¼˜ç¼ºç‚¹
- [ ] é€‰æ‹©åˆé€‚çš„é‡åŒ–æ ¼å¼ (INT8/INT4/FP8)
- [ ] ä½¿ç”¨ vLLM éƒ¨ç½²é‡åŒ–æ¨¡å‹
- [ ] ä½¿ç”¨ SGLang éƒ¨ç½² INT4 æ¨¡å‹
- [ ] å®ç° fake quantization
- [ ] è¿›è¡Œç²¾åº¦å¯¹é½æµ‹è¯•
- [ ] è¯Šæ–­é‡åŒ–ç²¾åº¦é—®é¢˜
- [ ] æ ¹æ®åœºæ™¯é€‰æ‹©é‡åŒ–æ–¹æ¡ˆ

---

## ğŸ“š åŠ¨æ‰‹ç»ƒä¹ 

**ç»ƒä¹  8.1**: å¯¹æ¯”ä¸åŒé‡åŒ–æ ¼å¼çš„æ€§èƒ½å’Œç²¾åº¦

ä»»åŠ¡:
1. åŠ è½½ Llama-2-7B çš„ FP16ã€INT8ã€INT4 ç‰ˆæœ¬
2. æµ‹é‡æ˜¾å­˜å ç”¨ã€æ¨ç†é€Ÿåº¦ã€ç²¾åº¦
3. ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨

**ç»ƒä¹  8.2**: é‡åŒ– Llama-3-70B å¹¶æµ‹è¯• (ä½¿ç”¨ vLLM + AWQ)

ä»»åŠ¡:
1. ä¸‹è½½ Llama-3-70B-AWQ æ¨¡å‹
2. ä½¿ç”¨ vLLM åŠ è½½å¹¶æµ‹è¯•
3. å¯¹æ¯” FP16 å’Œ INT4 çš„æ€§èƒ½

**ç»ƒä¹  8.3**: ä½¿ç”¨ SGLang éƒ¨ç½² INT4 æ¨¡å‹å¹¶ benchmark â­

ä»»åŠ¡:
1. å®‰è£… SGLang
2. å¯åŠ¨ INT4 æ¨ç†æœåŠ¡
3. è¿›è¡Œæ€§èƒ½ benchmark
4. è¯„ä¼°ç²¾åº¦æŸå¤±

**ç»ƒä¹  8.4**: (è¿›é˜¶) å®ç°ç®€å•çš„ fake quantization â­

ä»»åŠ¡:
1. å®ç° FakeInt4QuantizationSTE ç±»
2. åœ¨å°å‹æ¨¡å‹ä¸Šæµ‹è¯•
3. éªŒè¯æ¢¯åº¦æ­£ç¡®ä¼ é€’

---

## ğŸ¯ æ€»ç»“

**å…³é”®è¦ç‚¹**:
- é‡åŒ–é€šè¿‡é™ä½ç²¾åº¦èŠ‚çœæ˜¾å­˜å’Œæå‡é€Ÿåº¦
- PTQ å¿«é€Ÿä½†å¯èƒ½æœ‰ç²¾åº¦æŸå¤±,QAT ç²¾åº¦é«˜ä½†æˆæœ¬é«˜
- INT4 æ˜¯å½“å‰æé™å‹ç¼©çš„é¦–é€‰ (75% èŠ‚çœ)
- KV Cache é‡åŒ–å¯¹é•¿åºåˆ—å¾ˆé‡è¦
- è®­ç»ƒå’Œæ¨ç†å¿…é¡»ç²¾åº¦å¯¹é½æ‰èƒ½ä¿è¯ç¨³å®šæ€§
- FP4/FP8 æ˜¯æœªæ¥æ–¹å‘ (Blackwell æ”¯æŒ)

**ä¸‹ä¸€ç« **: ç¬¬9ç«  æŠ•æœºé‡‡æ ·â€”â€”é€šè¿‡æ¨æµ‹è§£ç åŠ é€Ÿç”Ÿæˆã€‚

---

**æœ‰é—®é¢˜?åŠ å…¥ [ç¬¬8ç«  Discord é¢‘é“](https://discord.gg/TODO) è®¨è®º!**
