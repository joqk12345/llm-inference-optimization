# ç¬¬7ç« : è¯·æ±‚è°ƒåº¦ç­–ç•¥

> **ğŸ’° æˆæœ¬å½±å“** (åŸºäºè¡Œä¸šæ•°æ®)
> - **ååæå‡**: Continuous Batching å¯å°†ååé‡æå‡ 3-10 å€
> - **å»¶è¿Ÿæ”¹å–„**: P95 å»¶è¿Ÿå¯é™ä½ 50-70%
> - **GPU åˆ©ç”¨ç‡**: ä» 30-40% æå‡åˆ° 80-90%

## ç®€ä»‹

åœ¨ç¬¬5ç« ä¸­,æˆ‘ä»¬å­¦ä¹ äº† Continuous Batching çš„åŸºæœ¬åŸç†â€”â€”é€šè¿‡åŠ¨æ€è°ƒæ•´ batch,æ¶ˆé™¤ padding æµªè´¹,è®© GPU æ—¶åˆ»æ»¡è½½ã€‚ä½†å¦‚ä½•é«˜æ•ˆå®ç° Continuous Batching?å¦‚ä½•å†³å®šå“ªäº›è¯·æ±‚å¯ä»¥ä¸€èµ·å¤„ç†?å¦‚ä½•å¹³è¡¡å»¶è¿Ÿå’Œååé‡?

è¿™å°±æ˜¯**è°ƒåº¦å™¨ (Scheduler)** çš„èŒè´£ã€‚è°ƒåº¦å™¨æ˜¯ vLLM çš„æ ¸å¿ƒç»„ä»¶,å†³å®šäº†æ¨ç†ç³»ç»Ÿçš„æ€§èƒ½ä¸Šé™ã€‚ä¸€ä¸ªä¼˜ç§€çš„è°ƒåº¦å™¨å¯ä»¥:
- åœ¨æœ‰é™çš„ GPU æ˜¾å­˜ä¸‹æœåŠ¡æ›´å¤šè¯·æ±‚
- é™ä½ P95 å»¶è¿Ÿ
- æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡
- æ”¯æŒ PD åˆ†ç¦»ç­‰é«˜çº§ç‰¹æ€§

æœ¬ç« å°†æ·±å…¥è®²è§£:
- ä¸ºä»€ä¹ˆéœ€è¦è°ƒåº¦,è°ƒåº¦çš„ç›®æ ‡æ˜¯ä»€ä¹ˆ
- åŸºç¡€è°ƒåº¦ç­–ç•¥ (FIFOã€Static Batching)
- Continuous Batching çš„åŸç†å’Œå®ç°
- vLLM çš„è°ƒåº¦å™¨å®ç° (è¿­ä»£çº§è°ƒåº¦ã€Overlap Scheduling)
- é«˜çº§è°ƒåº¦ç­–ç•¥ (ä¼˜å…ˆçº§ã€SJFã€è‡ªé€‚åº”)
- PD åˆ†ç¦» (Prefill-Decode åˆ†ç¦») çš„æ¶æ„æ¼”è¿›

**å­¦å®Œæœ¬ç« ,ä½ å°†èƒ½å¤Ÿè®¾è®¡å¹¶ä¼˜åŒ–è‡ªå·±çš„æ¨ç†è°ƒåº¦ç³»ç»Ÿã€‚**

---

## 7.1 è°ƒåº¦çš„å¿…è¦æ€§

### 7.1.1 ä¸ºä»€ä¹ˆéœ€è¦è°ƒåº¦

**åœºæ™¯**: å¤šä¸ªç”¨æˆ·åŒæ—¶å‘é€æ¨ç†è¯·æ±‚

```
æ—¶é—´çº¿:
t=0ms:  User A å‘é€è¯·æ±‚ (prompt: 100 tokens)
t=10ms: User B å‘é€è¯·æ±‚ (prompt: 50 tokens)
t=20ms: User C å‘é€è¯·æ±‚ (prompt: 200 tokens)

GPU èµ„æº:
- æ€»æ˜¾å­˜: 40GB (A100)
- æ¨¡å‹å ç”¨: 13GB (Llama-2-7B)
- å‰©ä½™: 27GB

é—®é¢˜:
1. ä¸‰ä¸ªè¯·æ±‚å¦‚ä½•æ’åº?
2. æ˜¯å¦å¯ä»¥å¹¶è¡Œå¤„ç†?
3. å¦‚ä½•é¿å…é•¿è¯·æ±‚é¥¿æ­»çŸ­è¯·æ±‚?
4. å¦‚ä½•æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡?
```

**æ²¡æœ‰è°ƒåº¦å™¨çš„é—®é¢˜**:
```
âŒ ä¸²è¡Œå¤„ç†:
  A â†’ B â†’ C
  User C ç­‰å¾…æ—¶é—´è¿‡é•¿ ( unfairness)

âŒ ç®€å•æ‰¹å¤„ç†:
  [A, B, C] ä¸€èµ·å¤„ç†
  éœ€è¦ç­‰å¾…æœ€æ…¢çš„è¯·æ±‚å®Œæˆ
  å¤§é‡ padding æµªè´¹

âŒ å…ˆæ¥å…ˆæœåŠ¡:
  é•¿è¯·æ±‚é˜»å¡çŸ­è¯·æ±‚
  P95 å»¶è¿Ÿé«˜
```

**è°ƒåº¦å™¨çš„ä»·å€¼**:
```
âœ… åŠ¨æ€è°ƒæ•´:
  æ ¹æ®è¯·æ±‚é•¿åº¦å’Œèµ„æºæƒ…å†µåŠ¨æ€è°ƒåº¦

âœ… å…¬å¹³æ€§:
  é¿å…é•¿è¯·æ±‚é¥¿æ­»çŸ­è¯·æ±‚

âœ… é«˜æ•ˆæ€§:
  æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡å’Œååé‡
```

---

### 7.1.2 æœåŠ¡è´¨é‡ vs ååé‡

**æœåŠ¡è´¨é‡ (Quality of Service, QoS)**:
- **å»¶è¿Ÿ**: TTFT (é¦–å­—å»¶è¿Ÿ)ã€TBT (å­—é—´å»¶è¿Ÿ)
- **å…¬å¹³æ€§**: æ‰€æœ‰è¯·æ±‚éƒ½èƒ½åŠæ—¶å¤„ç†
- **å¯é æ€§**: è¯·æ±‚ä¸è¶…æ—¶ã€ä¸ä¸¢å¤±

**ååé‡ (Throughput)**:
- å•ä½æ—¶é—´å†…å¤„ç†çš„è¯·æ±‚æ•°
- å•ä½æ—¶é—´å†…ç”Ÿæˆçš„ tokens æ•°

**æƒè¡¡æ›²çº¿**:
```
ååé‡
  â†‘
  â”‚     â•±
  â”‚    â•±  â† æœ€å¤§åŒ–åå (ç‰ºç‰²å»¶è¿Ÿ)
  â”‚   â•±
  â”‚  â•± â† æœ€ä½³å¹³è¡¡ç‚¹
  â”‚ â•±
  â”‚â•±     â† æœ€ä½å»¶è¿Ÿ (ç‰ºç‰²åå)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ å»¶è¿Ÿ
```

**è°ƒåº¦å™¨çš„ç›®æ ‡**: æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹

---

### 7.1.3 è°ƒåº¦å™¨çš„ç›®æ ‡

**ä¸»è¦ç›®æ ‡**:
1. âœ… **æœ€å°åŒ–å»¶è¿Ÿ**: P50ã€P95ã€P99 å»¶è¿Ÿå°½å¯èƒ½ä½
2. âœ… **æœ€å¤§åŒ–ååé‡**: åœ¨ç»™å®šç¡¬ä»¶ä¸ŠæœåŠ¡æ›´å¤šç”¨æˆ·
3. âœ… **å…¬å¹³æ€§**: é¿å…é•¿è¯·æ±‚é¥¿æ­»çŸ­è¯·æ±‚
4. âœ… **èµ„æºåˆ©ç”¨**: GPU åˆ©ç”¨ç‡ >80%

**æ¬¡è¦ç›®æ ‡**:
- ç®€å•æ€§: æ˜“äºç†è§£å’Œè°ƒè¯•
- å¯æ‰©å±•æ€§: æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²
- é²æ£’æ€§: å®¹å¿å¼‚å¸¸æƒ…å†µ

**è®¾è®¡åŸåˆ™**:
```
ä¼˜å…ˆçº§ 1: ä¸è¶…æ—¶ (SLA)
ä¼˜å…ˆçº§ 2: ä½å»¶è¿Ÿ (ç”¨æˆ·ä½“éªŒ)
ä¼˜å…ˆçº§ 3: é«˜åå (æˆæœ¬æ•ˆç‡)
ä¼˜å…ˆçº§ 4: ç®€å•å¯é  (è¿ç»´æˆæœ¬)
```

---

## 7.2 åŸºç¡€è°ƒåº¦ç­–ç•¥

### 7.2.1 FIFO (First In First Out)

**åŸç†**: æŒ‰è¯·æ±‚åˆ°è¾¾é¡ºåºå¤„ç†

```python
class FIFOScheduler:
    def __init__(self):
        self.queue = []

    def add_request(self, request):
        self.queue.append(request)

    def schedule(self):
        if self.queue:
            return [self.queue.pop(0)]  # è¿”å›ç¬¬ä¸€ä¸ªè¯·æ±‚
        return []
```

**ä¼˜ç‚¹**:
- âœ… å®ç°ç®€å•
- âœ… å…¬å¹³ (å…ˆæ¥å…ˆæœåŠ¡)
- âœ… æ— é¥¥é¥¿ (æ¯ä¸ªè¯·æ±‚æœ€ç»ˆéƒ½ä¼šè¢«å¤„ç†)

**ç¼ºç‚¹**:
- âŒ ååé‡ä½ (ä¸€æ¬¡åªå¤„ç†ä¸€ä¸ªè¯·æ±‚)
- âŒ GPU åˆ©ç”¨ç‡ä½ (~30-40%)
- âŒ é•¿è¯·æ±‚é˜»å¡åç»­æ‰€æœ‰è¯·æ±‚

**é€‚ç”¨åœºæ™¯**:
- å•ç”¨æˆ·ç¯å¢ƒ
- ä½å¹¶å‘åœºæ™¯
- å¯¹å…¬å¹³æ€§è¦æ±‚é«˜çš„åœºæ™¯

---

### 7.2.2 é™æ€æ‰¹å¤„ç† (Static Batching)

**åŸç†**: å°†å¤šä¸ªè¯·æ±‚æ‰“åŒ…æˆä¸€ä¸ªå›ºå®šå¤§å°çš„ batch

```python
class StaticBatchScheduler:
    def __init__(self, batch_size=8):
        self.batch_size = batch_size
        self.queue = []

    def add_request(self, request):
        self.queue.append(request)

    def schedule(self):
        if len(self.queue) >= self.batch_size:
            batch = self.queue[:self.batch_size]
            self.queue = self.queue[self.batch_size:]
            return batch
        return []
```

**Padding çš„é—®é¢˜**:
```
Batch ä¸­çš„è¯·æ±‚é•¿åº¦ä¸ä¸€è‡´:
Request A: 10 tokens
Request B: 50 tokens
Request C: 20 tokens

éœ€è¦ padding åˆ°æœ€é•¿:
Padded A: [padÃ—40][10 tokens]
Padded B: [50 tokens]
Padded C: [padÃ—30][20 tokens]

æµªè´¹: (40 + 0 + 30) / 100 = 70% padding!
```

**ä¼˜ç‚¹**:
- âœ… æé«˜ååé‡ (ç›¸æ¯” FIFO)
- âœ… GPU åˆ©ç”¨ç‡æå‡ (~60-70%)

**ç¼ºç‚¹**:
- âŒ å¤§é‡ padding æµªè´¹
- âŒ çŸ­è¯·æ±‚è¢«é•¿è¯·æ±‚é˜»å¡
- âŒ æ— æ³•åŠ¨æ€è°ƒæ•´

**é€‚ç”¨åœºæ™¯**:
- è¯·æ±‚é•¿åº¦ç›¸è¿‘çš„åœºæ™¯
- å¯¹å»¶è¿Ÿä¸æ•æ„Ÿçš„ç¦»çº¿æ‰¹å¤„ç†

---

### 7.2.3 ä¼˜ç¼ºç‚¹åˆ†æ

| ç­–ç•¥ | ååé‡ | å»¶è¿Ÿ | GPU åˆ©ç”¨ç‡ | å®ç°å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ |
|------|-------|------|-----------|-----------|---------|
| **FIFO** | ä½ | æœ€ä½ (å•è¯·æ±‚) | 30-40% | ç®€å• | ä½å¹¶å‘ |
| **Static Batching** | ä¸­ | é«˜ (ç­‰å¾… batch) | 60-70% | ç®€å• | ç¦»çº¿æ‰¹å¤„ç† |
| **Continuous Batching** | é«˜ | ä½ | 80-95% | ä¸­ç­‰ | ç”Ÿäº§ç¯å¢ƒ |

**ç»“è®º**: Continuous Batching æ˜¯ç”Ÿäº§ç¯å¢ƒçš„æœ€ä½³é€‰æ‹©

---

## 7.3 åŠ¨æ€æ‰¹å¤„ç† (Continuous Batching)

### 7.3.1 é—®é¢˜: é™æ€æ‰¹å¤„ç†çš„æµªè´¹

**åœºæ™¯å›é¡¾**:
```
Batch ä¸­æœ‰ 3 ä¸ªè¯·æ±‚:
Request A: 10 tokens (å·²ç”Ÿæˆ 100,è¿˜éœ€ 50)
Request B: 50 tokens (å·²ç”Ÿæˆ 200,è¿˜éœ€ 10)
Request C: 20 tokens (å·²ç”Ÿæˆ 150,è¿˜éœ€ 30)

é—®é¢˜ 1: Request B å®Œæˆå
  â†’ Batch ä¸­è¿˜æœ‰ A å’Œ C
  â†’ B çš„ä½ç½®ç©ºç€ (padding æµªè´¹)

é—®é¢˜ 2: æ–°è¯·æ±‚ D åˆ°è¾¾
  â†’ å¿…é¡»ç­‰å¾…æ•´ä¸ª batch å®Œæˆ
  â†’ å»¶è¿Ÿé«˜

é—®é¢˜ 3: Batch ä¸­è¯·æ±‚é•¿åº¦å·®å¼‚å¤§
  â†’ å¤§é‡ padding
  â†’ GPU è®¡ç®—æµªè´¹
```

**æµªè´¹é‡åŒ–**:
```
å‡è®¾ batch_size = 8,æ¯ä¸ªè¯·æ±‚å¹³å‡ç”Ÿæˆ 100 tokens

Static Batching:
- éœ€è¦ç­‰å¾…æ‰€æœ‰ 8 ä¸ªè¯·æ±‚å®Œæˆ
- P95 å»¶è¿Ÿ = æœ€æ…¢è¯·æ±‚çš„å®Œæˆæ—¶é—´
- Padding æ¯”ä¾‹ = 50-70%
```

---

### 7.3.2 Continuous Batching åŸç†

**æ ¸å¿ƒæ€æƒ³**:
1. **å»æ‰ batch ç»´åº¦**,ç”¨ attention mask æ§åˆ¶ token äº¤äº’
2. **åŠ¨æ€æ›¿æ¢å®Œæˆçš„è¯·æ±‚**,ç«‹å³åŠ å…¥æ–°è¯·æ±‚
3. **æ··åˆ Prefill å’Œ Decode**,æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡

**Ragged Batching**:
```python
# æ‹¼æ¥æ‰€æœ‰è¯·æ±‚çš„ tokens
tokens = [
    # Request A (3 tokens)
    A1, A2, A3,
    # Request B (2 tokens)
    B1, B2,
    # Request C (4 tokens)
    C1, C2, C3, C4,
]

# Attention mask: å—å¯¹è§’çŸ©é˜µ
mask = [
    [1, 0, 0, 0, 0, 0, 0, 0, 0],  # A1
    [1, 1, 0, 0, 0, 0, 0, 0, 0],  # A2
    [1, 1, 1, 0, 0, 0, 0, 0, 0],  # A3
    [0, 0, 0, 1, 0, 0, 0, 0, 0],  # B1
    [0, 0, 0, 1, 1, 0, 0, 0, 0],  # B2
    [0, 0, 0, 0, 0, 1, 0, 0, 0],  # C1
    [0, 0, 0, 0, 0, 1, 1, 0, 0],  # C2
    [0, 0, 0, 0, 0, 1, 1, 1, 0],  # C3
    [0, 0, 0, 0, 0, 1, 1, 1, 1],  # C4
]
```

**åŠ¨æ€æ›¿æ¢**:
```python
def continuous_batching_step(scheduled, running, completed):
    """
    scheduled: ç­‰å¾…è°ƒåº¦çš„è¯·æ±‚
    running: æ­£åœ¨è¿è¡Œçš„è¯·æ±‚
    completed: åˆšå®Œæˆçš„è¯·æ±‚
    """
    # 1. ç§»é™¤å®Œæˆçš„è¯·æ±‚
    for req in completed:
        running.remove(req)

    # 2. ä»ç­‰å¾…é˜Ÿåˆ—ä¸­åŠ å…¥æ–°è¯·æ±‚
    num_slots = batch_size - len(running)
    for i in range(num_slots):
        if scheduled:
            new_req = scheduled.pop(0)
            running.append(new_req)

    # 3. é‡æ–°æ„å»º attention mask
    mask = build_ragged_mask(running)

    return running, mask
```

---

### 7.3.3 å›¾è§£å·¥ä½œæµç¨‹

**è¿­ä»£çº§è°ƒåº¦ (Iteration-level Scheduling)**:

```
æ—¶é—´çº¿ (æ¯æ¬¡è¿­ä»£ ~10ms):

Iter 1:
  Running: [Req A (100â†’101), Req B (50â†’51), Req C (200â†’201)]
  GPU: å¤„ç† 3 ä¸ªè¯·æ±‚,å„ç”Ÿæˆ 1 ä¸ª token

Iter 2:
  Running: [Req A (101â†’102), Req B (52â†’53), Req C (201â†’202)]
  GPU: å¤„ç† 3 ä¸ªè¯·æ±‚,å„ç”Ÿæˆ 1 ä¸ª token

Iter 3:
  Req B å®Œæˆ (ç”Ÿæˆ <eos>)
  Running: [Req A (102â†’103), Req C (202â†’203)]
  ç©ºå‡º 1 ä¸ª slot
  Scheduled: [Req D (æ–°è¯·æ±‚,éœ€è¦ prefill)]
  Action: ç”¨ Req D æ›¿æ¢ Req B
  Running: [Req A (103â†’104), Req C (203â†’204), Req D (prefillâ†’1)]
  GPU: å¤„ç† 3 ä¸ªè¯·æ±‚ (decode + decode + prefill)

Iter 4:
  Running: [Req A (104â†’105), Req C (204â†’205), Req D (1â†’2)]
  GPU: å¤„ç† 3 ä¸ªè¯·æ±‚

Iter 5:
  Req A å’Œ Req D åŒæ—¶å®Œæˆ
  Running: [Req C (205â†’206)]
  ç©ºå‡º 2 ä¸ª slots
  Scheduled: [Req E, Req F]
  Action: åŠ å…¥ Req E å’Œ Req F
  Running: [Req C (206â†’207), Req E (prefillâ†’1), Req F (prefillâ†’1)]
  GPU: å¤„ç† 3 ä¸ªè¯·æ±‚
```

**å…³é”®è§‚å¯Ÿ**:
- GPU æ—¶åˆ»ä¿æŒæ»¡è½½ (3 ä¸ªè¯·æ±‚)
- å®Œæˆçš„è¯·æ±‚ç«‹å³è¢«æ›¿æ¢
- Prefill å’Œ Decode æ··åˆå¤„ç†
- æ—  padding æµªè´¹

---

### 7.3.4 æ€§èƒ½æå‡åˆ†æ

**ååé‡æå‡**:
```
å‡è®¾:
- GPU æ¯æ¬¡è¿­ä»£å¯å¤„ç† 1024 tokens
- å¹³å‡è¯·æ±‚é•¿åº¦: 100 tokens

Static Batching:
- Batch size: 8
- æ¯ä¸ªè¿­ä»£: 8 ä¸ªè¯·æ±‚ (å„ 1 token)
- æ¯ 100 ä¸ªè¿­ä»£å®Œæˆä¸€æ‰¹ (8 ä¸ªè¯·æ±‚)
- ååé‡: 8 requests / 100 iterations = 0.08 req/iter

Continuous Batching:
- è¿­ä»£ 1-10: Req A-D (prefill)
- è¿­ä»£ 11-20: Req E-H (decode)
- è¿­ä»£ 21: Req A å®Œæˆ,åŠ å…¥ Req I (prefill)
- è¿­ä»£ 22-30: Req E-I (decode)
- ...
- ååé‡: ~0.25 req/iter (3x æå‡!)
```

**å»¶è¿Ÿæ”¹å–„**:
```
å‡è®¾:
- 100 ä¸ªè¯·æ±‚æ’é˜Ÿ
- Batch size: 8

Static Batching:
- ç¬¬ 100 ä¸ªè¯·æ±‚éœ€è¦ç­‰å¾… 12 ä¸ª batch
- P95 å»¶è¿Ÿ: 12 Ã— 100 iterations = 1200 iterations

Continuous Batching:
- ç¬¬ 100 ä¸ªè¯·æ±‚ç­‰å¾… ~3 ä¸ª batch
- P95 å»¶è¿Ÿ: 3 Ã— 100 iterations = 300 iterations

æ”¹å–„: 1200 / 300 = 4x
```

**GPU åˆ©ç”¨ç‡**:
```
Static Batching:
  - Padding: 50-70%
  - GPU åˆ©ç”¨ç‡: 30-50%

Continuous Batching:
  - Padding: 0-5%
  - GPU åˆ©ç”¨ç‡: 80-95%
```

---

## 7.4 vLLM çš„è°ƒåº¦å™¨å®ç°

### 7.4.1 è¯·æ±‚ç”Ÿå‘½å‘¨æœŸç®¡ç†

**çŠ¶æ€æœº**:
```
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   Waiting   â”‚  â† ç­‰å¾…è°ƒåº¦
                            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ schedule()
                                   â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  Scheduled  â”‚  â† å·²è°ƒåº¦,ç­‰å¾…æ‰§è¡Œ
                            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ execute()
                                   â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”Œâ”€â”€â”€â”€â†’â”‚   Running   â”‚  â† æ­£åœ¨æ‰§è¡Œ
                      â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                      â”‚            â”‚
                      â”‚            â”‚ generate token
                      â”‚            â–¼
                      â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚     â”‚  Decoding   â”‚  â† ç”Ÿæˆä¸­
                      â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                      â”‚            â”‚
                      â”‚            â”‚ complete / abort
                      â”‚            â–¼
                      â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â””â”€â”€â”€â”€â”€â”‚  Finished   â”‚  â† å®Œæˆ/ä¸­æ–­
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**vLLM çš„è¯·æ±‚å¯¹è±¡**:
```python
class Sequence:
    def __init__(self, request_id, prompt):
        self.request_id = request_id
        self.prompt = prompt
        self.prompt_tokens = tokenize(prompt)
        self.output_tokens = []

        # çŠ¶æ€
        self.status = "waiting"  # waiting, running, finished

        # KV Cache
        self.block_table = []  # Physical blocks

        # å…ƒæ•°æ®
        self.arrival_time = time.time()
        self.start_time = None
        self.finish_time = None
```

---

### 7.4.2 é¢„åˆ†é… vs åŠ¨æ€åˆ†é…

**é¢„åˆ†é… (Pre-allocation)**:
```python
# ä¼ ç»Ÿæ–¹æ³•: é¢„åˆ†é…æœ€å¤§ç©ºé—´
def allocate_max_space(request):
    max_tokens = request.max_new_tokens
    prompt_tokens = len(request.prompt_tokens)
    total = prompt_tokens + max_tokens
    # é¢„åˆ†é… total tokens çš„ç©ºé—´
    return allocate_blocks(total)
```

**é—®é¢˜**:
- æµªè´¹æ˜¾å­˜ (å¤§å¤šæ•°è¯·æ±‚ä¸ä¼šè¾¾åˆ° max_new_tokens)
- é™åˆ¶å¹¶å‘æ•°

**åŠ¨æ€åˆ†é… (Dynamic Allocation)**:
```python
# vLLM æ–¹æ³•: åŠ¨æ€å¢é•¿
def allocate_dynamic(request):
    # åˆå§‹åˆ†é…: prompt + å°‘é‡ decode
    initial = len(request.prompt_tokens) + 16
    blocks = allocate_blocks(initial)

    # åŠ¨æ€å¢é•¿
    while need_more_space(request):
        new_blocks = allocate_blocks(16)
        blocks.extend(new_blocks)

    return blocks
```

**ä¼˜åŠ¿**:
- èŠ‚çœæ˜¾å­˜ (30-50%)
- æé«˜å¹¶å‘æ•°
- æ”¯æŒ max_new_tokens å¾ˆå¤§çš„åœºæ™¯

---

### 7.4.3 è¿­ä»£çº§è°ƒåº¦ (Iteration-level Scheduling)

**å®šä¹‰**: æ¯æ¬¡è¿­ä»£ (iteration) é‡æ–°è°ƒåº¦ä¸€æ¬¡

```python
class Scheduler:
    def schedule(self):
        """æ¯æ¬¡è¿­ä»£è°ƒç”¨"""
        scheduled = self._schedule()
        self.model_executor.execute_model(scheduled)
        self._process_outputs()

    def _schedule(self):
        """å†³å®šå“ªäº›è¯·æ±‚å¯ä»¥æ‰§è¡Œ"""
        scheduled = []

        # 1. ä» running ä¸­é€‰æ‹©
        for seq in self.running:
            if self._can_schedule(seq):
                scheduled.append(seq)

        # 2. ä» waiting ä¸­é€‰æ‹©
        for seq in self.waiting:
            if self._can_schedule(seq):
                scheduled.append(seq)
                self.running.append(seq)
                self.waiting.remove(seq)

        return scheduled

    def _can_schedule(self, seq):
        """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„èµ„æº"""
        # 1. æ£€æŸ¥ KV Cache ç©ºé—´
        required_blocks = estimate_blocks(seq)
        if len(self.free_blocks) < required_blocks:
            return False

        # 2. æ£€æŸ¥ GPU è®¡ç®—
        # (CUDA æ”¯æŒå¹¶å‘,é€šå¸¸ä¸æ˜¯ç“¶é¢ˆ)
        return True
```

**è°ƒåº¦æµç¨‹**:
```
æ¯æ¬¡è¿­ä»£:
  1. è°ƒåº¦å™¨å†³å®šå“ªäº›è¯·æ±‚å¯ä»¥æ‰§è¡Œ
  2. å‡†å¤‡è¾“å…¥æ•°æ®
  3. å¯åŠ¨ GPU kernel
  4. GPU æ‰§è¡Œæ¨ç†
  5. è·å–è¾“å‡º
  6. æ›´æ–°è¯·æ±‚çŠ¶æ€
  7. å›åˆ°æ­¥éª¤ 1
```

---

### 7.4.4 Overlap Scheduling (Mini-SGLang) âš¡ï¸ 2025 æ–°å¢

> **ğŸ’¡ æ·±åº¦æ¥æº**: [Mini-SGLang Blog](https://lmsys.org/blog/2025-12-17-minisgl/) + [Berkeley EECS-2025-192](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-192.pdf)
>
> **æ ¸å¿ƒé—®é¢˜**: Berkeley è®ºæ–‡æŒ‡å‡º CPU overhead å¯¼è‡´ GPU é—²ç½® â†’ Overlap Scheduling æ˜¯è§£å†³æ–¹æ¡ˆ
>
> **æ€§èƒ½æå‡**: æ¶ˆé™¤ GPU stalls,æå‡ååé‡ 20-30%

---

#### 7.4.4.1 CPU å¼€é”€å¯¼è‡´ GPU é—²ç½®é—®é¢˜

**Berkeley EECS-2025-192 çš„å‘ç°**:
- CPU å¼€é”€å æ¨ç†æ—¶é—´çš„ **10-20%**
- ä¸»è¦æ¥æº:
  - Kernel launch (å¯åŠ¨ GPU kernel)
  - Memory copy (CPUâ†”GPU æ•°æ®ä¼ è¾“)
  - Synchronization (ç­‰å¾… GPU å®Œæˆ)
  - Batch scheduling (å†³å®šå“ªäº›è¯·æ±‚ä¸€èµ·å¤„ç†)

**é—®é¢˜**:
- vLLM çš„è¿­ä»£çº§è°ƒåº¦æ˜¯ **ä¸²è¡Œ** çš„:
  ```
  Step 1: CPU è°ƒåº¦ä¸‹ä¸€æ‰¹è¯·æ±‚
  Step 2: CPU å‡†å¤‡è¾“å…¥æ•°æ®
  Step 3: CPU å¯åŠ¨ GPU kernel
  Step 4: GPU è®¡ç®— (æ­¤æ—¶ CPU é—²ç½®!)
  Step 5: CPU ç­‰å¾… GPU å®Œæˆ
  Step 6: å›åˆ° Step 1
  ```
- ç»“æœ: **GPU åˆ©ç”¨ç‡ä½**,æœ‰æ˜æ˜¾çš„ GPU stalls

**Nsight Systems åˆ†æ** (æ—  overlap):
```
Timeline:
CPU: |--Schedule1--|--Prepare2--|--Launch3--|
GPU:              |<--Compute1-->|    stalls    |
```
çœ‹åˆ° GPU æœ‰æ˜æ˜¾çš„é—²ç½®æœŸ (stalls)

---

#### 7.4.4.2 Overlap Scheduling è®¾è®¡æ€æƒ³

**æ ¸å¿ƒæ€æƒ³**:
- **CPU-GPU å¹¶è¡Œæ‰§è¡Œ**:
  - CPU å‡†å¤‡ä¸‹ä¸€æ‰¹è¯·æ±‚æ—¶,GPU æ­£åœ¨è®¡ç®—å½“å‰æ‰¹æ¬¡
  - GPU è®¡ç®—å®Œæˆå,ä¸‹ä¸€æ‰¹è¯·æ±‚å·²ç» ready,ç«‹å³å¼€å§‹
- **ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼**:
  - CPU: ç”Ÿäº§è€… (å‡†å¤‡ batches)
  - GPU: æ¶ˆè´¹è€… (æ‰§è¡Œ batches)

**å¯¹æ¯”**:
```
æ—  Overlap (vLLM é»˜è®¤):
CPU: |--Schedule--|--Prepare--|
GPU:                 |--Compute--|<-stall->|--Compute--|

æœ‰ Overlap (Mini-SGLang):
CPU: |--Schedule1--|--Prepare2--|--Prepare3--|
GPU:                 |--Compute1-->|--Compute2-->|
```
GPU æŒç»­è¿è¡Œ,æ— é—²ç½®!

---

#### 7.4.4.3 å®ç°æœºåˆ¶

**æ¶æ„è®¾è®¡**:
```python
class OverlapScheduler:
    def __init__(self):
        self.cpu_queue = Queue()  # CPU å‡†å¤‡çš„è¯·æ±‚é˜Ÿåˆ—
        self.gpu_queue = Queue()  # GPU å¾…æ‰§è¡Œçš„é˜Ÿåˆ—
        self.cpu_thread = Thread(target=self._cpu_worker)
        self.gpu_thread = Thread(target=self._gpu_worker)

    def start(self):
        """å¯åŠ¨ CPU å’Œ GPU çº¿ç¨‹"""
        self.cpu_thread.start()
        self.gpu_thread.start()

    def _cpu_worker(self):
        """CPU çº¿ç¨‹: å‡†å¤‡ batches"""
        while True:
            # è°ƒåº¦ä¸‹ä¸€æ‰¹è¯·æ±‚
            scheduled = self._schedule_next_batch()

            # å‡†å¤‡è¾“å…¥æ•°æ®
            inputs = self._prepare_inputs(scheduled)

            # æ”¾å…¥ GPU é˜Ÿåˆ—
            self.gpu_queue.put(inputs)

    def _gpu_worker(self):
        """GPU çº¿ç¨‹: æ‰§è¡Œ batches"""
        while True:
            # ä»é˜Ÿåˆ—è·å– (é˜»å¡ç­‰å¾…)
            inputs = self.gpu_queue.get()

            # æ‰§è¡Œæ¨ç†
            outputs = self.model_executor.execute(inputs)

            # å¤„ç†è¾“å‡º
            self._process_outputs(outputs)
```

**å…³é”®ä¼˜åŒ–**:
1. **Pipeline æ·±åº¦**: é€šå¸¸ 2-3 ä¸ª batches çš„ pipeline
2. **åŒæ­¥æœºåˆ¶**: ä½¿ç”¨æ¡ä»¶å˜é‡é¿å… busy waiting
3. **å†…å­˜ç®¡ç†**: é¢„åˆ†é… buffers é¿å…è¿è¡Œæ—¶åˆ†é…

---

#### 7.4.4.4 æ€§èƒ½æå‡

**ååé‡æå‡**:
```
æ—  Overlap:
- CPU å¼€é”€: 15%
- GPU stalls: 10%
- æœ‰æ•ˆè®¡ç®—: 75%
- ååé‡: 100 req/s

æœ‰ Overlap:
- CPU å¼€é”€: 5% (å¹¶è¡ŒåŒ–)
- GPU stalls: 0% (æ— é—²ç½®)
- æœ‰æ•ˆè®¡ç®—: 95%
- ååé‡: 126 req/s (1.26x æå‡)
```

**å»¶è¿Ÿæ”¹å–„**:
```
P95 å»¶è¿Ÿé™ä½ 20-30%
- CPU å‡†å¤‡æ—¶é—´ä¸é˜»å¡ GPU
- è¯·æ±‚æ›´å¿«å¼€å§‹å¤„ç†
```

---

#### 7.4.4.5 vLLM çš„å®ç°çŠ¶æ€

**å½“å‰çŠ¶æ€** (v0.6.x):
- âœ… æ”¯æŒ iteration-level scheduling
- âš ï¸ éƒ¨åˆ†æ”¯æŒ overlap (experimental)
- ğŸš§ æœªæ¥ç‰ˆæœ¬ä¼šå®Œå…¨æ”¯æŒ

**å¦‚ä½•å¯ç”¨** (å®éªŒæ€§):
```python
from vllm import LLM

llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    enable_overlap_schedule=True,  # å®éªŒæ€§åŠŸèƒ½
)
```

---

### 7.4.5 Dynamic Memory Management (åŠ¨æ€å†…å­˜ç®¡ç†)

> **ğŸ’¡ æ¥æº**: SGLang v0.2 æ ¸å¿ƒç‰¹æ€§
>
> **é—®é¢˜**: é¢„ç•™ max_new_tokens çš„ç©ºé—´æµªè´¹å¤§é‡å†…å­˜
>
> **è§£å†³**: æ ¹æ®å®é™…ä½¿ç”¨æƒ…å†µåŠ¨æ€è°ƒæ•´é¢„ç•™å¤§å°

**æ ¸å¿ƒé—®é¢˜**:
```python
# ç”¨æˆ·è®¾ç½®
max_new_tokens = 2048

# ä¼ ç»Ÿåšæ³•: é¢„ç•™ 2048 tokens çš„ç©ºé—´
reserved = 2048

# å®é™…æƒ…å†µ: å¤§å¤šæ•°è¯·æ±‚åªç”Ÿæˆ 500 tokens
actual = 500

# æµªè´¹: 2048 - 500 = 1548 tokens (75% æµªè´¹!)
```

**Dynamic Memory Management**:
```python
class DynamicMemoryManager:
    def __init__(self, initial_beta=0.5):
        """
        beta: é¢„ç•™æ¯”ä¾‹
              åˆå§‹: 0.5 (é¢„ç•™ 50% çš„ max_new_tokens)
        """
        self.beta = initial_beta
        self.actual_usage_history = []

    def allocate(self, prompt_len, max_new_tokens):
        """åŠ¨æ€åˆ†é…å†…å­˜"""
        # é¢„ç•™: prompt + (beta Ã— max_new_tokens)
        reserved = int(prompt_len + self.beta * max_new_tokens)
        blocks = allocate_blocks(reserved)
        return blocks

    def on_token_generated(self, seq):
        """ç”Ÿæˆæ–° token æ—¶è°ƒç”¨"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å±•
        current_tokens = len(seq.output_tokens)
        max_tokens = seq.max_new_tokens

        if current_tokens > self.beta * max_tokens * 0.8:
            # å³å°†åˆ°è¾¾é¢„ç•™ä¸Šé™,æ‰©å±•
            self._expand_reservation(seq)

    def on_request_complete(self, seq):
        """è¯·æ±‚å®Œæˆæ—¶è°ƒç”¨"""
        # è®°å½•å®é™…ä½¿ç”¨æƒ…å†µ
        actual_tokens = len(seq.output_tokens)
        max_tokens = seq.max_new_tokens
        usage_ratio = actual_tokens / max_tokens
        self.actual_usage_history.append(usage_ratio)

        # åªä¿ç•™æœ€è¿‘ 100 ä¸ªè¯·æ±‚çš„å†å²
        if len(self.actual_usage_history) > 100:
            self.actual_usage_history.pop(0)

    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.actual_usage_history:
            return {}

        return {
            'beta': self.beta,
            'avg_usage_ratio': sum(self.actual_usage_history) / len(self.actual_usage_history),
            'memory_saved_pct': (1 - self.beta) * 100
        }
```

**å·¥ä½œæµç¨‹**:
```
è¯·æ±‚åˆ°æ¥æ—¶:
  1. ç”¨æˆ·è¯·æ±‚: prompt=1000 tokens, max_new_tokens=2048

  2. ä¼ ç»Ÿåšæ³•:
     é¢„ç•™: 1000 + 2048 = 3048 tokens çš„ KV Cache

  3. Dynamic Memory Management:
     é¢„ç•™: 1000 + (0.5 Ã— 2048) = 1000 + 1024 = 2024 tokens
     (Î²=0.5,èŠ‚çœ 33% å†…å­˜)

è¯·æ±‚è¿›è¡Œä¸­:
  1. è¯·æ±‚å·²ç”Ÿæˆ 600 tokens
  2. å‘ç°å³å°†åˆ°è¾¾ max_new_tokens çš„ 30%
  3. åŠ¨æ€æ‰©å±•é¢„ç•™: 1024 â†’ 1433 tokens
  4. å¦‚æœ GPU å†…å­˜ä¸è¶³,ç­‰å¾…å…¶ä»–è¯·æ±‚å®Œæˆ

è¯·æ±‚å®Œæˆæ—¶:
  1. è¯·æ±‚åœ¨ 600 tokens æ—¶é‡åˆ° EOS
  2. é‡Šæ”¾æ‰€æœ‰ KV Cache (1000 + 600 = 1600 tokens)
  3. è®°å½•å®é™…ä½¿ç”¨ç‡: 600 / 2048 = 29.3%
  4. æ›´æ–° Î²: 0.5 â†’ 0.35 (æ ¹æ®å†å²å¹³å‡)
  5. ä¸‹æ¬¡è¯·æ±‚åªé¢„ç•™: 1000 + (0.35 Ã— 2048) = 1716 tokens
```

**æ€§èƒ½æå‡**:
```
å†…å­˜èŠ‚çœ:
  åœºæ™¯        | ä¼ ç»Ÿåšæ³• | åŠ¨æ€ç®¡ç† | èŠ‚çœ
  Chat (500)  | 3048     | 2024     | 33%
  RAG (800)   | 3048     | 2240     | 27%
  Code (1200) | 3048     | 2640     | 13%

ååé‡æå‡:
  æ›´å¤§çš„ batch size (å› ä¸ºå†…å­˜èŠ‚çœ)
  å®æµ‹: 1.5-2x throughput æå‡
```

---

## 7.5 é«˜çº§è°ƒåº¦ç­–ç•¥

### 7.5.1 ä¼˜å…ˆçº§è°ƒåº¦

**åŸç†**: ä¸åŒè¯·æ±‚æœ‰ä¸åŒä¼˜å…ˆçº§

```python
class PriorityScheduler:
    def __init__(self):
        # å¤šä¸ªé˜Ÿåˆ—,ä¸åŒä¼˜å…ˆçº§
        self.queues = {
            'high': [],    # é«˜ä¼˜å…ˆçº§ (VIP ç”¨æˆ·)
            'normal': [],  # æ­£å¸¸ä¼˜å…ˆçº§
            'low': [],     # ä½ä¼˜å…ˆçº§ (å…è´¹ç”¨æˆ·)
        }

    def add_request(self, request, priority='normal'):
        self.queues[priority].append(request)

    def schedule(self):
        # ä¼˜å…ˆå¤„ç†é«˜ä¼˜å…ˆçº§é˜Ÿåˆ—
        if self.queues['high']:
            return [self.queues['high'].pop(0)]
        elif self.queues['normal']:
            return [self.queues['normal'].pop(0)]
        else:
            return [self.queues['low'].pop(0)]
```

**åº”ç”¨åœºæ™¯**:
- VIP ç”¨æˆ· vs æ™®é€šç”¨æˆ·
- ä»˜è´¹ç”¨æˆ· vs å…è´¹ç”¨æˆ·
- å®æ—¶è¯·æ±‚ vs ç¦»çº¿æ‰¹å¤„ç†

---

### 7.5.2 æœ€çŸ­ä½œä¸šä¼˜å…ˆ (SJF)

**åŸç†**: ä¼˜å…ˆå¤„ç†é¢„è®¡å®Œæˆæ—¶é—´æœ€çŸ­çš„è¯·æ±‚

```python
class SJFScheduler:
    def schedule(self, pending_requests):
        # æŒ‰é¢„è®¡å®Œæˆæ—¶é—´æ’åº
        sorted_requests = sorted(
            pending_requests,
            key=lambda r: r.estimated_duration()
        )
        # è¿”å›å‰ N ä¸ª
        return sorted_requests[:batch_size]
```

**ä¼˜åŠ¿**:
- âœ… é™ä½å¹³å‡å»¶è¿Ÿ
- âœ… æé«˜ååé‡

**åŠ£åŠ¿**:
- âŒ å¯èƒ½é¥¿æ­»é•¿è¯·æ±‚
- âŒ éœ€è¦å‡†ç¡®ä¼°è®¡è¯·æ±‚é•¿åº¦

**æ”¹è¿›**: Shortest Remaining Time First (SRTF)
- åŠ¨æ€é‡æ–°è¯„ä¼°
- è€ƒè™‘å·²æ‰§è¡Œçš„æ—¶é—´

---

### 7.5.3 è½®è¯¢è°ƒåº¦ (Round Robin)

**åŸç†**: å…¬å¹³åœ°è½®è½¬å¤„ç†æ¯ä¸ªé˜Ÿåˆ—

```python
class RoundRobinScheduler:
    def __init__(self, time_slice=10):
        self.time_slice = time_slice  # æ¯ä¸ª queue çš„æ—¶é—´ç‰‡
        self.queues = {
            'queue1': [],
            'queue2': [],
            'queue3': [],
        }
        self.current_queue = 0
        self.timer = 0

    def schedule(self):
        # æ—¶é—´ç‰‡ç”¨å®Œ,åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªé˜Ÿåˆ—
        if self.timer >= self.time_slice:
            self.current_queue = (self.current_queue + 1) % len(self.queues)
            self.timer = 0

        # ä»å½“å‰é˜Ÿåˆ—å–è¯·æ±‚
        queue_name = list(self.queues.keys())[self.current_queue]
        if self.queues[queue_name]:
            self.timer += 1
            return [self.queues[queue_name].pop(0)]

        return []
```

**ä¼˜åŠ¿**:
- âœ… ç»å¯¹å…¬å¹³
- âœ… æ— é¥¥é¥¿

**åŠ£åŠ¿**:
- âŒ ä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€
- âŒ å¯èƒ½é™ä½ååé‡

---

### 7.5.4 è‡ªé€‚åº”è°ƒåº¦

**åŸç†**: æ ¹æ®ç³»ç»ŸçŠ¶æ€åŠ¨æ€è°ƒæ•´è°ƒåº¦ç­–ç•¥

```python
class AdaptiveScheduler:
    def __init__(self):
        self.strategies = {
            'low_load': FIFOScheduler(),
            'high_load': ContinuousBatchScheduler(),
            'mixed': PriorityScheduler(),
        }
        self.current_strategy = None

    def schedule(self):
        # ç›‘æ§ç³»ç»ŸçŠ¶æ€
        load = self.get_system_load()
        queue_length = len(self.waiting_queue)

        # æ ¹æ®çŠ¶æ€é€‰æ‹©ç­–ç•¥
        if load < 0.3:
            self.current_strategy = self.strategies['low_load']
        elif load > 0.8:
            self.current_strategy = self.strategies['high_load']
        else:
            self.current_strategy = self.strategies['mixed']

        return self.current_strategy.schedule()
```

**ä¼˜åŠ¿**:
- âœ… é€‚åº”ä¸åŒå·¥ä½œè´Ÿè½½
- âœ… è‡ªåŠ¨ä¼˜åŒ–

**æŒ‘æˆ˜**:
- âš ï¸ ç­–ç•¥åˆ‡æ¢å¼€é”€
- âš ï¸ å‚æ•°è°ƒä¼˜å¤æ‚

---

## 7.6 å®æˆ˜é…ç½®

### 7.6.1 vLLM è°ƒåº¦å‚æ•°è°ƒä¼˜

**å…³é”®å‚æ•°**:
```bash
vllm serve meta-llama/Llama-2-7b-hf \
  # Batch ç›¸å…³
  --max-num-batched-tokens 8192 \        # æ¯æ¬¡ iteration æœ€å¤§ tokens
  --max-num-seqs 256 \                    # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°

  # Memory ç›¸å…³
  --gpu-memory-utilization 0.9 \         # GPU å†…å­˜åˆ©ç”¨ç‡
  --block-size 16 \                       # PagedAttention block å¤§å°

  # è°ƒåº¦ç›¸å…³
  --max-paddings 256 \                    # æœ€å¤§ padding æ•°é‡
  --schedule-policy "fcfs" \              # è°ƒåº¦ç­–ç•¥ (fcfs/priority)
```

**è°ƒä¼˜å»ºè®®**:
```
åœºæ™¯ 1: ä½å»¶è¿Ÿä¼˜å…ˆ
  --max-num-batched-tokens 4096  # å‡å° batch size
  --max-num-seqs 64              # å‡å°‘å¹¶å‘

åœºæ™¯ 2: é«˜ååä¼˜å…ˆ
  --max-num-batched-tokens 16384 # å¢å¤§ batch size
  --max-num-seqs 512             # å¢åŠ å¹¶å‘

åœºæ™¯ 3: æ··åˆå·¥ä½œè´Ÿè½½
  --max-num-batched-tokens 8192  # å¹³è¡¡
  --schedule-policy "priority"   # å¯ç”¨ä¼˜å…ˆçº§
```

---

### 7.6.2 ä¸åŒåœºæ™¯çš„è°ƒåº¦ç­–ç•¥

**åœºæ™¯ 1: Chatbot æœåŠ¡**
```
ç‰¹å¾:
  - å¤§é‡çŸ­è¯·æ±‚
  - ç”¨æˆ·æ•æ„Ÿå»¶è¿Ÿ

æ¨èé…ç½®:
  - Continuous Batching
  - è¾ƒå°çš„ batch size (å‡å°‘ç­‰å¾…)
  - FIFO ä¼˜å…ˆ (å…¬å¹³æ€§)

å‚æ•°:
  --max-num-batched-tokens 4096
  --max-num-seqs 128
  --schedule-policy "fcfs"
```

**åœºæ™¯ 2: RAG åº”ç”¨**
```
ç‰¹å¾:
  - é•¿ prompt (æ–‡æ¡£å†…å®¹)
  - çŸ­è¾“å‡º (ç­”æ¡ˆ)
  - é«˜ Prefill æ¯”ä¾‹

æ¨èé…ç½®:
  - Prefix Caching (ç¼“å­˜æ–‡æ¡£)
  - è¾ƒå¤§çš„ batch size (Prefill é˜¶æ®µ)
  - ä¼˜å…ˆçº§è°ƒåº¦ (VIP ç”¨æˆ·)

å‚æ•°:
  --enable-prefix-caching
  --max-num-batched-tokens 16384
  --schedule-policy "priority"
```

**åœºæ™¯ 3: æ‰¹é‡å¤„ç†**
```
ç‰¹å¾:
  - ç¦»çº¿ä»»åŠ¡
  - ä¸æ•æ„Ÿå»¶è¿Ÿ
  - è¿½æ±‚ååé‡

æ¨èé…ç½®:
  - å¤§ batch size
  - Static Batching (å¯ä»¥æ¥å—)
  - SJF è°ƒåº¦ (æœ€å°åŒ–å¹³å‡å®Œæˆæ—¶é—´)

å‚æ•°:
  --max-num-batched-tokens 32768
  --max-num-seqs 512
```

---

## 7.7 Prefill-Decode åˆ†ç¦» (PD åˆ†ç¦») âš ï¸ æŠ€æœ¯è¯„ä¼°ä¸­

> **ğŸ’¡ 2025 å¹´æŠ€æœ¯è¶‹åŠ¿**: PD åˆ†ç¦»åœ¨ 2025 å¹´ä»æ¦‚å¿µå¿«é€Ÿæ¼”è¿›ä¸ºç”Ÿäº§æ ‡å‡†ã€‚vLLMã€SGLang ç­‰ä¸»æµæ¡†æ¶éƒ½å·²æ”¯æŒ,å‡ ä¹æ‰€æœ‰å‚å•†éƒ½åœ¨é‡‡ç”¨è¿™ç§æ¶æ„ã€‚

### 7.7.1 ä»€ä¹ˆæ˜¯ PD åˆ†ç¦»

**Prefill é˜¶æ®µ**: å¹¶è¡Œå¤„ç† prompt,è®¡ç®—å¯†é›†
- è¾“å…¥: æ•´ä¸ª prompt
- è®¡ç®—: çŸ©é˜µä¹˜æ³•ä¸ºä¸»
- ç‰¹ç‚¹: è®¡ç®—å¯†é›†,å¯ä»¥å¹¶è¡Œ

**Decode é˜¶æ®µ**: ä¸²è¡Œç”Ÿæˆ token,å†…å­˜å¸¦å®½å¯†é›†
- è¾“å…¥: æ¯æ¬¡ä¸€ä¸ªæ–° token
- è®¡ç®—: å†…å­˜è¯»å–ä¸ºä¸»
- ç‰¹ç‚¹: å¸¦å®½å¯†é›†,ä¸²è¡Œç”Ÿæˆ

**ä¸¤ç§é˜¶æ®µçš„è®¡ç®—æ¨¡å¼å·®å¼‚**:
```
Prefill:
  GPU åˆ©ç”¨: è®¡ç®— 90%, å¸¦å®½ 10%
  ç“¶é¢ˆ: ç®—åŠ› (FLOPS)
  æœ€ä¼˜ GPU: H100 (é«˜ç®—åŠ›)

Decode:
  GPU åˆ©ç”¨: è®¡ç®— 30%, å¸¦å®½ 70%
  ç“¶é¢ˆ: å†…å­˜å¸¦å®½
  æœ€ä¼˜ GPU: A100 (é«˜å¸¦å®½,ä½æˆæœ¬)
```

**ä¸ºä»€ä¹ˆéœ€è¦åˆ†ç¦»?**
- åŒä¸€ä¸ªç¡¬ä»¶æ— æ³•åŒæ—¶ä¼˜åŒ–ä¸¤ç§æ¨¡å¼
- åˆ†ç¦»åå¯ä»¥é’ˆå¯¹æ€§ä¼˜åŒ–
- èµ„æºåˆ©ç”¨ç‡æå‡ 2-3 å€

---

### 7.7.2 PD åˆ†ç¦»çš„æ¶æ„æ¼”è¿›

**2025 å¹´åˆ**: æ¦‚å¿µæå‡º
- å­¦æœ¯è®ºæ–‡å‘è¡¨
- ç¤¾åŒºå¼€å§‹è®¨è®º

**2025 å¹´ä¸­**: vLLMã€SGLang ç­‰ç¤¾åŒºåˆä½œå®ç°
- vLLM æ·»åŠ  PD åˆ†ç¦»æ”¯æŒ
- SGLang æ¨å‡º RadixAttention

**2025 å¹´åº•**: æˆä¸ºç”Ÿäº§æ ‡å‡†æ¶æ„
- å‡ ä¹æ‰€æœ‰å‚å•†éƒ½åœ¨é‡‡ç”¨
- æœ€ä½³å®è·µé€æ­¥å®Œå–„

**ä»æ¦‚å¿µåˆ°ç”Ÿäº§åªç”¨äº†ä¸€å¹´**

---

### 7.7.3 PD åˆ†ç¦»çš„æŠ€æœ¯ä¼˜åŠ¿

**å¼‚æ„éƒ¨ç½²**:
```
Prefill Worker: H100 (ç®—åŠ›ä¼˜åŒ–)
  - é«˜ FLOPS
  - å¤„ç†æ–°è¯·æ±‚çš„ Prefill

Decode Worker: A100 (å¸¦å®½ä¼˜åŒ–)
  - é«˜å†…å­˜å¸¦å®½
  - å¤„ç† Decode é˜¶æ®µ
  - æˆæœ¬æ›´ä½
```

**èµ„æºéš”ç¦»**:
```
æ— åˆ†ç¦»:
  é•¿è¯·æ±‚çš„ Prefill é˜»å¡çŸ­è¯·æ±‚çš„ Decode
  â†’ P99 å»¶è¿Ÿé«˜

æœ‰åˆ†ç¦»:
  Prefill å’Œ Decode ç‹¬ç«‹è°ƒåº¦
  â†’ é•¿è¯·æ±‚ä¸å½±å“çŸ­è¯·æ±‚
```

**å¼¹æ€§æ‰©å±•**:
```
é«˜å³°æœŸ:
  å¢åŠ  Prefill Worker (æ–°ç”¨æˆ·å¤š)

ç¨³å®šæœŸ:
  å¢åŠ  Decode Worker (ç”Ÿæˆå¤š)
```

**æ€§èƒ½ä¼˜åŒ–**:
```
Prefill Worker:
  - å¤§ batch size
  - ç®—å­èåˆ
  - Tensor Core ä¼˜åŒ–

Decode Worker:
  - é«˜å¸¦å®½ä¼˜åŒ–
  - KV Cache ä¼˜åŒ–
  - Speculative Decoding
```

---

### 7.7.4 vLLM çš„ PD åˆ†ç¦»å®ç°

**æ¶æ„è®¾è®¡**:
```python
# Prefill Worker
class PrefillWorker:
    def __init__(self, model_path):
        self.model = load_model(model_path)
        self.cache_engine = CacheEngine()

    def process(self, requests):
        """å¤„ç† Prefill é˜¶æ®µ"""
        for req in requests:
            # è®¡ç®— prompt çš„ KV Cache
            kv_cache = self.model.prefill(req.prompt)

            # å­˜å‚¨åˆ° Cache Engine
            self.cache_engine.store(req.id, kv_cache)

        return kv_cache

# Decode Worker
class DecodeWorker:
    def __init__(self, model_path, prefill_worker_url):
        self.model = load_model(model_path)
        self.cache_engine = CacheEngine()
        self.prefill_worker = PrefillClient(prefill_worker_url)

    def process(self, requests):
        """å¤„ç† Decode é˜¶æ®µ"""
        for req in requests:
            # ä» Prefill Worker è·å– KV Cache
            kv_cache = self.prefill_worker.fetch(req.id)

            # åŠ è½½åˆ°æœ¬åœ° Cache Engine
            self.cache_engine.load(req.id, kv_cache)

            # å¼€å§‹ Decode
            output = self.model.decode(kv_cache, req.max_tokens)

        return output
```

**é€šä¿¡æœºåˆ¶**: KV Cache çš„ä¼ è¾“
```python
# åºåˆ—åŒ– KV Cache
def serialize_kv_cache(kv_cache):
    """å°† KV Cache åºåˆ—åŒ–ä¸ºå­—èŠ‚æµ"""
    import pickle
    return pickle.dumps(kv_cache)

# ååºåˆ—åŒ– KV Cache
def deserialize_kv_cache(data):
    """ä»å­—èŠ‚æµæ¢å¤ KV Cache"""
    import pickle
    return pickle.loads(data)

# RPC è°ƒç”¨
prefill_worker.push_kv_cache(
    request_id=req.id,
    kv_cache_bytes=serialize_kv_cache(kv_cache)
)
```

**è°ƒåº¦ç­–ç•¥**:
```python
def schedule_for_pd(requests):
    """å°†è¯·æ±‚åˆ†é…åˆ° Prefill æˆ– Decode Worker"""
    prefill_requests = []
    decode_requests = []

    for req in requests:
        if req.state == 'waiting':
            # æ–°è¯·æ±‚ â†’ Prefill
            prefill_requests.append(req)
        elif req.state == 'decoding':
            # æ­£åœ¨ç”Ÿæˆ â†’ Decode
            decode_requests.append(req)

    return prefill_requests, decode_requests
```

---

### 7.7.5 SGLang çš„ PD åˆ†ç¦»å®è·µ

**RadixAttention**: ç»Ÿä¸€çš„æ³¨æ„åŠ›æŠ½è±¡
```python
class RadixAttention:
    def forward(self, query, key, value, state):
        # è‡ªåŠ¨æ£€æµ‹æ˜¯ Prefill è¿˜æ˜¯ Decode
        if state.is_prefill:
            return self._prefill_forward(query, key, value)
        else:
            return self._decode_forward(query, key, value)
```

**è‡ªåŠ¨åˆ†ç¦»**: æ— éœ€æ‰‹åŠ¨é…ç½®
```bash
python -m sglang.launch_server \
  --model meta-llama/Llama-3-8B \
  --enable-pd-separation  # è‡ªåŠ¨å¯ç”¨
```

**ç”Ÿäº§ç»éªŒ**: ç¨³å®šæ€§ã€æ€§èƒ½ç›‘æ§
```
å…³é”®æŒ‡æ ‡:
  - Prefill Worker: GPU åˆ©ç”¨ç‡ >80%
  - Decode Worker: å†…å­˜å¸¦å®½åˆ©ç”¨ç‡ >70%
  - KV Cache ä¼ è¾“: å»¶è¿Ÿ <10ms

å‘Šè­¦é˜ˆå€¼:
  - Prefill é˜Ÿåˆ—é•¿åº¦ >100: è€ƒè™‘æ‰©å®¹
  - Decode é˜Ÿåˆ—é•¿åº¦ >500: è€ƒè™‘æ‰©å®¹
  - KV Cache ä¼ è¾“å»¶è¿Ÿ >50ms: æ£€æŸ¥ç½‘ç»œ
```

---

### 7.7.6 PD åˆ†ç¦»çš„æŒ‘æˆ˜

**KV Cache ä¼ è¾“**:
```
é—®é¢˜: ç½‘ç»œå¼€é”€å’Œåºåˆ—åŒ–
  - KV Cache å¾ˆå¤§ (æ•°ç™¾ MB åˆ°æ•° GB)
  - åºåˆ—åŒ–/ååºåˆ—åŒ–å¼€é”€
  - ç½‘ç»œä¼ è¾“å»¶è¿Ÿ

è§£å†³æ–¹æ¡ˆ:
  - ä½¿ç”¨å…±äº«å­˜å‚¨ (NVLinkã€InfiniBand)
  - å‹ç¼© KV Cache
  - å¢é‡ä¼ è¾“ (åªä¼ è¾“æ–°å¢éƒ¨åˆ†)
```

**è´Ÿè½½å‡è¡¡**:
```
é—®é¢˜: Prefill å’Œ Decode çš„é€Ÿç‡åŒ¹é…
  - Prefill å¿«: Decode æˆä¸ºç“¶é¢ˆ
  - Decode å¿«: Prefill æˆä¸ºç“¶é¢ˆ

è§£å†³æ–¹æ¡ˆ:
  - åŠ¨æ€è°ƒæ•´ Worker æ•°é‡
  - è‡ªé€‚åº”è°ƒåº¦ç­–ç•¥
  - ç›‘æ§å’Œè‡ªåŠ¨æ‰©ç¼©å®¹
```

**å®¹é”™å¤„ç†**:
```
é—®é¢˜: Worker æ•…éšœå¦‚ä½•æ¢å¤
  - Prefill Worker æ•…éšœ: æ–°è¯·æ±‚æ— æ³•å¤„ç†
  - Decode Worker æ•…éšœ: æ­£åœ¨ç”Ÿæˆçš„è¯·æ±‚ä¸­æ–­

è§£å†³æ–¹æ¡ˆ:
  - å†—ä½™éƒ¨ç½² (å¤š Worker)
  - KV Cache æŒä¹…åŒ–
  - è‡ªåŠ¨æ•…éšœè½¬ç§»
```

**å¤æ‚åº¦å¢åŠ **:
```
é—®é¢˜: éƒ¨ç½²å’Œè¿ç»´çš„æŒ‘æˆ˜
  - éœ€è¦ç®¡ç†ä¸¤ç§ Worker
  - é…ç½®æ›´å¤æ‚
  - è°ƒè¯•æ›´å›°éš¾

è§£å†³æ–¹æ¡ˆ:
  - å®Œå–„çš„ç›‘æ§ä½“ç³»
  - è‡ªåŠ¨åŒ–éƒ¨ç½²å·¥å…·
  - ç»Ÿä¸€çš„æ—¥å¿—å’Œè¿½è¸ª
```

---

### 7.7.7 å®æˆ˜æ¡ˆä¾‹

**æ¡ˆä¾‹ 1: å•æœº GPU çš„ PD åˆ†ç¦»**
```
ç¡¬ä»¶: å•æœº 4 Ã— A100 40GB

éƒ¨ç½²:
  GPU 0-1: Prefill Worker (2 ä¸ª)
  GPU 2-3: Decode Worker (2 ä¸ª)

æ€§èƒ½:
  ååé‡: 1.8x æå‡ (ç›¸æ¯”æ— åˆ†ç¦»)
  P95 å»¶è¿Ÿ: é™ä½ 40%
```

**æ¡ˆä¾‹ 2: è·¨æœºå™¨çš„ PD åˆ†ç¦»éƒ¨ç½²**
```
ç¡¬ä»¶:
  æœºå™¨ A: 4 Ã— H100 (Prefill)
  æœºå™¨ B: 8 Ã— A100 (Decode)

ç½‘ç»œ: InfiniBand (100 Gbps)

æ€§èƒ½:
  ååé‡: 2.5x æå‡
  æˆæœ¬: é™ä½ 30% (A100 æ¯” H100 ä¾¿å®œ)
```

**æ¡ˆä¾‹ 3: å¼‚æ„ GPU (H100 + H200) çš„å®è·µ**
```
ç¡¬ä»¶:
  H100: Prefill (ç®—åŠ›ä¼˜åŒ–)
  H200: Decode (å¸¦å®½ä¼˜åŒ–,å¤§å†…å­˜)

æ€§èƒ½:
  ååé‡: 3x æå‡
  æ”¯æŒæ›´é•¿åºåˆ— (H200 141GB å†…å­˜)
```

---

## âœ… ç« èŠ‚æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬ç« å,ä½ åº”è¯¥èƒ½å¤Ÿ:

- [ ] è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦è°ƒåº¦å™¨
- [ ] å¯¹æ¯” FIFOã€Static Batchingã€Continuous Batching
- [ ] æè¿° Continuous Batching çš„å·¥ä½œæµç¨‹
- [ ] ç†è§£ vLLM çš„è¿­ä»£çº§è°ƒåº¦
- [ ] è§£é‡Š Overlap Scheduling çš„åŸç†å’Œä¼˜åŠ¿
- [ ] é…ç½® vLLM çš„è°ƒåº¦å‚æ•°
- [ ] é’ˆå¯¹ä¸åŒåœºæ™¯é€‰æ‹©åˆé€‚çš„è°ƒåº¦ç­–ç•¥
- [ ] ç†è§£ PD åˆ†ç¦»çš„æ¶æ„æ¼”è¿›
- [ ] è®¾è®¡ PD åˆ†ç¦»çš„éƒ¨ç½²æ–¹æ¡ˆ
- [ ] è¯„ä¼° PD åˆ†ç¦»çš„æ”¶ç›Šå’ŒæŒ‘æˆ˜

---

## ğŸ“š åŠ¨æ‰‹ç»ƒä¹ 

**ç»ƒä¹  7.1**: å¯¹æ¯”é™æ€æ‰¹å¤„ç†å’ŒåŠ¨æ€æ‰¹å¤„ç†

åœºæ™¯:
- 8 ä¸ªè¯·æ±‚,é•¿åº¦åˆ†åˆ«ä¸º: [10, 50, 20, 100, 30, 15, 80, 25] tokens
- å‡è®¾æ¯ä¸ªè¯·æ±‚éƒ½ç”Ÿæˆ 100 tokens

ä»»åŠ¡:
1. è®¡ç®— Static Batching çš„ padding æ•°é‡
2. è®¡ç®— Continuous Batching çš„ padding æ•°é‡
3. æ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„ GPU åˆ©ç”¨ç‡

**ç»ƒä¹  7.2**: é’ˆå¯¹ä¸åŒåœºæ™¯ä¼˜åŒ–è°ƒåº¦å‚æ•°

åœºæ™¯:
- Chatbot: 100 ä¸ªå¹¶å‘,å¹³å‡ 50 tokens,å¯¹å»¶è¿Ÿæ•æ„Ÿ
- RAG: 20 ä¸ªå¹¶å‘,å¹³å‡ 2000 tokens prompt,å¯¹ååé‡æ•æ„Ÿ
- æ‰¹å¤„ç†: 1000 ä¸ªè¯·æ±‚,ç¦»çº¿ä»»åŠ¡,è¿½æ±‚æœ€å¿«å®Œæˆ

ä»»åŠ¡:
1. ä¸ºæ¯ä¸ªåœºæ™¯è®¾è®¡è°ƒåº¦ç­–ç•¥
2. é€‰æ‹©åˆé€‚çš„è°ƒåº¦ç®—æ³•
3. é…ç½® vLLM å‚æ•°

**ç»ƒä¹  7.3**: ä½¿ç”¨ vLLM éƒ¨ç½² PD åˆ†ç¦»æ¶æ„ â­

ä»»åŠ¡:
1. è®¾è®¡ä¸€ä¸ª PD åˆ†ç¦»çš„éƒ¨ç½²æ–¹æ¡ˆ
2. é€‰æ‹©åˆé€‚çš„ç¡¬ä»¶é…ç½®
3. ç¼–å†™ docker-compose.yml
4. è¯„ä¼°æ€§èƒ½æå‡å’Œæˆæœ¬

---

## ğŸ¯ æ€»ç»“

**å…³é”®è¦ç‚¹**:
- è°ƒåº¦å™¨æ˜¯æ¨ç†ç³»ç»Ÿçš„æ ¸å¿ƒ,å†³å®šæ€§èƒ½ä¸Šé™
- Continuous Batching é€šè¿‡åŠ¨æ€è°ƒæ•´,æ¶ˆé™¤ padding æµªè´¹
- Overlap Scheduling é€šè¿‡ CPU-GPU å¹¶è¡Œ,æ¶ˆé™¤ GPU stalls
- Dynamic Memory Management é€šè¿‡åŠ¨æ€åˆ†é…,èŠ‚çœ 30-50% å†…å­˜
- PD åˆ†ç¦»æ˜¯ 2025 å¹´çš„ç”Ÿäº§æ ‡å‡†,å¸¦æ¥ 2-3x æ€§èƒ½æå‡
- ä¸åŒåœºæ™¯éœ€è¦ä¸åŒçš„è°ƒåº¦ç­–ç•¥

**ä¸‹ä¸€ç« **: ç¬¬8ç«  é‡åŒ–æŠ€æœ¯â€”â€”å¦‚ä½•é€šè¿‡é‡åŒ–èŠ‚çœæ˜¾å­˜å¹¶æå‡æ¨ç†é€Ÿåº¦ã€‚

---

**æœ‰é—®é¢˜?åŠ å…¥ [ç¬¬7ç«  Discord é¢‘é“](https://discord.gg/TODO) è®¨è®º!**
