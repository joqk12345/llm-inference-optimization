# ç¬¬6ç« : KV Cacheä¼˜åŒ–

> **ğŸ’° æˆæœ¬å½±å“** (åŸºäºè¡Œä¸šæ•°æ®)
> - **æ˜¾å­˜èŠ‚çœ**: KV Cacheä¼˜åŒ–å¯å‡å°‘æ˜¾å­˜å ç”¨50-70%
> - **ååæå‡**: åœ¨åŒæ ·ç¡¬ä»¶ä¸Šå¯æœåŠ¡2-3å€æ›´å¤šç”¨æˆ·
> - **æˆæœ¬èŠ‚çœ**: å…¸å‹åœºæ™¯ä»$0.002/tokené™åˆ°$0.001/token

## ç®€ä»‹

åœ¨ç¬¬5ç« ä¸­,æˆ‘ä»¬å­¦ä¹ äº† KV Cache çš„åŸºæœ¬åŸç†â€”â€”é€šè¿‡ç¼“å­˜å†å² token çš„ Key å’Œ Value å‘é‡,é¿å…é‡å¤è®¡ç®—,å°†å¤æ‚åº¦ä» O(nÂ²) é™åˆ° O(n)ã€‚ä½†ä¼ ç»Ÿçš„ KV Cache å®ç°æœ‰ä¸€ä¸ªè‡´å‘½é—®é¢˜:**å†…å­˜ç¢ç‰‡åŒ–**,å¯¼è‡´ GPU æ˜¾å­˜åˆ©ç”¨ç‡åªæœ‰ 60-70%ã€‚

vLLM çš„æ ¸å¿ƒåˆ›æ–°â€”â€”**PagedAttention**,å€Ÿé‰´æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜æœºåˆ¶,å°†æ˜¾å­˜åˆ©ç”¨ç‡æå‡åˆ° 90-95%ã€‚è¿™æ˜¯ vLLM è®ºæ–‡è¢«å¼•ç”¨ 2000+ æ¬¡çš„æ ¹æœ¬åŸå› ,ä¹Ÿæ˜¯ Prefix Cachingã€Continuous Batching ç­‰é«˜çº§ç‰¹æ€§çš„åŸºç¡€ã€‚

æœ¬ç« å°†æ·±å…¥è®²è§£:
- ä¼ ç»Ÿ KV Cache çš„é—®é¢˜å’Œå±€é™æ€§
- PagedAttention çš„è®¾è®¡æ€æƒ³å’Œå®ç°ç»†èŠ‚
- Block allocation å’Œ eviction ç­–ç•¥
- Prefix Caching çš„åŸç†å’Œæ€§èƒ½æå‡
- å¤šç§ KV Cache ä¼˜åŒ–æŠ€æœ¯ (GQAã€é‡åŒ–ã€å…±äº«ç­‰)

**å­¦å®Œæœ¬ç« ,ä½ å°†ç†è§£ä¸ºä»€ä¹ˆ vLLM æ¯”å…¶ä»–æ¨ç†æ¡†æ¶å¿« 3-5 å€ã€‚**

---

## 6.1 Transformer å›é¡¾

### 6.1.1 æ³¨æ„åŠ›æœºåˆ¶åŸç†

**Attention çš„æœ¬è´¨**: è®©æ¯ä¸ª token èƒ½å¤Ÿ"çœ‹åˆ°"å¹¶"èšåˆ"å…¶ä»– token çš„ä¿¡æ¯

```python
# ç®€åŒ–çš„ Attention è®¡ç®—
def attention(Query, Key, Value):
    # 1. è®¡ç®—ç›¸ä¼¼åº¦
    scores = Query @ Key.T  # [seq_len, seq_len]

    # 2. å½’ä¸€åŒ–
    attn_weights = softmax(scores / sqrt(d_k))

    # 3. åŠ æƒæ±‚å’Œ
    output = attn_weights @ Value  # [seq_len, d_v]

    return output
```

**ç‰©ç†æ„ä¹‰**:
```
Token "bank":
Query: "æˆ‘æ˜¯åè¯,æˆ‘æƒ³æ‰¾ä¸é‡‘èç›¸å…³çš„ä¸Šä¸‹æ–‡"
Key:   "æˆ‘æ˜¯åè¯,æˆ‘å¯ä»¥è¢«é‡‘èç›¸å…³çš„æŸ¥è¯¢æ‰¾åˆ°"
Value: "æˆ‘çš„å…·ä½“è¯­ä¹‰å†…å®¹æ˜¯'é“¶è¡Œ'"

Attention("bank", "The money in the ___ was stolen"):
â†’ "bank" çš„ Query ä¸ "money" çš„ Key åŒ¹é…åº¦é«˜
â†’ "bank" èšåˆäº† "money" çš„è¯­ä¹‰
â†’ ç†è§£ä¸º"é“¶è¡Œ"è€Œä¸æ˜¯"æ²³å²¸"
```

---

### 6.1.2 Kã€Vã€Q æ˜¯ä»€ä¹ˆ

**ä¸‰ä¸ªæŠ•å½±çŸ©é˜µ**: Wqã€Wkã€Wv

```
è¾“å…¥: x (æ¯ä¸ª token çš„è¡¨ç¤º)

Query  (Q): x @ Wq  â†’ "æˆ‘æƒ³æ‰¾ä»€ä¹ˆ?"
Key    (K): x @ Wk  â†’ "æˆ‘èƒ½æä¾›ä»€ä¹ˆ?"
Value  (V): x @ Wv  â†’ "æˆ‘çš„å®é™…å†…å®¹"

ä¾‹å¦‚:
Token: "apple"
Q: "æˆ‘æ˜¯æ°´æœ,æˆ‘æƒ³æ‰¾ä¸é£Ÿç‰©ç›¸å…³çš„ä¸Šä¸‹æ–‡"
K: "æˆ‘æ˜¯æ°´æœ,æˆ‘å¯ä»¥è¢«ä¸é£Ÿç‰©ç›¸å…³çš„æŸ¥è¯¢æ‰¾åˆ°"
V: "æˆ‘çš„è¯­ä¹‰æ˜¯'è‹¹æœ'"
```

**å¤šå¤´æ³¨æ„åŠ›** (Multi-Head Attention):
```
æ¯ä¸ª head å­¦ä¹ ä¸åŒçš„å…³ç³»æ¨¡å¼:
- Head 1: è¯­æ³•å…³ç³» (ä¸»è°“å®¾)
- Head 2: è¯­ä¹‰å…³ç³» (åŒä¹‰è¯ã€åä¹‰è¯)
- Head 3: æŒ‡ä»£å…³ç³» (he â†’ John)
- Head 4: æ—¶æ€å…³ç³» (is â†’ ç°åœ¨)
...

æœ€ç»ˆè¾“å‡º: æ‹¼æ¥æ‰€æœ‰ heads,å†ç»è¿‡çº¿æ€§å˜æ¢
```

---

### 6.1.3 ä¸ºä»€ä¹ˆéœ€è¦ç¼“å­˜

**ç”Ÿæˆè¿‡ç¨‹çš„é‡å¤è®¡ç®—**:

```
æ­¥éª¤ 1: "The capital of France is"
â†’ è®¡ç®— Token 0-6 çš„ Kã€V
â†’ ç”Ÿæˆ "Paris"

æ­¥éª¤ 2: "The capital of France is Paris"
â†’ é‡æ–°è®¡ç®— Token 0-7 çš„ Kã€V âŒ
â†’ Token 0-6 çš„ Kã€V é‡å¤è®¡ç®—äº†!

æ­¥éª¤ 3: "The capital of France is Paris and"
â†’ é‡æ–°è®¡ç®— Token 0-8 çš„ Kã€V âŒâŒ
â†’ Token 0-7 çš„ Kã€V åˆé‡å¤è®¡ç®—äº†!
```

**æ ¸å¿ƒæ´å¯Ÿ**:
- æ—§ token çš„ Kã€V åœ¨æ¯æ¬¡ç”Ÿæˆæ­¥éª¤ä¸­ä¸å˜
- åªæœ‰æ–° token çš„ Kã€V æ˜¯æ–°çš„
- ç¼“å­˜æ—§ Kã€V,åªè®¡ç®—æ–° Kã€V â†’ å¤§å¹…å‡å°‘è®¡ç®—

**ç¼“å­˜çš„å¥½å¤„**:
```
æ—  KV Cache:
- æ¯ä¸ªæ­¥éª¤: O(nÂ²)
- æ€»å¤æ‚åº¦: O(nÂ³)

æœ‰ KV Cache:
- ç¬¬ä¸€ä¸ªæ­¥éª¤: O(nÂ²)
- åç»­æ­¥éª¤: O(n)
- æ€»å¤æ‚åº¦: O(nÂ²)
```

---

## 6.2 KV Cache åŸç†

### 6.2.1 ç”Ÿæˆè¿‡ç¨‹çš„é‡å¤è®¡ç®—é—®é¢˜

**é—®é¢˜åœºæ™¯**: ç”Ÿæˆç¬¬ n+1 ä¸ª token

**æœ´ç´ åšæ³•**:
```
å·²ç”Ÿæˆ: "Hello, my name is John" (7 tokens)
ç›®æ ‡:   ç”Ÿæˆç¬¬ 8 ä¸ª token

æœ´ç´ æ–¹æ³•:
1. å°†æ‰€æœ‰ 8 ä¸ª tokens é‡æ–°è¾“å…¥æ¨¡å‹
2. é‡æ–°è®¡ç®—æ‰€æœ‰ token çš„ K å’Œ V
3. åªä½¿ç”¨æœ€åä¸€ä¸ª token çš„è¾“å‡º
```

**å¤æ‚åº¦åˆ†æ**:
```
ç”Ÿæˆç¬¬ 1 ä¸ª token:  O(1Â²)
ç”Ÿæˆç¬¬ 2 ä¸ª token:  O(2Â²)
ç”Ÿæˆç¬¬ 3 ä¸ª token:  O(3Â²)
...
ç”Ÿæˆç¬¬ n ä¸ª token:  O(nÂ²)

æ€»å¤æ‚åº¦: O(1Â² + 2Â² + ... + nÂ²) = O(nÂ³)
```

**å¯è§†åŒ–æµªè´¹**:
```
ç¬¬ 2 æ­¥: é‡æ–°è®¡ç®— Token 1 çš„ Kã€V âŒ æµªè´¹
ç¬¬ 3 æ­¥: é‡æ–°è®¡ç®— Token 1, 2 çš„ Kã€V âŒ æµªè´¹
...
ç¬¬ n æ­¥: é‡æ–°è®¡ç®— Token 1, 2, ..., n-1 çš„ Kã€V âŒ æµªè´¹
```

---

### 6.2.2 KV Cache çš„æ ¸å¿ƒæ€æƒ³

**æ ¸å¿ƒæ´å¯Ÿ**: æ—§ token çš„ Kã€V å·²ç»è®¡ç®—è¿‡,ç¼“å­˜èµ·æ¥!

**åšæ³•**:
```
Prefill é˜¶æ®µ:
è¾“å…¥: "Hello, my name is"
è®¡ç®—:
  Token 0: K0, V0
  Token 1: K1, V1
  Token 2: K2, V2
  Token 3: K3, V3
å­˜å‚¨: ç¼“å­˜æ‰€æœ‰ K, V

Decode é˜¶æ®µ - ç¬¬ 1 æ­¥:
è¾“å…¥: æ–° Token 4
è®¡ç®—:
  Token 4: K4, V4
å¤ç”¨: K0, K1, K2, K3 (ä»ç¼“å­˜è¯»å–)
ç»„åˆ: [K0, K1, K2, K3, K4]
      [V0, V1, V2, V3, V4]

Decode é˜¶æ®µ - ç¬¬ 2 æ­¥:
è¾“å…¥: æ–° Token 5
è®¡ç®—:
  Token 5: K5, V5
å¤ç”¨: K0, K1, K2, K3, K4 (ä»ç¼“å­˜è¯»å–)
ç»„åˆ: [K0, K1, K2, K3, K4, K5]
      [V0, V1, V2, V3, V4, V5]
```

**æ•ˆæœ**: é¿å…é‡å¤è®¡ç®—
- âœ… æ¯ä¸ª token çš„ Kã€V åªè®¡ç®—ä¸€æ¬¡
- âœ… å¤§å¹…å‡å°‘è®¡ç®—é‡

**ä»£ä»·**: æ˜¾å­˜å ç”¨ O(n)
- âŒ éœ€è¦å­˜å‚¨æ‰€æœ‰å†å² token çš„ Kã€V
- âŒ åºåˆ—è¶Šé•¿,æ˜¾å­˜å ç”¨è¶Šå¤§

---

### 6.2.3 å¦‚ä½•å‡å°‘è®¡ç®—é‡

**æ—  KV Cache çš„è®¡ç®—**:
```python
for step in range(num_tokens):
    # æ¯æ¬¡é‡æ–°è®¡ç®—æ‰€æœ‰ tokens
    all_tokens = tokens[:step+1]
    K, V = model.compute_kv(all_tokens)  # O((step+1)Â²)
    output = model.generate(K, V)
```

**æœ‰ KV Cache çš„è®¡ç®—**:
```python
# Prefill: è®¡ç®—ç¬¬ä¸€ä¸ª token
K, V = model.compute_kv(tokens[:1])  # O(1Â²)
cache = {'K': K, 'V': V}

for step in range(1, num_tokens):
    # åªè®¡ç®—æ–° token
    new_K, new_V = model.compute_kv(tokens[step:step+1])  # O(1)
    # å¤ç”¨ç¼“å­˜
    K = torch.cat([cache['K'], new_K], dim=1)
    V = torch.cat([cache['V'], new_V], dim=1)
    cache = {'K': K, 'V': V}
    output = model.generate(K, V)
```

**åŠ é€Ÿæ¯”**:
```
åºåˆ—é•¿åº¦ n = 1000:
- æ—  KV Cache: ~1,000,000,000 æ¬¡è¿ç®—
- æœ‰ KV Cache: ~1,000,000 + 999 Ã— 1,000 = ~2,000,000 æ¬¡è¿ç®—
- åŠ é€Ÿæ¯”: ~500x
```

---

### 6.2.4 å›¾è§£ KV Cache å·¥ä½œæµç¨‹

**æ—¶é—´çº¿å¯è§†åŒ–**:

```
æ­¥éª¤ 1 (Prefill):
è¾“å…¥: "The capital of"
Tokens: [t0,   t1,   t2]
è®¡ç®—:  [K0,V0 K1,V1 K2,V2]
ç¼“å­˜:  âœ“âœ“âœ“
è¾“å‡º:  "France"

æ­¥éª¤ 2 (Decode):
è¾“å…¥: "The capital of France"
Tokens: [t0,   t1,   t2,   t3]
å¤ç”¨:  [K0,V0 K1,V1 K2,V2] âœ“âœ“âœ“ ä»ç¼“å­˜è¯»å–
è®¡ç®—:  [K3,V3]           âœ“âœ“ æ–°è®¡ç®—
ç¼“å­˜:  [K0,V0 K1,V1 K2,V2 K3,V3]
è¾“å‡º:  " is"

æ­¥éª¤ 3 (Decode):
è¾“å…¥: "The capital of France is"
Tokens: [t0,   t1,   t2,   t3,   t4]
å¤ç”¨:  [K0,V0 K1,V1 K2,V2 K3,V3] âœ“âœ“âœ“âœ“ ä»ç¼“å­˜è¯»å–
è®¡ç®—:  [K4,V4]                   âœ“âœ“ æ–°è®¡ç®—
ç¼“å­˜:  [K0,V0 K1,V1 K2,V2 K3,V3 K4,V4]
è¾“å‡º:  "Paris"
```

**æ˜¾å­˜å ç”¨å¢é•¿**:
```
æ¯ä¸ª token çš„ KV Cache:
= 2 Ã— num_layers Ã— num_heads Ã— head_dim Ã— bytes

Llama-2-7B:
= 2 Ã— 32 Ã— 32 Ã— 128 Ã— 2 bytes
= 524,288 bytes
â‰ˆ 0.5 MB/token

1000 tokens:
= 1000 Ã— 0.5 MB
= 500 MB

10000 tokens:
= 10000 Ã— 0.5 MB
= 5000 MB = 5 GB
```

---

## 6.3 KV Cache å®ç°

### 6.3.1 æœ´ç´ å®ç°æ–¹å¼

**è¿ç»­å†…å­˜åˆ†é…**:
```python
class NaiveKVCache:
    def __init__(self, max_batch_size, max_seq_len, hidden_dim):
        # é¢„åˆ†é…è¿ç»­å†…å­˜
        self.cache_k = torch.zeros(
            max_batch_size,
            num_layers,
            num_heads,
            max_seq_len,
            head_dim,
            dtype=torch.float16,
            device='cuda'
        )
        self.cache_v = torch.zeros(
            max_batch_size,
            num_layers,
            num_heads,
            max_seq_len,
            head_dim,
            dtype=torch.float16,
            device='cuda'
        )

    def append(self, batch_idx, layer_idx, new_k, new_v):
        # è¿½åŠ æ–° token çš„ Kã€V
        seq_len = self.seq_lens[batch_idx]
        self.cache_k[batch_idx, layer_idx, :, seq_len:seq_len+1, :] = new_k
        self.cache_v[batch_idx, layer_idx, :, seq_len:seq_len+1, :] = new_v
        self.seq_lens[batch_idx] += 1
```

**é—®é¢˜**:
```
1. å¿…é¡»é¢„å…ˆçŸ¥é“ max_batch_size å’Œ max_seq_len
   - å®é™…åœºæ™¯: æ— æ³•é¢„æµ‹
   - ä¿å®ˆä¼°è®¡: æµªè´¹å¤§é‡å†…å­˜
   - æ¿€è¿›ä¼°è®¡: å¯èƒ½æº¢å‡º

2. å†…å­˜ç¢ç‰‡åŒ–
   - Request A: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 1000 tokens â†’ åˆ†é… 1000
   - Request B: [â–ˆâ–ˆâ–ˆâ–ˆ] 500 tokens â†’ åˆ†é… 500
   - Request A å®Œæˆ â†’ é‡Šæ”¾ 1000
   - Request C éœ€è¦ 800 â†’ æ— æ³•ä½¿ç”¨ Request A çš„ç©ºé—´!
     (Request B å æ®äº†ä¸­é—´ä½ç½®)

3. GPU åˆ©ç”¨ç‡ä½
   - å†…å­˜ç¢ç‰‡åŒ–å¯¼è‡´å¤§é‡ç©ºé—´æ— æ³•ä½¿ç”¨
   - å®é™…åˆ©ç”¨ç‡åªæœ‰ 60-70%
```

---

### 6.3.2 PagedAttention åŸç† âš¡ï¸ (vLLM çš„æ ¸å¿ƒ)

> **ğŸ’¡ æ·±åº¦æ¥æº**: [Berkeley EECS-2025-192](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-192.pdf)
>
> **æ ¸å¿ƒæ´å¯Ÿ**: PagedAttention å€Ÿé‰´æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜æœºåˆ¶,å°† KV Cache åˆ†æˆå›ºå®šå¤§å°çš„ pages,å®ç°é«˜æ•ˆçš„å†…å­˜ç®¡ç†ã€‚
>
> **ä¸ºä»€ä¹ˆé‡è¦**:
> - vLLM æœ€æ ¸å¿ƒçš„åˆ›æ–° (è®ºæ–‡å¼•ç”¨ 2000+)
> - å†…å­˜åˆ©ç”¨ç‡ä» 60-70% æå‡åˆ° 90-95%
> - Prefix Caching çš„åº•å±‚åŸºç¡€

---

#### 6.3.2.1 ä¼ ç»Ÿ KV Cache çš„é—®é¢˜

**è¿ç»­å†…å­˜åˆ†é…çš„ç¼ºé™·**:
```
Request 1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 1000 tokens â†’ è¿ç»­åˆ†é… 1000 token ç©ºé—´
Request 2: [â–ˆâ–ˆâ–ˆâ–ˆ] 500 tokens â†’ è¿ç»­åˆ†é… 500 token ç©ºé—´
Request 1 å®Œæˆ â†’ é‡Šæ”¾ 1000 tokens
Request 3 éœ€è¦ 800 tokens â†’ æ— æ³•ä½¿ç”¨ Request 1 çš„ç©ºé—´ (ç¢ç‰‡åŒ–!)
```

**å†…å­˜ç¢ç‰‡åŒ–**:

- **External fragmentation**: è¯·æ±‚ä¹‹é—´çš„å°ç©ºéš™æ— æ³•åˆ©ç”¨
  ```
  GPU Memory: [Req1: 1000][ç©ºéš™: 200][Req2: 500][ç©ºéš™: 300]
  Request 3 éœ€è¦ 800 tokens â†’ å¤±è´¥! (ç©ºéš™ä¸å¤Ÿå¤§)
  ```

- **Internal fragmentation**: é¢„åˆ†é…çš„å›ºå®šå¤§å°å¯èƒ½æµªè´¹
  ```
  é¢„åˆ†é… 2048 tokens â†’ å®é™…ä½¿ç”¨ 1000 tokens â†’ æµªè´¹ 1048 tokens
  ```

**é™æ€å†…å­˜åˆ†é…çš„é—®é¢˜**:
- å¿…é¡»é¢„å…ˆçŸ¥é“æœ€å¤§ batch size å’Œæœ€å¤§åºåˆ—é•¿åº¦
- æ— æ³•åŠ¨æ€è°ƒæ•´å†…å­˜ä½¿ç”¨
- GPU åˆ©ç”¨ç‡ä½ (å¤§é‡å†…å­˜æµªè´¹)

---

#### 6.3.2.2 PagedAttention çš„è®¾è®¡æ€æƒ³

**çµæ„Ÿæ¥æº: OS è™šæ‹Ÿå†…å­˜**
```
OS Virtual Memory:  Pages (4KB) + Page Table
vLLM KV Cache:      Blocks (16 tokens) + Block Table
```

**æ ¸å¿ƒæ¦‚å¿µ**:
- **Logical blocks**: é€»è¾‘ä¸Šçš„è¿ç»­åºåˆ— (ç”¨æˆ·è§†è§’)
- **Physical blocks**: GPU å†…å­˜ä¸­çš„å®é™…å— (ç³»ç»Ÿè§†è§’)
- **Block table**: æ˜ å°„å…³ç³» (logical â†’ physical)

**å·¥ä½œåŸç†**:
```
Request: [token1-16][token17-32][token33-48][...]
Logical:  Block 0      Block 1       Block 2
Physical: Block 15     Block 7       Block 23
         (åˆ†æ•£åœ¨ç‰©ç†å†…å­˜ä¸­,ä½†é€»è¾‘ä¸Šè¿ç»­)
```

**å…³é”®ä¼˜åŠ¿**:
- ä¸éœ€è¦è¿ç»­å†…å­˜
- Physical blocks å¯ä»¥åˆ†æ•£åœ¨ GPU å†…å­˜ä»»æ„ä½ç½®
- é€»è¾‘ä¸Šè¿ç»­,ç‰©ç†ä¸Šåˆ†æ•£

**ç±»æ¯”**:
```
ä¼ ç»Ÿ KV Cache:
â†’ åƒè¿ç»­çš„æ•°ç»„
â†’ å¿…é¡»æ‰¾åˆ°è¶³å¤Ÿå¤§çš„è¿ç»­ç©ºé—´
â†’ å®¹æ˜“ç¢ç‰‡åŒ–

PagedAttention:
â†’ åƒé“¾è¡¨
â†’ æ¯ä¸ªèŠ‚ç‚¹ (block) ç‹¬ç«‹åˆ†é…
â†’ é€šè¿‡æŒ‡é’ˆ (block table) è¿æ¥
â†’ å†…å­˜åˆ©ç”¨ç‡é«˜
```

---

#### 6.3.2.3 Block Allocation ç­–ç•¥

**é¢„åˆ†é…ç­–ç•¥**:
```python
# vLLM çš„å¯åŠ¨æ—¶åˆ†é…
def allocate_at_startup():
    # è®¡ç®—å¯ç”¨ GPU å†…å­˜
    gpu_memory = get_gpu_memory()
    # é¢„åˆ†é… 90% ç»™ KV Cache (ä¿ç•™ 10% ç»™æ¨¡å‹ weights)
    num_blocks = (gpu_memory * 0.9) / BLOCK_SIZE
    # åˆ›å»º block pool
    block_pool = BlockPool(num_blocks)
    return block_pool
```

**åŠ¨æ€åˆ†é…ç®—æ³•**:
```python
def allocate_blocks(request, num_tokens):
    num_blocks = ceil(num_tokens / BLOCK_SIZE)  # 16 tokens/block
    for i in range(num_blocks):
        block = find_free_block()
        if block is None:
            # å†…å­˜ä¸è¶³,è§¦å‘ eviction
            trigger_eviction_policy()
            block = find_free_block()
        request.blocks.append(block)
    return request.blocks
```

**Block çš„å¤§å°é€‰æ‹©**:
- é»˜è®¤: 16 tokens/block
- ä¸ºä»€ä¹ˆæ˜¯ 16?
  - å¤ªå° (å¦‚ 8): block table å¤ªå¤§,ç®¡ç†å¼€é”€é«˜
  - å¤ªå¤§ (å¦‚ 32): internal fragmentation ä¸¥é‡
  - 16 æ˜¯ç»éªŒæœ€ä¼˜å€¼ (å¹³è¡¡å¼€é”€å’Œæµªè´¹)

**å†…å­˜åˆ©ç”¨ç‡å¯¹æ¯”**:
```
ä¼ ç»Ÿæ–¹æ³•:
Request 1: 1000 tokens â†’ åˆ†é… 1000 (è¿ç»­)
Request 2: 500 tokens â†’ åˆ†é… 500 (è¿ç»­)
Request 3: 800 tokens â†’ éœ€è¦è¿ç»­ 800 â†’ å¤±è´¥!
å†…å­˜åˆ©ç”¨ç‡: (1000 + 500) / 2048 = 73%

PagedAttention:
Request 1: 1000 tokens â†’ 63 blocks
Request 2: 500 tokens â†’ 32 blocks
Request 3: 800 tokens â†’ 50 blocks (åˆ†æ•£ä½¿ç”¨ç¢ç‰‡ç©ºé—´)
å†…å­˜åˆ©ç”¨ç‡: (1000 + 500 + 800) / 2048 = 91%
```

---

#### 6.3.2.4 Block Eviction ç­–ç•¥

**LRU (Least Recently Used)**:
```python
class LRU_Eviction:
    def __init__(self):
        self.access_time = {}  # block_id â†’ timestamp

    def evict(self, num_blocks):
        # æŒ‰è®¿é—®æ—¶é—´æ’åº
        sorted_blocks = sorted(
            self.access_time.items(),
            key=lambda x: x[1]  # æŒ‰æ—¶é—´å‡åº
        )
        # é©±é€æœ€ä¹…æœªä½¿ç”¨çš„ blocks
        return [block[0] for block in sorted_blocks[:num_blocks]]
```
- é€‚ç”¨åœºæ™¯: å¤§å¤šæ•°è¯·æ±‚å…·æœ‰æ—¶é—´å±€éƒ¨æ€§
- ä¼˜åŠ¿: ç®€å•,æœ‰æ•ˆ
- åŠ£åŠ¿: ä¸è€ƒè™‘è®¿é—®é¢‘ç‡

**LFU (Least Frequently Used)**:
```python
class LFU_Eviction:
    def __init__(self):
        self.access_count = {}  # block_id â†’ count

    def evict(self, num_blocks):
        # æŒ‰è®¿é—®é¢‘ç‡æ’åº
        sorted_blocks = sorted(
            self.access_count.items(),
            key=lambda x: x[1]  # æŒ‰é¢‘ç‡å‡åº
        )
        # é©±é€è®¿é—®é¢‘ç‡æœ€ä½çš„ blocks
        return [block[0] for block in sorted_blocks[:num_blocks]]
```
- é€‚ç”¨åœºæ™¯: æŸäº› prefix è¢«é¢‘ç¹å¤ç”¨ (å¦‚ç³»ç»Ÿæç¤ºè¯)
- ä¼˜åŠ¿: ä¿ç•™çƒ­ç‚¹æ•°æ®
- åŠ£åŠ¿: å†·å¯åŠ¨æ—¶æ•ˆæœå·®

**vLLM çš„æ··åˆç­–ç•¥**:
```python
class HybridEviction:
    def evict(self, num_blocks):
        # Prefix cache blocks: ä½¿ç”¨ LFU
        # (ç³»ç»Ÿæç¤ºè¯ç­‰,è¢«é¢‘ç¹å¤ç”¨)
        prefix_blocks = self.get_prefix_blocks()
        prefix_evict = lfu_evict(prefix_blocks, num_blocks // 2)

        # Decode blocks: ä½¿ç”¨ LRU
        # (æ–°ç”Ÿæˆçš„ tokens,æ—¶é—´å±€éƒ¨æ€§)
        decode_blocks = self.get_decode_blocks()
        decode_evict = lru_evict(decode_blocks, num_blocks // 2)

        return prefix_evict + decode_evict
```
- ä¼˜åŠ¿: å…¼é¡¾ cache hit rate å’Œå†…å­˜æ•ˆç‡
- ç»“æœ: ä¼˜äºå•ä¸€ç­–ç•¥

---

#### 6.3.2.5 Memory Manager å®ç°

**CacheEngine çš„æ ¸å¿ƒèŒè´£**:
```python
class CacheEngine:
    def __init__(self, block_size, num_gpu_blocks):
        self.block_size = block_size  # 16 tokens
        self.num_gpu_blocks = num_gpu_blocks
        self.free_blocks = set(range(num_gpu_blocks))
        self.block_table = {}  # {request_id: [block_ids]}
        self.hash_table = {}  # {block_hash: block_id}  # For prefix caching

    def allocate(self, request_id, num_blocks):
        """åˆ†é… blocks ç»™è¯·æ±‚"""
        if len(self.free_blocks) < num_blocks:
            raise OutOfMemory(f"Need {num_blocks}, "
                            f"only {len(self.free_blocks)} free")
        blocks = list(self.free_blocks)[:num_blocks]
        self.free_blocks.difference_update(blocks)
        self.block_table[request_id] = blocks
        return blocks

    def free(self, request_id):
        """é‡Šæ”¾è¯·æ±‚çš„ blocks"""
        blocks = self.block_table.pop(request_id)
        self.free_blocks.update(blocks)

    def get_block_hash(self, block_id):
        """è®¡ç®— block çš„ hash (ç”¨äº prefix caching)"""
        block_data = self.get_block_data(block_id)
        # ä½¿ç”¨ SHA256 æˆ–è‡ªå®šä¹‰å¿«é€Ÿ hash
        return hash(block_data.tobytes())

    def check_prefix_cache(self, request_id, block_hashes):
        """æ£€æŸ¥ prefix cache hit"""
        cached_blocks = []
        for h in block_hashes:
            if h in self.hash_table:
                cached_blocks.append(self.hash_table[h])
            else:
                break  # ç¬¬ä¸€ä¸ª miss,åç»­æ— æ³•ä½¿ç”¨
        return cached_blocks
```

---

#### 6.3.2.6 PagedAttention vs ä¼ ç»Ÿæ–¹æ¡ˆå¯¹æ¯”

| ç»´åº¦ | è¿ç»­å†…å­˜ | PagedAttention |
|------|---------|----------------|
| **å†…å­˜åˆ©ç”¨ç‡** | 60-70% | 90-95% |
| **ç¢ç‰‡åŒ–** | ä¸¥é‡ | è½»å¾® |
| **Prefix Caching** | å›°éš¾ | å®¹æ˜“ (hash-based) |
| **å®ç°å¤æ‚åº¦** | ç®€å• | ä¸­ç­‰ |
| **æ€§èƒ½å¼€é”€** | æ—  | è½»å¾® (block table lookup) |
| **é€‚ç”¨åœºæ™¯** | å•è¯·æ±‚ã€çŸ­åºåˆ— | å¤šè¯·æ±‚ã€é•¿åºåˆ—ã€ç”Ÿäº§ç¯å¢ƒ |

**æ€§èƒ½å¼€é”€åˆ†æ**:
- Block table lookup: O(1) hash table
- é¢å¤–å†…å­˜: block_table (æ¯ä¸ªè¯·æ±‚ ~1KB)
- ç›¸æ¯”æ”¶ç›Š (+30% å†…å­˜åˆ©ç”¨ç‡),å¼€é”€å¯å¿½ç•¥

---

#### 6.3.2.7 çœŸå®æ¡ˆä¾‹åˆ†æ

**æ¡ˆä¾‹ 1: ChatGPT é£æ ¼å¯¹è¯**
```
ç³»ç»Ÿæç¤ºè¯: 500 tokens ("You are a helpful assistant...")
ç”¨æˆ·è¾“å…¥: 50 tokens
æ¨¡å‹è¾“å‡º: 100 tokens

ä¼ ç»Ÿæ–¹æ³•:
  - æ¯ä¸ªè¯·æ±‚éœ€è¦ 650 tokens è¿ç»­ç©ºé—´
  - ç³»ç»Ÿæç¤ºè¯æ¯æ¬¡é‡æ–°è®¡ç®—
  - å†…å­˜åˆ©ç”¨ç‡: ~65%

PagedAttention + Prefix Caching:
  - ç³»ç»Ÿæç¤ºè¯: 32 blocks (cached)
  - 100 ä¸ªè¯·æ±‚å…±äº«è¿™ 32 ä¸ª blocks
  - æ¯ä¸ªè¯·æ±‚åªéœ€è¦: ç”¨æˆ·è¾“å…¥ 4 blocks + è¾“å‡º 7 blocks
  - å†…å­˜åˆ©ç”¨ç‡: ~92%
```

**æ¡ˆä¾‹ 2: é•¿æ–‡æ¡£æ‘˜è¦**
```
æ–‡æ¡£é•¿åº¦: 10,000 tokens
æ‘˜è¦é•¿åº¦: 200 tokens
å¹¶å‘æ•°: 10 ä¸ªè¯·æ±‚

ä¼ ç»Ÿæ–¹æ³•:
  - éœ€è¦ 10 Ã— 10,200 = 102,000 tokens è¿ç»­ç©ºé—´
  - 24GB GPU åªèƒ½å¤„ç† ~2 ä¸ªå¹¶å‘è¯·æ±‚
  - å†…å­˜åˆ©ç”¨ç‡: ~60%

PagedAttention:
  - åŠ¨æ€åˆ†é… blocks
  - å¯ä»¥å¤„ç† ~5 ä¸ªå¹¶å‘è¯·æ±‚
  - å†…å­˜åˆ©ç”¨ç‡: ~88%
  - ååé‡æå‡: 2.5x
```

---

#### 6.3.2.8 å®æˆ˜é…ç½®

**å¯åŠ¨ vLLM æ—¶å¯ç”¨ PagedAttention** (é»˜è®¤å¯ç”¨):
```bash
vllm serve meta-llama/Llama-2-7b-hf \
  --block-size 16 \              # Block å¤§å° (é»˜è®¤: 16)
  --gpu-memory-utilization 0.9 \  # GPU å†…å­˜åˆ©ç”¨ç‡
  --max-num-batched-tokens 8192  # æœ€å¤§ batch tokens
```

**ç›‘æ§ block ä½¿ç”¨æƒ…å†µ**:
```python
from vllm import LLM

llm = LLM(model="meta-llama/Llama-2-7b-hf")

# è·å– block allocator ç»Ÿè®¡
stats = llm.llm_engine.cache_engine.get_stats()
print(f"Free blocks: {stats['num_free_blocks']}")
print(f"Used blocks: {stats['num_used_blocks']}")
print(f"GPU utilization: {stats['gpu_utilization']:.2%}")
```

---

#### 6.3.2.9 æ€§èƒ½ç›‘æ§

**å…³é”®æŒ‡æ ‡**:
```python
# Block ä½¿ç”¨ç‡
block_utilization = used_blocks / total_blocks

# Cache hit rate (Prefix Caching)
cache_hit_rate = cache_hits / total_requests

# Memory fragmentation
fragmentation = 1 - (largest_free_block / total_free_blocks)
```

**å‘Šè­¦é˜ˆå€¼**:
```
Block åˆ©ç”¨ç‡ > 95% â†’ è€ƒè™‘å¢åŠ  GPU æˆ–å‡å° batch size
Cache hit rate < 30% â†’ Prefix Caching æ•ˆæœä¸ä½³
Fragmentation > 20% â†’ å¯èƒ½éœ€è¦è°ƒæ•´ block size
```

---

#### 6.3.2.10 æ€»ç»“: PagedAttention çš„æ ¸å¿ƒä»·å€¼

**å…³é”®æˆå°±**:
1. âœ… å†…å­˜åˆ©ç”¨ç‡ä» 60-70% æå‡åˆ° 90-95%
2. âœ… æ”¯æŒ Prefix Caching (ç›¸åŒ prompt åªè®¡ç®—ä¸€æ¬¡)
3. âœ… åŠ¨æ€å†…å­˜åˆ†é… (ä¸éœ€è¦é¢„çŸ¥åºåˆ—é•¿åº¦)
4. âœ… æ”¯æŒæ›´é«˜çš„å¹¶å‘å’Œæ›´é•¿çš„åºåˆ—

**é€‚ç”¨åœºæ™¯**:
- âœ… å¤šç§Ÿæˆ· SaaS (å¤§é‡å¹¶å‘è¯·æ±‚)
- âœ… é•¿åºåˆ—ç”Ÿæˆ (æ–‡æ¡£æ‘˜è¦ã€é•¿å¯¹è¯)
- âœ… å…±äº« prompt (ç³»ç»Ÿæç¤ºè¯ã€RAG åœºæ™¯)

**æƒè¡¡**:
- âš ï¸ è½»å¾®çš„å†…å­˜å¼€é”€ (block table)
- âš ï¸ å®ç°å¤æ‚åº¦å¢åŠ 
- âœ… ä½†æ”¶ç›Šè¿œå¤§äºæˆæœ¬

---

## 6.4 KV Cache ä¼˜åŒ–æŠ€æœ¯

### 6.4.1 Multi-Query Attention vs Multi-Head Attention

**Multi-Head Attention (MHA)**:
```
æ¯ä¸ª head æœ‰ç‹¬ç«‹çš„ Kã€V:
Head 0: K0, V0
Head 1: K1, V1
Head 2: K2, V2
...
Head 31: K31, V31

KV Cache å¤§å°:
= 32 heads Ã— 2 Ã— seq_len Ã— head_dim Ã— bytes
```

**Multi-Query Attention (MQA)**:
```
æ‰€æœ‰ heads å…±äº« Kã€V:
Heads 0-31: å…±äº« K, V

KV Cache å¤§å°:
= 1 Ã— 2 Ã— seq_len Ã— head_dim Ã— bytes
= MHA çš„ 1/32!
```

**å¯¹æ¯”**:

| ç»´åº¦ | MHA | MQA |
|------|-----|-----|
| **KV Cache å¤§å°** | å¤§ (32x) | å° (1x) |
| **æ¨¡å‹è´¨é‡** | é«˜ | ç•¥ä½ |
| **æ¨ç†é€Ÿåº¦** | æ…¢ | å¿« |
| **é€‚ç”¨åœºæ™¯** | è¿½æ±‚è´¨é‡ | è¿½æ±‚é€Ÿåº¦ |

---

### 6.4.2 Grouped-Query Attention (GQA)

**æŠ˜ä¸­æ–¹æ¡ˆ**:
```
32 ä¸ª heads,åˆ†æˆ 8 ç»„,æ¯ç»„å…±äº« Kã€V:
Group 0 (Heads 0-3): å…±äº« K0, V0
Group 1 (Heads 4-7): å…±äº« K1, V1
Group 2 (Heads 8-11): å…±äº« K2, V2
...
Group 7 (Heads 28-31): å…±äº« K7, V7

KV Cache å¤§å°:
= 8 groups Ã— 2 Ã— seq_len Ã— head_dim Ã— bytes
= MHA çš„ 1/4, MQA çš„ 8x
```

**ä¸ºä»€ä¹ˆ GQA æ˜¯æœ€ä½³é€‰æ‹©**:
- âœ… æ¥è¿‘ MHA çš„è´¨é‡
- âœ… æ¥è¿‘ MQA çš„é€Ÿåº¦
- âœ… Llama-3ã€Mistral ç­‰ç°ä»£æ¨¡å‹é‡‡ç”¨

---

### 6.4.3 Shared KV Cache

**è·¨è¯·æ±‚å…±äº«**:
```
Request A: "System: You are assistant. User: Hello"
Request B: "System: You are assistant. User: Hi"

å…±äº«éƒ¨åˆ†: "System: You are assistant."
â†’ åªè®¡ç®—ä¸€æ¬¡ KV Cache
â†’ Request A å’Œ B éƒ½å¤ç”¨
```

**å®ç°**:
```python
class SharedKVCache:
    def __init__(self):
        self.global_cache = {}  # {token_seq_hash: (K, V)}

    def get_or_compute(self, tokens):
        hash = compute_hash(tokens)
        if hash in self.global_cache:
            return self.global_cache[hash]
        else:
            K, V = model.compute_kv(tokens)
            self.global_cache[hash] = (K, V)
            return K, V
```

---

### 6.4.4 é‡åŒ– KV Cache

**FP16 â†’ INT8**:
```
FP16: æ¯ä¸ªå…ƒç´  2 bytes
INT8: æ¯ä¸ªå…ƒç´  1 byte

KV Cache å¤§å°å‡åŠ!
```

**é‡åŒ–æ–¹æ³•**:
```python
def quantize_kv(kv_cache):
    # FP16 â†’ INT8
    scale = kv_cache.abs().max() / 127
    kv_int8 = (kv_cache / scale).round().char()
    return kv_int8, scale

def dequantize_kv(kv_int8, scale):
    # INT8 â†’ FP16
    return kv_int8.float() * scale
```

**è´¨é‡å½±å“**:
- âœ… å¤§å¤šæ•°åœºæ™¯ä¸‹ç²¾åº¦æŸå¤±å¯æ¥å—
- âš ï¸ å¤æ‚æ¨ç†ä»»åŠ¡å¯èƒ½å—å½±å“
- ğŸ’¡ å»ºè®®: å…ˆå®éªŒ,å†å†³å®šæ˜¯å¦ä½¿ç”¨

---

## 6.5 KV Cache çš„ä»£ä»·

### 6.5.1 æ˜¾å­˜å ç”¨åˆ†æ

**Llama-2-7B çš„ KV Cache**:
```
å•å±‚å•å¤´çš„ KV Cache:
= 2 Ã— seq_len Ã— head_dim Ã— bytes
= 2 Ã— 4096 Ã— 128 Ã— 2
= 2,097,152 bytes
â‰ˆ 2 MB

æ‰€æœ‰å±‚æ‰€æœ‰å¤´:
= 2 MB Ã— 32 layers Ã— 32 heads
= 2,048 MB
â‰ˆ 2 GB (seq_len = 4096)
```

**ä¸åŒæ¨¡å‹çš„ KV Cache å¤§å°**:

| æ¨¡å‹ | å±‚æ•° | Heads | Head Dim | 4K tokens | 8K tokens | 16K tokens |
|------|------|-------|----------|----------|----------|-----------|
| Llama-2-7B | 32 | 32 | 128 | 0.5 GB | 1 GB | 2 GB |
| Llama-2-13B | 40 | 40 | 128 | 0.8 GB | 1.6 GB | 3.2 GB |
| Llama-2-70B | 80 | 64 | 128 | 2 GB | 4 GB | 8 GB |

**å¹¶å‘è¯·æ±‚çš„æ˜¾å­˜éœ€æ±‚**:
```
10 ä¸ªå¹¶å‘è¯·æ±‚,æ¯ä¸ª 4K tokens:
- Llama-2-7B: 10 Ã— 0.5 GB = 5 GB
- Llama-2-70B: 10 Ã— 2 GB = 20 GB

åŠ ä¸Šæ¨¡å‹æƒé‡:
- Llama-2-7B (13 GB) + KV (5 GB) = 18 GB â†’ å•å¼  A100 (40GB) âœ…
- Llama-2-70B (140 GB) â†’ éœ€è¦æ¨¡å‹å¹¶è¡Œ
```

---

### 6.5.2 åºåˆ—é•¿åº¦é™åˆ¶

**GPU æ˜¾å­˜é™åˆ¶åºåˆ—é•¿åº¦**:
```
A100 40GB:
- æ¨¡å‹æƒé‡: 13 GB (Llama-2-7B FP16)
- å‰©ä½™: 27 GB
- KV Cache: 27 GB / 0.5 GB per 4K = 216K tokens
- å•è¯·æ±‚: 216K tokens
- 10 ä¸ªå¹¶å‘: 21.6K tokens each
```

**å®é™…è€ƒè™‘**:
- âš ï¸ Prefill é˜¶æ®µéœ€è¦ä¸´æ—¶æ˜¾å­˜
- âš ï¸ Decode é˜¶æ®µéœ€è¦é¢å¤–æ˜¾å­˜
- âš ï¸ ç•™ä¸€äº› buffer é¿å…OOM

---

### 6.5.3 æƒè¡¡: è®¡ç®—vsæ˜¾å­˜

**æœ‰ KV Cache**:
```
âœ… ä¼˜ç‚¹:
  - å¤§å¹…å‡å°‘è®¡ç®—
  - é™ä½å»¶è¿Ÿ

âŒ ç¼ºç‚¹:
  - å ç”¨å¤§é‡æ˜¾å­˜
  - é™åˆ¶å¹¶å‘æ•°å’Œåºåˆ—é•¿åº¦
```

**æ—  KV Cache**:
```
âœ… ä¼˜ç‚¹:
  - èŠ‚çœæ˜¾å­˜
  - æ”¯æŒæ›´é•¿åºåˆ—

âŒ ç¼ºç‚¹:
  - è®¡ç®—é‡å¤§
  - å»¶è¿Ÿé«˜
```

**æœ€ä½³å®è·µ**:
```
çŸ­åºåˆ— (< 2K tokens): ä¸éœ€è¦ KV Cache
ä¸­ç­‰åºåˆ— (2K-8K): KV Cache
é•¿åºåˆ— (> 8K): KV Cache + é‡åŒ– + åˆ†å—å¤„ç†
```

---

## 6.6 å®æˆ˜å¯¹æ¯”

### 6.6.1 æ—  KV Cache vs æœ‰ KV Cache

**æ€§èƒ½æµ‹è¯•**:
```
æ¨¡å‹: Llama-2-7B
ç¡¬ä»¶: A100 40GB
åºåˆ—é•¿åº¦: 1024 tokens

æ—  KV Cache:
- TTFT: 200ms
- TBT: 50ms/token
- æ€»æ—¶é—´ (ç”Ÿæˆ 100 tokens): 200ms + 100 Ã— 50ms = 5.2s

æœ‰ KV Cache:
- TTFT: 200ms (ç›¸åŒ)
- TBT: 5ms/token (10x å¿«!)
- æ€»æ—¶é—´ (ç”Ÿæˆ 100 tokens): 200ms + 100 Ã— 5ms = 0.7s

åŠ é€Ÿæ¯”: 5.2s / 0.7s = 7.4x
```

---

### 6.6.2 æ€§èƒ½æå‡é‡åŒ–åˆ†æ

**ä¸åŒåºåˆ—é•¿åº¦çš„åŠ é€Ÿæ¯”**:
```
åºåˆ—é•¿åº¦ = 100:
  æ—  KV Cache: ~100ms
  æœ‰ KV Cache: ~100ms
  åŠ é€Ÿæ¯”: 1x (å¤ªçŸ­,æ²¡ä¼˜åŠ¿)

åºåˆ—é•¿åº¦ = 1000:
  æ—  KV Cache: ~10s
  æœ‰ KV Cache: ~1.5s
  åŠ é€Ÿæ¯”: 6.7x

åºåˆ—é•¿åº¦ = 10000:
  æ—  KV Cache: ~1000s
  æœ‰ KV Cache: ~8s
  åŠ é€Ÿæ¯”: 125x!
```

**ç»“è®º**: åºåˆ—è¶Šé•¿,KV Cache çš„ä¼˜åŠ¿è¶Šæ˜æ˜¾

---

### 6.6.3 vLLM çš„ KV Cache å®ç°

**å…³é”®ç‰¹æ€§**:
1. âœ… PagedAttention (é«˜å†…å­˜åˆ©ç”¨ç‡)
2. âœ… Prefix Caching (è·¨è¯·æ±‚å¤ç”¨)
3. âœ… è‡ªåŠ¨ eviction (LRU/LFU)
4. âœ… åŠ¨æ€ block åˆ†é…

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from vllm import LLM, SamplingParams

# vLLM è‡ªåŠ¨å¯ç”¨ PagedAttention
llm = LLM(model="meta-llama/Llama-2-7b-hf")

# Prefix Caching è‡ªåŠ¨ç”Ÿæ•ˆ
prompts = [
    "System: You are assistant. User: Hello",
    "System: You are assistant. User: Hi",
]
# ç¬¬äºŒä¸ªè¯·æ±‚ä¼šå¤ç”¨ç¬¬ä¸€ä¸ªçš„ system prompt KV Cache

outputs = llm.generate(prompts)
```

---

## 6.7 Prefix Caching â­â­â­

> **ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ**: é‡å¤çš„ prompt (å¦‚ç³»ç»Ÿæç¤ºè¯) åªéœ€è¦è®¡ç®—ä¸€æ¬¡,åç»­è¯·æ±‚ç›´æ¥å¤ç”¨ KV Cacheã€‚
>
> **ğŸ¯ æ€§èƒ½æå‡**: ChatGPT é£æ ¼å¯¹è¯åœºæ™¯å¯æå‡ 2-5 å€ååé‡ã€‚
>
> **æ¥æº**: vLLM æ ¸å¿ƒç‰¹æ€§ä¹‹ä¸€,å·²åœ¨ç”Ÿäº§ç¯å¢ƒå¤§è§„æ¨¡éªŒè¯ã€‚

### 6.7.1 ä»€ä¹ˆæ˜¯ Prefix Caching

**å®šä¹‰**: è·¨è¯·æ±‚å¤ç”¨ç›¸åŒ prompt çš„ KV Cache

**æ ¸å¿ƒé—®é¢˜**: é‡å¤ prompt çš„è®¡ç®—æµªè´¹
```
Request 1: "System: You are helpful. User: What is AI?"
Request 2: "System: You are helpful. User: Tell me a joke"
Request 3: "System: You are helpful. User: How are you?"

é—®é¢˜: "System: You are helpful." è®¡ç®—äº† 3 æ¬¡! âŒ
```

**å…¸å‹åœºæ™¯**:
- ç³»ç»Ÿæç¤ºè¯ ("You are a helpful assistant...")
- å¤šè½®å¯¹è¯çš„ä¸Šä¸‹æ–‡
- RAG åœºæ™¯çš„å›ºå®šçŸ¥è¯† prefix

**ä¸ºä»€ä¹ˆå«"Prefix"**:
- Cache çš„æ˜¯ prompt éƒ¨åˆ† (å³åºåˆ—çš„ prefix)
- ç”Ÿæˆçš„éƒ¨åˆ† (decode é˜¶æ®µ) å› äººè€Œå¼‚,æ— æ³•å¤ç”¨

---

### 6.7.2 Prefix Caching çš„æ ¸å¿ƒæ€æƒ³

**ä¼ ç»Ÿ KV Cache**: å•æ¬¡è¯·æ±‚å†…å¤ç”¨
- Token 0 çš„ KV è¢« token 1, 2, 3...å¤ç”¨
- ä½†è¯·æ±‚ç»“æŸå,Cache è¢«æ¸…ç©º

**Prefix Caching**: è·¨è¯·æ±‚å¤ç”¨
- è¯·æ±‚ 1: è®¡ç®—å®Œæ•´ prompt çš„ KV â†’ Cache
- è¯·æ±‚ 2: æ£€æµ‹åˆ°ç›¸åŒ prefix â†’ ç›´æ¥å¤ç”¨ â†’ è·³è¿‡è®¡ç®—
- è¯·æ±‚ 3ã€4ã€5...: åŒè¯·æ±‚ 2

**ç±»æ¯”**:
```
ä¼ ç»Ÿ Cache: å‡½æ•°å†…çš„ memoization
Prefix Caching: å…¨å±€ distributed cache (å¦‚ Redis)
```

---

### 6.7.3 vLLM çš„å®ç°: Hash-based KV Cache

**æŒ‘æˆ˜**: å¦‚ä½•æ£€æµ‹ä¸¤ä¸ªè¯·æ±‚çš„ prefix æ˜¯å¦ç›¸åŒ?

**æ–¹æ¡ˆ 1: å­—ç¬¦ä¸²æ¯”è¾ƒ** (Naive)
- æ¯æ¬¡æ¯”è¾ƒ prompt æ–‡æœ¬
- é—®é¢˜: æ…¢!è€Œä¸”è¯­ä¹‰ç›¸åŒçš„ token å¯èƒ½æ¥è‡ªä¸åŒæ–‡æœ¬

**æ–¹æ¡ˆ 2: vLLM çš„ Hash-based æ–¹æ³•** â­
- å¯¹æ¯ä¸ª Block çš„ KV Cache è®¡ç®— Hash
- Hash ç›¸åŒçš„ Block è¢«è®¤ä¸ºå†…å®¹ç›¸åŒ

**Hash ç®—æ³•**:
```python
def compute_block_hash(block_kv):
    """
    è¾“å…¥: Block çš„ KV tensor
    è¾“å‡º: å›ºå®šé•¿åº¦çš„ hash å€¼
    å®ç°: SHA256 æˆ–è‡ªå®šä¹‰å¿«é€Ÿ hash
    """
    # æ–¹æ³• 1: SHA256 (å‡†ç¡®ä½†æ…¢)
    import hashlib
    return hashlib.sha256(block_kv.tobytes()).hexdigest()

    # æ–¹æ³• 2: å¿«é€Ÿ hash (vLLM å¯èƒ½ä½¿ç”¨çš„)
    # ç®€å•çš„ XOR æˆ– rolling hash
    return fast_hash(block_kv)
```

**Cache Hit æ£€æµ‹æµç¨‹**:
1. æ–°è¯·æ±‚åˆ°æ¥
2. è®¡ç®— prompt tokens å¯¹åº”çš„ logical blocks
3. æŸ¥è¯¢ hash table: æ˜¯å¦å·²æœ‰è¿™äº› blocks çš„ KV?
4. å¦‚æœ hit: ç›´æ¥å¼•ç”¨å·²æœ‰ physical blocks
5. å¦‚æœ miss: åˆ†é…æ–°çš„ physical blocks å¹¶è®¡ç®—

---

### 6.7.4 Prefix Caching çš„å·¥ä½œæµç¨‹

**é¦–æ¬¡è¯·æ±‚ (Cold Path)**:
```
1. ç”¨æˆ·å‘é€ prompt (å«ç³»ç»Ÿæç¤ºè¯)
2. vLLM è®¡ç®—æ‰€æœ‰ tokens çš„ KV Cache
3. å°† KV Cache åˆ†æˆ blocks,è®¡ç®—æ¯ä¸ª block çš„ hash
4. å­˜å‚¨åˆ° cache engine (hash table)
5. è¿”å›ç»“æœ
```

**åç»­è¯·æ±‚ (Warm Path)**:
```
1. ç”¨æˆ·å‘é€ç›¸åŒç³»ç»Ÿæç¤ºè¯çš„æ–°è¯·æ±‚
2. vLLM è®¡ç®— blocks çš„ hash
3. **Cache Hit!**: å‘ç°å·²æœ‰å¯¹åº”çš„ KV Cache
4. ç›´æ¥å¼•ç”¨å·²æœ‰ blocks,è·³è¿‡ prefill è®¡ç®—
5. åªéœ€è®¡ç®—ç”¨æˆ·è¾“å…¥çš„æ–° tokens
6. è¿”å›ç»“æœ (å¿«å¾—å¤š!)
```

**éƒ¨åˆ† Hit åœºæ™¯**:
```
ç³»ç»Ÿæç¤ºè¯: hit âœ…
ç”¨æˆ·è¾“å…¥: miss âŒ

â†’ å¤ç”¨ç³»ç»Ÿæç¤ºè¯çš„ KV
â†’ åªè®¡ç®—ç”¨æˆ·è¾“å…¥éƒ¨åˆ†
â†’ ä»ç„¶æœ‰åŠ é€Ÿæ•ˆæœ
```

---

### 6.7.5 æ€§èƒ½æå‡åˆ†æ

**ç†è®ºåŠ é€Ÿæ¯”**:
```
å‡è®¾:
- ç³»ç»Ÿæç¤ºè¯é•¿åº¦ = P tokens
- ç”¨æˆ·è¾“å…¥é•¿åº¦ = U tokens

æ—  Prefix Caching:
  æ¯æ¬¡è®¡ç®— P + U

æœ‰ Prefix Caching:
  é¦–æ¬¡: P + U
  åç»­: U

åŠ é€Ÿæ¯” â‰ˆ (P + U) / U = 1 + P/U
```

**å®é™…æ¡ˆä¾‹**:
```
åœºæ™¯ 1: ç³»ç»Ÿæç¤ºè¯ 200 tokens,ç”¨æˆ·è¾“å…¥ 50 tokens
  åŠ é€Ÿæ¯” = (200 + 50) / 50 = **5 å€**

åœºæ™¯ 2: ç³»ç»Ÿæç¤ºè¯ 1000 tokens (RAG åœºæ™¯),ç”¨æˆ·è¾“å…¥ 20 tokens
  åŠ é€Ÿæ¯” = (1000 + 20) / 20 = **51 å€** (æç«¯ case)

åœºæ™¯ 3: æ— ç³»ç»Ÿæç¤ºè¯
  åŠ é€Ÿæ¯” = 1x (æ— æ•ˆæœ)
```

**å†…å­˜å¼€é”€**:
- Hash table å­˜å‚¨: æ¯ä¸ª block ~32 bytes hash
- KV Cache å­˜å‚¨: åŸæœ¬å°±éœ€è¦,ä¸ç®—é¢å¤–å¼€é”€

**è€ƒè™‘å› ç´ **:
- Cache expiration æ—¶é—´
- Memory pressure
- è‡³å°‘ä¿ç•™ system prompt çš„ breakpoint

---

### 6.7.6 vLLM é…ç½®

**å¯ç”¨ Prefix Caching** (v0.6.0+):
```bash
vllm serve meta-llama/Llama-2-7b-hf \
  --enable-prefix-caching \
  --max-num-seqs 128
```

**ç›‘æ§ Cache Hit Rate**:
```python
from vllm import LLM

llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    enable_prefix_caching=True
)

# ç”Ÿæˆå¤šä¸ªè¯·æ±‚
for prompt in prompts:
    llm.generate(prompt)

# è·å– cache ç»Ÿè®¡
stats = llm.llm_engine.cache_engine.get_prefix_cache_stats()
print(f"Cache hit rate: {stats['hit_rate']:.2%}")
print(f"Total tokens cached: {stats['total_tokens']}")
print(f"Tokens served from cache: {stats['cached_tokens']}")
```

---

### 6.7.7 å®æˆ˜æ¡ˆä¾‹

**æ¡ˆä¾‹ 1: Chatbot æœåŠ¡**
```
åœºæ™¯:
- ç³»ç»Ÿæç¤ºè¯: 500 tokens
- ç”¨æˆ·è¾“å…¥: å¹³å‡ 50 tokens
- æ¯åˆ†é’Ÿ 1000 ä¸ªè¯·æ±‚

æ—  Prefix Caching:
  - æ¯ä¸ªè¯·æ±‚è®¡ç®— 550 tokens
  - æ€»è®¡ç®—: 1000 Ã— 550 = 550K tokens/åˆ†é’Ÿ

æœ‰ Prefix Caching:
  - é¦–ä¸ªè¯·æ±‚: 550 tokens
  - åç»­ 999 ä¸ªè¯·æ±‚: æ¯ä¸ª 50 tokens
  - æ€»è®¡ç®—: 550 + 999 Ã— 50 = 50.5K tokens/åˆ†é’Ÿ
  - åŠ é€Ÿæ¯”: 550K / 50.5K = **10.9x**
```

**æ¡ˆä¾‹ 2: RAG åº”ç”¨**
```
åœºæ™¯:
- çŸ¥è¯†åº“ prefix: 2000 tokens
- ç”¨æˆ·é—®é¢˜: å¹³å‡ 30 tokens
- æ¯åˆ†é’Ÿ 500 ä¸ªè¯·æ±‚

æ—  Prefix Caching:
  - æ€»è®¡ç®—: 500 Ã— 2030 = 1,015K tokens/åˆ†é’Ÿ

æœ‰ Prefix Caching:
  - æ€»è®¡ç®—: 2030 + 499 Ã— 30 = 17K tokens/åˆ†é’Ÿ
  - åŠ é€Ÿæ¯”: 1,015K / 17K = **59.7x**
```

---

### 6.7.8 æœ€ä½³å®è·µ

**1. è¯†åˆ«å¯ç¼“å­˜çš„ Prefix**
```
âœ… é€‚åˆç¼“å­˜:
  - ç³»ç»Ÿæç¤ºè¯
  - å›ºå®šçš„çŸ¥è¯†åº“å†…å®¹
  - å¤šè½®å¯¹è¯çš„å†å²
  - å…±äº«çš„ä¸Šä¸‹æ–‡

âŒ ä¸é€‚åˆç¼“å­˜:
  - å®Œå…¨éšæœºçš„è¾“å…¥
  - æ¯æ¬¡éƒ½ä¸åŒçš„ç”¨æˆ·æŸ¥è¯¢
```

**2. åˆç†è®¾ç½® Cache å¤§å°**
```
å¤ªå°:
  - é¢‘ç¹ eviction
  - Cache hit rate ä½

å¤ªå¤§:
  - å ç”¨è¿‡å¤šæ˜¾å­˜
  - å½±å“å¹¶å‘èƒ½åŠ›

å»ºè®®:
  - æ ¹æ®å®é™… hit rate è°ƒæ•´
  - ç›®æ ‡: 70-90% hit rate
```

**3. ç›‘æ§å’Œè°ƒä¼˜**
```python
# å®šæœŸæ£€æŸ¥ cache æ•ˆæœ
def monitor_prefix_cache(llm):
    stats = llm.get_cache_stats()

    if stats['hit_rate'] < 0.5:
        print("âš ï¸  Cache hit rate ä½äº 50%,è€ƒè™‘ä¼˜åŒ–:")
        print("  1. å¢åŠ ç³»ç»Ÿæç¤ºè¯é•¿åº¦")
        print("  2. æ£€æŸ¥æ˜¯å¦æœ‰å¯å…±äº«çš„ prefix")
        print("  3. è°ƒæ•´ cache size")

    print(f"Cache hit rate: {stats['hit_rate']:.2%}")
    print(f"Memory usage: {stats['memory_used']}/{stats['memory_total']}")
```

---

## âœ… ç« èŠ‚æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬ç« å,ä½ åº”è¯¥èƒ½å¤Ÿ:

- [ ] è§£é‡Šä¼ ç»Ÿ KV Cache çš„å†…å­˜ç¢ç‰‡åŒ–é—®é¢˜
- [ ] æè¿° PagedAttention çš„è®¾è®¡æ€æƒ³ (å€Ÿé‰´ OS è™šæ‹Ÿå†…å­˜)
- [ ] å¯¹æ¯” Logical blocks å’Œ Physical blocks
- [ ] ç†è§£ Block allocation å’Œ eviction ç­–ç•¥
- [ ] è®¡ç®— KV Cache çš„æ˜¾å­˜å ç”¨
- [ ] å¯¹æ¯” MHAã€MQAã€GQA çš„ KV Cache å¤§å°
- [ ] è§£é‡Š Prefix Caching çš„å·¥ä½œåŸç†
- [ ] è®¡ç®— Prefix Caching çš„åŠ é€Ÿæ¯”
- [ ] é…ç½® vLLM å¯ç”¨ Prefix Caching
- [ ] ç›‘æ§ cache hit rate å¹¶ä¼˜åŒ–

---

## ğŸ“š åŠ¨æ‰‹ç»ƒä¹ 

**ç»ƒä¹  6.1**: è®¡ç®— KV Cache æ˜¾å­˜å ç”¨

Llama-2-7B çš„é…ç½®:
- å±‚æ•°: 32
- Attention heads: 32
- Head dimension: 128
- æ•°æ®ç±»å‹: FP16 (2 bytes)
- Block size: 16 tokens

é—®é¢˜:
1. å•ä¸ª block çš„ KV cache å¤§å°æ˜¯å¤šå°‘?
2. 100 ä¸ª blocks éœ€è¦å¤šå°‘æ˜¾å­˜?
3. å¦‚æœå¯ç”¨ Prefix Caching,ç¼“å­˜ 1000 ä¸ª blocks,æ€»æ˜¾å­˜å ç”¨æ˜¯å¤šå°‘?

**ç»ƒä¹  6.2**: å¯¹æ¯” PagedAttention å’Œä¼ ç»Ÿæ–¹æ³•

å‡è®¾æœ‰ä»¥ä¸‹è¯·æ±‚åºåˆ—:
```
Request A: 1000 tokens â†’ å®Œæˆ
Request B: 500 tokens â†’ è¿›è¡Œä¸­
Request C: éœ€è¦ 800 tokens â†’ æ–°è¯·æ±‚
GPU æ€»æ˜¾å­˜: 2048 tokens
```

ä»»åŠ¡:
1. ä¼ ç»Ÿæ–¹æ³•èƒ½å¦å¤„ç† Request C? ä¸ºä»€ä¹ˆ?
2. PagedAttention å¦‚ä½•å¤„ç†è¿™ä¸ªåœºæ™¯?
3. è®¡ç®—ä¸¤ç§æ–¹æ³•çš„å†…å­˜åˆ©ç”¨ç‡

**ç»ƒä¹  6.3**: Prefix Caching åŠ é€Ÿæ¯”è®¡ç®—

åœºæ™¯:
- ç³»ç»Ÿæç¤ºè¯: 800 tokens
- ç”¨æˆ·è¾“å…¥: å¹³å‡ 40 tokens
- æ¯å°æ—¶ 10,000 ä¸ªè¯·æ±‚

ä»»åŠ¡:
1. è®¡ç®—æ—  Prefix Caching çš„æ€»è®¡ç®—é‡
2. è®¡ç®—æœ‰ Prefix Caching çš„æ€»è®¡ç®—é‡
3. è®¡ç®—åŠ é€Ÿæ¯”
4. å¦‚æœç³»ç»Ÿæç¤ºè¯å¢åŠ åˆ° 2000 tokens,åŠ é€Ÿæ¯”æ˜¯å¤šå°‘?

---

## ğŸ¯ æ€»ç»“

**å…³é”®è¦ç‚¹**:
- ä¼ ç»Ÿ KV Cache é­å—å†…å­˜ç¢ç‰‡åŒ–,åˆ©ç”¨ç‡åªæœ‰ 60-70%
- PagedAttention å€Ÿé‰´ OS è™šæ‹Ÿå†…å­˜,å°†åˆ©ç”¨ç‡æå‡åˆ° 90-95%
- Block allocation å’Œ eviction ç­–ç•¥æ˜¯ PagedAttention çš„æ ¸å¿ƒ
- Prefix Caching é€šè¿‡è·¨è¯·æ±‚å¤ç”¨,å¯å¸¦æ¥ 2-50x åŠ é€Ÿ
- GQA æ˜¯è´¨é‡ä¸é€Ÿåº¦çš„æœ€ä½³å¹³è¡¡
- vLLM è‡ªåŠ¨å¯ç”¨ PagedAttention å’Œ Prefix Caching

**ä¸‹ä¸€ç« **: ç¬¬7ç«  è¯·æ±‚è°ƒåº¦ç­–ç•¥â€”â€”ç†è§£ vLLM å¦‚ä½•é«˜æ•ˆè°ƒåº¦å¤šä¸ªè¯·æ±‚ã€‚

---

**æœ‰é—®é¢˜?åŠ å…¥ [ç¬¬6ç«  Discord é¢‘é“](https://discord.gg/TODO) è®¨è®º!**
